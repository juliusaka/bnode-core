"""Kullback-Leibler divergence computation for VAE training.

Provides functions to compute KL divergence between learned latent distributions
and standard normal prior, with support for timeseries data and dimension analysis.

Attention:
    This documentation is generated by AI. Please be aware of possible inaccurcies.
"""

import torch
from typing import Optional

def kullback_leibler(
    mu: torch.Tensor, 
    logvar: torch.Tensor, 
    per_dimension: bool = False, 
    reduce: bool = True, 
    time_series_aggregation_mode: Optional[str] = 'mean'
) -> torch.Tensor:
    """Compute KL divergence KL(N(mu, exp(logvar)) || N(0, I)).
    
    Calculates the Kullback-Leibler divergence between a learned normal distribution
    N(mu, sigma^2) and the standard normal prior N(0, 1). Uses the analytical formula:
    KL = -0.5 * sum(1 + logvar - mu^2 - exp(logvar))
    
    Args:
        mu: Mean of learned distribution, shape (batch, latent_dim) or 
            (batch, latent_dim, seq_len) for timeseries.
        logvar: Log-variance of learned distribution, same shape as mu.
        per_dimension: If True, return KL divergence per latent dimension instead of
            summing across dimensions. Default: False.
        reduce: If True, return mean over batch. If False, return per-sample values.
            Default: True.
        time_series_aggregation_mode: How to aggregate over time dimension if input is
            timeseries (3D). Options: 'mean', 'max', 'sum', or None (keep time dim).
            Default: 'mean'.
    
    Returns:
        KL divergence tensor. Shape depends on parameters:
            - per_dimension=False, reduce=True: scalar
            - per_dimension=False, reduce=False: (batch,)
            - per_dimension=True, reduce=True: (latent_dim,)
            - per_dimension=True, reduce=False: (batch, latent_dim)
    
    Note:
        The KL divergence is always non-negative and equals zero only when the learned
        distribution matches the prior exactly.
    """
    is_timeseries = len(mu.shape) == 3
    kl = -0.5 *(1 + logvar - mu.pow(2) - logvar.exp())
    
    if is_timeseries:
        if time_series_aggregation_mode == 'mean':
            kl = torch.mean(kl, dim=2)
        elif time_series_aggregation_mode == 'max':
            kl = torch.max(kl, dim=2)
        elif time_series_aggregation_mode == 'sum':
            kl = torch.sum(kl, dim=2)
        elif time_series_aggregation_mode == None:
            pass
    
    if per_dimension is False:
        kl = torch.sum(kl, dim=1)  
    
    if reduce:
        kl = torch.mean(kl, dim=0)
    
    return kl

def count_populated_dimensions(
    mu: torch.Tensor, 
    logvar: torch.Tensor, 
    threshold: float = 0.05, 
    kl_timeseries_aggregation_mode: str = 'mean', 
    return_idx: bool = False
) -> torch.Tensor:
    """Count number of latent dimensions actively used by the model.
    
    A dimension is considered "populated" or "active" if its KL divergence exceeds
    a threshold. This helps diagnose posterior collapse (when KL â†’ 0 for all dimensions)
    and track how many dimensions the model actually uses.
    
    Args:
        mu: Mean of learned distribution, shape (batch, latent_dim) or (batch, latent_dim, seq_len).
        logvar: Log-variance of learned distribution, same shape as mu.
        threshold: Minimum KL divergence for a dimension to be considered active. Default: 0.05.
        kl_timeseries_aggregation_mode: Aggregation mode for timeseries data ('mean', 'max', 'sum').
            Default: 'mean'.
        return_idx: If True, also return boolean mask of active dimensions. Default: False.
    
    Returns:
        Tuple of (n_dim_populated, idx):
            - n_dim_populated: Number of dimensions with KL > threshold (scalar tensor).
            - idx: Boolean mask of active dimensions (tensor or None if return_idx=False).
    
    Note:
        Computes KL per dimension averaged over batch (per_dimension=True, reduce=True).
        Uses torch.no_grad() for efficiency since this is a diagnostic metric.
    """
    with torch.no_grad():
        kl = kullback_leibler(mu, logvar, per_dimension = True, reduce = True, time_series_aggregation_mode = kl_timeseries_aggregation_mode)
        # print histogram of kl divergence to terminal
        # print('kl divergence histogram: 0.0 - 2.0')
        # print(torch.histc(kl, bins=21, min=0, max=2.0))
        idx = kl > threshold
        n_dim_populated = torch.sum(idx)
    if return_idx:
        return n_dim_populated.detach(), idx.detach()
    else:
        return n_dim_populated.detach(), None