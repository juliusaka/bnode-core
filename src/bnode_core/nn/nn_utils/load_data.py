"""Dataset loading utilities for neural network training.

Provides functions to load HDF5 datasets and their configurations,
and create PyTorch-compatible dataset objects for training.

Attention:
    This documentation is generated by AI. Please be aware of possible inaccurcies.
"""

import h5py
import torch
from pathlib import Path
import logging
from omegaconf import OmegaConf
from bnode_core.config import base_pModelClass, train_test_config_class
from bnode_core.filepaths import filepath_dataset_config_from_name, filepath_dataset_from_config
from typing import Tuple, Optional, Union

def load_validate_dataset_config(path: Path) -> base_pModelClass:
    """Load and validate dataset configuration from YAML file.

    Args:
        path: Path to the dataset configuration YAML file.
        
    Returns:
        Validated dataset configuration as base_pModelClass instance.
        
    Raises:
        FileNotFoundError: If configuration file doesn't exist.
    
    Note:
        Uses OmegaConf to load YAML and validates against base_pModelClass schema.
    """

    if not path.exists():
        raise FileNotFoundError('Dataset config file not found: {}'.format(path))

    logging.info('Loading dataset config file: {}'.format(path))
    _dataset_config_dict = OmegaConf.load(path)
    _dataset_config_dict = OmegaConf.to_object(_dataset_config_dict) # make dict
    dataset_config = base_pModelClass(**_dataset_config_dict) # validate
    logging.info('Validated dataset config file: {}'.format(path))
    return dataset_config


def load_dataset_and_config(dataset_name: str, dataset_path: str) -> Tuple[h5py.File, Optional[base_pModelClass]]:
    """Load HDF5 dataset and its configuration.
    
    Loads the HDF5 dataset file and attempts to load its configuration.
    If configuration file doesn't exist, returns None for config.
    
    Args:
        dataset_name: Name/identifier of the dataset.
        dataset_path: Explicit path to dataset file, or empty string to use default location.
    
    Returns:
        Tuple of (dataset, dataset_config) where:

            - dataset: Open h5py.File handle to HDF5 dataset.
            - dataset_config: Validated configuration (base_pModelClass) or None if not found.
    
    Note:
        The returned h5py.File should be closed when done (dataset.close()).
        Uses filepath_dataset_from_config to resolve actual file path.
    """
    _path = filepath_dataset_from_config(dataset_name, dataset_path)

    dataset = h5py.File(_path, 'r')
    logging.info('Loaded dataset from file: {}'.format(_path))

    _path = filepath_dataset_config_from_name(dataset_name)
    if not _path.exists():
        logging.info('No dataset config file found, using information from dataset file')
        dataset_config = None
    else:
        dataset_config = load_validate_dataset_config(_path)
    return dataset, dataset_config

def make_stacked_dataset(
    dataset: h5py.File, 
    context: str, 
    seq_len_from_file: Optional[int] = None, 
    seq_len_batches: Optional[int] = None
) -> Union[torch.utils.data.StackDataset, 'TimeSeriesDataset']:
    """Create a PyTorch dataset from HDF5 data with optional time series batching.
    
    Loads time series data (states, derivatives, parameters, controls, outputs) from an HDF5
    file and wraps it in a PyTorch StackDataset. If seq_len_batches is specified, returns a
    TimeSeriesDataset that enables sliding window sampling for variable-length sequences.
    
    Args:
        dataset (h5py.File): Open HDF5 file containing time series data with groups for
            different contexts (train/test/validation).
        context (str): Dataset context to load. Must be one of: 'train', 'test', 'validation',
            'common_test', or 'common_validation'.
        seq_len_from_file (int, optional): If provided, truncates loaded sequences to this length
            from the original file data. Defaults to None (use full sequence length).
        seq_len_batches (int, optional): If provided, returns a TimeSeriesDataset that extracts
            subsequences of this length via sliding window. If None, returns standard StackDataset
            with full sequences. Defaults to None.
    
    Returns:
        torch.utils.data.StackDataset or TimeSeriesDataset: A dataset that yields dictionaries
            containing tensors for 'time', 'states', and optionally 'states_der', 'parameters',
            'controls', and 'outputs'. Each tensor has shape (batch, channels, time_steps).
    
    Note:
        - All None-valued arrays are automatically excluded from the returned dataset
        - Time tensor is replicated across batch dimension from single time vector
        - When seq_len_batches is used, the dataset length increases to accommodate all
          possible sliding windows across the original sequences
    """
    assert context in ['train', 'test', 'validation', 'common_test', 'common_validation'], 'context must be one of train, test, validation, common_test, common_validation'

    # get tensors of dataset
    time = dataset['time'][:]
    states = dataset[context]['states'][:]
    states_der = dataset[context]['states_der'][:] if 'states_der' in dataset[context].keys() else None
    parameters = dataset[context]['parameters'][:] if 'parameters' in dataset[context].keys() else None
    controls = dataset[context]['controls'][:] if 'controls' in dataset[context].keys() else None
    outputs = dataset[context]['outputs'][:] if 'outputs' in dataset[context].keys() else None

    # cut data from file to seq_len
    if seq_len_from_file is not None:
        time = time[:seq_len_from_file]
        states = states[:,:,:seq_len_from_file]
        states_der = states_der[:,:,:seq_len_from_file] if states_der is not None else None # TODO: add finite difference calculation for states_der and cfg.dataset_prep entries to say if derivatives are included
        parameters = parameters[:] if parameters is not None else None
        controls = controls[:,:,:seq_len_from_file] if controls is not None else None
        outputs = outputs[:,:,:seq_len_from_file] if outputs is not None else None

    # define wrapper to delete nones from kwargs dict
    def _delete_nones(**kwargs):
        kwargs = dict((k,v) for k,v in kwargs.items() if v is not None)
        return kwargs

    # make torch dataset with dict as output
    dataset_type = torch.utils.data.StackDataset if seq_len_batches is None else lambda *args, **kwargs: TimeSeriesDataset(seq_len_batches, *args, **kwargs)
    torch_dataset = dataset_type(
        **_delete_nones(
            time = torch.tensor(time, dtype=torch.float32).unsqueeze(0).expand(states.shape[0], -1).unsqueeze(1),
            states = torch.tensor(states, dtype=torch.float32),
            states_der = torch.tensor(states_der, dtype=torch.float32) if states_der is not None else None,
            parameters = torch.tensor(parameters, dtype=torch.float32) if parameters is not None else None,
            controls = torch.tensor(controls, dtype=torch.float32) if controls is not None else None,
            outputs = torch.tensor(outputs, dtype=torch.float32) if outputs is not None else None,
        )
    )
    logging.info('Created {} with {} and sequence length {}'.format(type(torch_dataset),torch_dataset.datasets.keys(), torch_dataset.datasets['time'].shape[2]))
    return torch_dataset

class TimeSeriesDataset(torch.utils.data.StackDataset):
    """Dataset for time series with sliding window sampling of variable-length subsequences.
    
    Extends StackDataset to enable extracting subsequences from longer time series via a
    sliding window approach. The full sequences are stored internally, but __getitem__
    returns only a subsequence of specified length. This enables training on different
    sequence lengths without reloading data, and increases effective dataset size by
    treating each sliding window position as a separate sample.
    
    The dataset expects dict-style data with a 'time' key, where all time series have shape
    (n_samples, n_channels, n_timesteps). Non-time-series data (2D) is replicated across
    all windows from the same sample.
    
    Attributes:
        seq_len (int): Length of subsequences returned by __getitem__.
        mapping (list): List of [sample_idx, start_pos, end_pos] tuples defining each
            sliding window position across all samples.
        _length (int): Total number of sliding windows (dataset length).
        _length_old (int): Original number of samples before windowing.
    """
    def __init__(self, seq_len: int, *args, **kwargs):
        """Initialize TimeSeriesDataset with sliding window parameters.
        
        Args:
            seq_len (int): Length of subsequences to extract. If larger than available
                time series length, will be clamped to maximum available length.
            *args: Positional arguments passed to parent StackDataset.
            **kwargs: Keyword arguments passed to parent StackDataset. Must result in
                a dict-style dataset with a 'time' key.
        
        Raises:
            AssertionError: If datasets is not a dict or lacks 'time' key.
        """
        super().__init__(*args, **kwargs)
        assert isinstance(self.datasets, dict), "can only handle dict style stacked datasets with one key-value pair time"
        assert 'time' in self.datasets.keys(), "need one dataset with key time to define the map" 
        self._length_old = self._length
        if seq_len > self.datasets['time'].shape[2]:
            Warning("seq_len is {}, setting to len of timeseries".format(seq_len))
            seq_len = self.datasets['time'].shape[2]
        self.seq_len = seq_len
        self.initialize_map(seq_len)

    def set_seq_len(self, seq_len: int):
        """Change the subsequence length and rebuild the sliding window mapping.
        
        Args:
            seq_len (int): New subsequence length. If None, 0, or larger than available
                time series length, will be clamped to maximum available length.
        """
        if seq_len == None or seq_len == 0 or seq_len > self.datasets['time'].shape[2]:
            Warning("seq_len is {}, setting to len of timeseries".format(seq_len))
            seq_len = self.datasets['time'].shape[2]
        self.seq_len = seq_len
        self.initialize_map(seq_len)

    def initialize_map(self, seq_len: int):
        """Create the sliding window index mapping for all samples.
        
        Builds a mapping list where each entry [sample_idx, start_pos, end_pos] defines
        a sliding window position. Windows slide by 1 timestep across each sample, then
        continue to the next sample. This treats each window position as an independent
        dataset item.
        
        Args:
            seq_len (int): Length of sliding windows. Must be at least 1.
        
        Raises:
            AssertionError: If seq_len < 1.
        
        Side Effects:
            - Sets self.mapping to list of [sample, start, end] tuples
            - Updates self._length to total number of windows across all samples
        """
        assert seq_len > 0, "seq_len must be at least 1"
        # define map
        n_batches_per_sample = (self.datasets['time'].shape[2] - (seq_len - 1))
        n_batches_total = n_batches_per_sample * self._length_old
        self.mapping = [[] for i in range(n_batches_total)]
        self._length = n_batches_total
        # fill out map. mapping shall contain [n_sample, start_position, stop_position]
        k_stop=seq_len # stop position in sequence
        j=0 # sample position in datasets
        for i in range(n_batches_total):
            self.mapping[i] = [j, k_stop-seq_len, k_stop]
            if k_stop + 2 > n_batches_per_sample + seq_len: # if the over next sequence would be out of bounds, go to next sample (+1 more because of > and not >=)
                k_stop = seq_len
                j += 1
            else:
                k_stop += 1
        #self.mapping = torch.tensor(np.array(self.mapping), dtype=torch.int64)


    def __getitem__(self, index: int) -> dict:
        """Get a single sliding window sample.
        
        Args:
            index (int): Index of the sliding window to retrieve.
        
        Returns:
            dict: Dictionary with same keys as self.datasets. For 3D+ arrays (time series),
                returns subsequence [start:end] from appropriate sample. For 2D arrays,
                returns full array for the sample.
        """
        i, k_start, k_stop = self.mapping[index]
        ret_val = {}
        for key, value in self.datasets.items():
            if value.ndim == 2:
                ret_val[key] = value[i, :]
            else:
                ret_val[key] = value[i, :, k_start:k_stop]
        return ret_val
    
    def __getitems__(self, indices: list) -> list:
        """Get multiple sliding window samples (batch retrieval).
        
        Args:
            indices (list): List of window indices to retrieve.
        
        Returns:
            list: List of dictionaries, one per index, each containing the requested
                sliding window data.
        
        Note:
            This method requires PyTorch >= 2.2 for optimal batched data loading.
        """
        samples = [None] * len(indices)
        for i, index in enumerate(indices):
            samples[i] = self.__getitem__(index)
        return samples
    
    def __len__(self) -> int:
        """Return the total number of sliding windows in the dataset.
        
        Returns:
            int: Total number of windows across all samples.
        """
        return self._length