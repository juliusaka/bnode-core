{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"bnode-core","text":"<p>Balanced Neural ODEs - Core Library</p>"},{"location":"#description","title":"Description","text":"<p>This repository contains code implementing \"Balanced Neural ODEs\" (BNODEs) as described in the paper:</p> <ul> <li>Julius Aka, Johannes Brunnemann, J\u00f6rg Eiden, Arne Speerforck, Lars Mikelsons. \"Balanced Neural ODEs: nonlinear model order reduction and Koopman operator approximations\", ICLR 2025. OpenReview (bibtex key)</li> </ul> <p>Balanced Neural ODEs (BNODEs) are a data-driven method to learn reduced-order models for high-dimensional dynamical systems. The approach combines Variational Autoencoders (VAEs) with Neural Ordinary Differential Equations (Neural ODEs) to learn a low-dimensional latent representation of the system's dynamics.</p> <p> </p> <p>It consits of the following main components:</p> <ul> <li>Encoder: Maps high-dimensional input data to a low-dimensional latent space. Distinct encoders are use for<ul> <li>(physical) parameter encoding (if applicable)</li> <li>Control input encoding (if applicable)</li> <li>Initial state encoding (intended to be used for simulation data with acces on the full state)</li> </ul> </li> <li>Neural ODE: Models the dynamics in the latent space using a neural network to parameterize the ODE.</li> <li>Decoder: Reconstructs the high-dimensional data from the latent representation.</li> </ul> <p>Each component can be set to be a nonlinear neural network or to be a purely linear model, allowing for flexibility in model complexity. For example, using linearity in the control encoder, the Neural ODE and the decoder results in a Koopman operator approximation for model-predictive control. </p> <p>The main features of the package this repository implements are:</p> <ul> <li>Dataset generation: Functions to generate dataset from physical models provided as FMU (Functional Mock-up Unit, FMI standard) with different sampling methods.</li> <li>Model training: Architecture implementation of BNODEs and state space NeuralODEs using PyTorch and torchdiffeq. Both models can be trained using the same trainer, facilitating a lot of special considerations needed for training Neural ODEs.</li> <li>Various utilties for enabling an efficient workflow, e.g. logging with mlflow, configuration management with hydra and a simple GUI for visualizing training results.</li> </ul>"},{"location":"#using-in-your-project","title":"Using in your project","text":"<p>To only use the package for data generation, you can install it with uv without extra <code>bnode-core</code>. If you want to use the training features, install torch (see below) first. Then add it with the <code>bnode-core[torch]</code> extra.</p>"},{"location":"#installation","title":"Installation","text":"<p>e.g. for running the examples in the <code>examples/</code> folder.</p> <p>TODO: add examples</p> <ol> <li> <p>Clone the repository:</p> <pre><code>git clone &lt;repository-url&gt;\ncd bnode-core\ngit submodule update --init #no need for recursive update\n</code></pre> </li> <li> <p>Install uv, a very-fast python package manager.</p> <p>Then, create a virtual environment:</p> <pre><code>uv venv \n</code></pre> </li> <li> <p>Install Torch: Depending on your hardware (available accelerators, e.g. CUDA ), install the appropriate version of PyTorch. UV does not support automatic backend selection in the default \"uv run / uv sync\" command yet. But the uv pip interface (replacement for commonly used pip commands) does support automatic backend selection.</p> <p>So you can install torch with</p> <pre><code>uv pip install torch torchvision torchaudio --torch-backend auto\n</code></pre> <p>before using <code>uv sync</code> / <code>uv run</code>. You can also manually select the appropriate command from uv doc.</p> <p>You're done!</p> </li> <li> <p>(Optional:) Use the virtual environment:</p> <p>Run </p> <pre><code>uv sync\n</code></pre> <p>at the first time (no 'uv run' command before) to install a virtual environment of the project defined in pyproject.toml. You can also use <code>uv sync</code> to test if the package can be installed in the way you specified it. </p> <p>To activate the virtual environment, use:</p> <pre><code>```\n[linux-bash]\nsource .venv/bin/activate\n[windows-powershell]\n.venv\\Scripts\\Activate\n```\n</code></pre> <p>in your terminal or add it in VS Code using the command palette (Ctrl+Shift+P) and searching for \"Python: Select Interpreter\".</p> </li> </ol> <p>You don't need to use the virtual environment, you can simply place <code>uv run</code> in front of the python-file you want to run to make it run in the specified environment.</p>"},{"location":"#usage","title":"Usage","text":""},{"location":"#documentation","title":"Documentation","text":"<p>To see the documentation, run:</p> <pre><code>make doc\n</code></pre> <p>or, if you don't have make installed,</p> <pre><code>uvx --with mkdocstrings  --with mkdocs-material --with mkdocstrings-python --with mkdocs-include-markdown-plugin mkdocs serve\n</code></pre> <p>and open the website shown in the terminal.</p> <p>(When deploying this on GitHub, the github action will automatically build and publish the documentation to GitHub pages.)</p>"},{"location":"#mlflow-tracking","title":"MLflow Tracking","text":"<p>For training, you don't need to start a MLflow tracking server, as the training scripts will automatically create a local folder <code>mlruns/</code> to log the training data.</p> <p>If you want to access the training data from other machines, you can run: <pre><code>uvx --from 'mlflow&gt;3.5.0,&lt;3.6.0' mlflow ui\n</code></pre> and access the server at <code>http://localhost:5000</code>.</p> <p>To start a dedicated MLflow tracking server, run:</p> <p><pre><code>mkdir -p mlflow\nuvx --from 'mlflow&gt;3.5.0,&lt;3.6.0' mlflow server --port 5001 --backend-store-uri sqlite:///mlflow/mlruns.db --artifacts-destination ./mlflow/mlartifacts --serve-artifacts\n</code></pre> This will start a MLflow tracking server at <code>http://localhost:5001</code>, storing the experiment and run metadata in an sqlite database located at <code>./mlflow/mlruns.db</code> and the artifacts (e.g. model files) in <code>./mlflow/mlartifacts</code>.</p> <p>For better performance, an sqlite database is recommended for the backend store uri, e.g. <code>--backend-store-uri sqlite:///mlflow.db</code>.  You don't need to do this for smaller projects, but for larger projects with many experiments and runs, this improves mlflow's performance significantly.</p> <p>For migrating existing data from a local <code>mlruns/</code> folder to the tracking server, you would need to write your own script.</p> <p>Environment variables of the tracking server are listed on the MLflow documentation.</p>"},{"location":"#package-structure","title":"Package Structure","text":"<p>The package is structured as follows:</p> <pre><code>\u2514\u2500\u2500\u2500bnode_core\n    \u2502   config.py\n    \u2502   filepaths.py\n    \u2502\n    \u251c\u2500\u2500\u2500data_generation\n    \u2502       data_preperation.py\n    \u2502       raw_data_generation.py\n    \u2502\n    \u251c\u2500\u2500\u2500ode\n    \u2502       bnode.py\n    \u2502       node.py\n    \u2502       trainer.py\n    \u2502\n    \u251c\u2500\u2500\u2500plots\n    \u2502\n    \u2514\u2500\u2500\u2500utils\n</code></pre>"},{"location":"#support","title":"Support","text":"<p>If you have questions or issues, please open an issue on GitHub. You can also reach out to me via email, see Authors.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>You are welcome to contribute further test models to this project. Please fork the repository and create a pull request with your changes.</p>"},{"location":"#development-setup","title":"Development Setup","text":"<p>For the development setup, Make should be installed. What does make do?  For us, this is just a convenient way to run common tasks.</p> <p>For installing <code>make</code> on Windows, download the setup from this website and add the make.exe to your PATH environment variable.  - (Usually, this is <code>C:\\Program Files (x86)\\GnuWin32\\bin</code>, add this to the PATH variable in the system environment variables, system properties --&gt; advanced system settings --&gt; environment variables, then restart your terminal).</p> <p>For linux, install make via your package manager, e.g. <code>sudo apt install make</code>.</p> <p>Afterwards, you can use the commands in the Makefile, e.g. <code>make check</code> to check the code style, <code>make format</code> to format the code, or <code>make test</code> to run the tests.</p>"},{"location":"#development-features","title":"Development Features","text":"<ul> <li>Continuous Integration <code>make allci</code><ul> <li>Ruff for linting <code>make check</code></li> <li>Ruff for formatting <code>make format</code></li> <li>Ty for type checking <code>make type</code></li> <li>Pytest for testing <code>make test</code></li> <li>Pytest-cov for testing coverage <code>make cov</code></li> <li>Pre-commit hooks to make some checks and formatting code before commits <code>make commit</code></li> </ul> </li> <li>Documentation<ul> <li>Mkdocs for documentation building with Markdown <code>make doc</code></li> <li>Using mkdocs-material as theme</li> <li>Automatic build of the API Reference page</li> <li>Docstrings are in Google style and used in mkdocs using mkdocstrings</li> <li>Pre-configured GitHub Action / Gitlab CI for publishing the documentation on Github pages / Gitlab page</li> </ul> </li> <li>see modern-python-boilerplate for including Docker, packaging, publishing to PyPI, etc. (we don't need this features yet)</li> </ul>"},{"location":"#authors","title":"Authors","text":"<ul> <li>Julius Aka,  Chair of Mechatronics,  University of Augsburg, julius.aka@uni-a.de</li> <li>Johannes Brunnemann</li> <li>J\u00f6rg Eiden</li> <li>Arne Speerforck</li> <li>Lars Mikelsons</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you use this code in your research, please cite the following paper: <pre><code>@inproceedings{\naka2025balanced,\ntitle={Balanced Neural {ODE}s: nonlinear model order reduction and Koopman operator approximations},\nauthor={Julius Aka and Johannes Brunnemann and J{\\\"o}rg Eiden and Arne Speerforck and Lars Mikelsons},\nbooktitle={The Thirteenth International Conference on Learning Representations},\nyear={2025},\nurl={https://openreview.net/forum?id=nA464tCGR5}\n}\n</code></pre></p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<p>The repository structure is inspired by modern-python-boilerplate.</p>"},{"location":"LICENSE/","title":"License","text":"<p>Copyright 2025 Julius Aka</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u201cSoftware\u201d), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \u201cAS IS\u201d, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"reference/","title":"API Reference","text":"<p>This section collects the API for <code>bnode-core</code> and a lot of information on how to use the code base, generated directly from the source using mkdocstrings. </p> <p>If you are new to the project, start with the guide in Home proceeded with information in Data Generation, before proceeding to BNODE Training. Of great important in this project are configuration files as described in Configuration.</p>"},{"location":"bnode_core/config/","title":"Configuration","text":""},{"location":"bnode_core/config/#bnode_core.config","title":"<code>bnode_core.config</code>","text":""},{"location":"bnode_core/config/#bnode_core.config--b-node-project-configuration-module","title":"B-NODE Project Configuration Module","text":"<p>This module defines the configuration schemas for the B-NODE project using Pydantic dataclasses. It encompasses various settings related to solver configurations, raw data generation, dataset preparation, and multiple neural network model families.</p>"},{"location":"bnode_core/config/#bnode_core.config--overview","title":"Overview","text":"<p>The module provides:</p> <ul> <li>Typed, validated dataclasses for solver settings, raw-data generation, dataset preparation, and various neural network model families (base, VAE, neural-ODE, latent-ODE).</li> <li>Field- and model-level validation through Pydantic's functional and model validators.</li> <li>Utilities for registering configuration dataclasses with Hydra's ConfigStore.</li> <li>Functions to convert OmegaConf DictConfig into corresponding validated Python dataclasses and to persist dataclasses as YAML files.</li> </ul>"},{"location":"bnode_core/config/#bnode_core.config--configuration-structure","title":"Configuration Structure","text":"<p>The configuration is organized into several key components:</p> <ul> <li>base_data_gen: Configuration for data generation, including model and dataset preparation settings.</li> <li>base_train_test: Configuration for training and testing neural network models, allowing for various model types.</li> <li>base_onnx_export: Configuration for exporting models to ONNX format.</li> </ul>"},{"location":"bnode_core/config/#bnode_core.config--utilities","title":"Utilities","text":"<p>The module includes the following utility functions:</p> <ul> <li><code>convert_cfg_to_dataclass(cfg: DictConfig) -&gt; dataclass</code>: Converts an OmegaConf DictConfig into the corresponding validated Python dataclass.</li> <li><code>save_dataclass_as_yaml(cfg: dataclass, path: str)</code>: Persists a dataclass to a YAML file.</li> </ul>"},{"location":"bnode_core/config/#bnode_core.config--dataclass-definitions","title":"Dataclass Definitions","text":"<p>The following dataclasses are defined in this module:</p> <ul> <li><code>SolverClass</code>: Configuration for simulation timing and solver behavior.</li> <li><code>RawDataClass</code>: Configuration for raw model and sampling settings used to generate datasets.</li> <li><code>base_dataset_prep_class</code>: Settings for dataset preparation, including slicing, filtering, and transforming data.</li> </ul> <p>Each dataclass includes detailed attributes and validation rules to ensure proper configuration and usage.</p> <p>Pydantic dataclass configuration schema used by the B-NODE project.</p> <p>This module provides:</p> <ul> <li>Typed, validated dataclasses for solver settings, raw-data generation,     dataset preparation and multiple neural-network model families (base, VAE,     neural-ODE, latent-ODE).</li> <li>Field- and model-level validation via pydantic functional and model validators.</li> <li>Helpers to register all config dataclasses with Hydra's ConfigStore.</li> <li>Small utilities to convert an OmegaConf DictConfig into the corresponding     Python dataclass and to persist dataclasses as YAML.</li> </ul>"},{"location":"bnode_core/config/#bnode_core.config--notes","title":"Notes","text":"<ul> <li> <p>Use get_config_store() at program startup to register the configuration     schemas with Hydra. Example:</p> <pre><code>    from bnode_core.config import get_config_store\n    cs = get_config_store()\n</code></pre> </li> <li> <p>The dataclasses are intended to be composed/loaded via Hydra + OmegaConf and     validated through pydantic validators. See each dataclass docstring for     details about available fields and validation behaviour.</p> </li> </ul>"},{"location":"bnode_core/config/#bnode_core.config--configstore-layout-high-level","title":"ConfigStore layout (high level)","text":"<pre><code>base_data_gen: data_gen_config(\n        pModel = base_pModelClass(\n                RawData = RawDataClass,\n                dataset_prep = base_dataset_prep_class\n        )\n)\n\nbase_train_test: train_test_config_class(\n        nn_model = one of:\n            - base_nn_model: base_nn_model_class\n            - pels_vae: base_nn_model_class(network=pels_vae_network_class, training=pels_vae_training_settings_class)\n            - neural_ode_base: base_ode_nn_model_class(network=neural_ode_network_class, training=base_neural_ode_training_settings_class)\n            - latent_ode_base: base_latent_ode_nn_model_class(network=latent_ode_network_class, training=base_latent_ode_training_settings_class)\n)\n\nbase_onnx_export: onnx_export_config_class  (inherits load_latent_ode_config_class)\n</code></pre>"},{"location":"bnode_core/config/#bnode_core.config--utilities_1","title":"Utilities","text":"<ul> <li> <p><code>convert_cfg_to_dataclass(cfg: DictConfig) -&gt; dataclass</code>     Convert an OmegaConf DictConfig (Hydra config) into the corresponding     validated Python dataclass (uses OmegaConf.to_object and pydantic validation).</p> </li> <li> <p><code>save_dataclass_as_yaml(cfg: dataclass, path: str)</code>     Persist a dataclass to a YAML file (uses pydantic/dataclasses.asdict then yaml.dump).</p> </li> </ul> <p>See individual dataclass definitions below for field-level documentation, validation rules and \"gotchas\".</p>"},{"location":"bnode_core/config/#bnode_core.config.SolverClass","title":"<code>SolverClass</code>","text":"<p>Configuration for simulation timing and solver behavior.</p> <p>Notes:</p> <ul> <li>sequence_length is auto-computed from start/end/timestep during model validation.</li> <li>Changing simulationStartTime, simulationEndTime, or timestep will recompute sequence_length.</li> <li>all times are in seconds</li> <li>the timeout variable aborts the simulation if it runs longer than the specified time (in seconds) in the raw data generation. If its None, no timeout is set.</li> </ul> <p>Attributes:</p> Name Type Description <code>simulationStartTime</code> <code>float</code> <p>Start time of the simulation in seconds.</p> <code>simulationEndTime</code> <code>float</code> <p>End time of the simulation in seconds.</p> <code>timestep</code> <code>float</code> <p>Fixed integration step size in seconds.</p> <code>tolerance</code> <code>float</code> <p>Solver tolerance used to run the co-sim FMU.</p> <code>sequence_length</code> <code>int</code> <p>Number of steps including the initial state; computed after validation.</p> <code>timeout</code> <code>Optional[float]</code> <p>Max wall-clock seconds allowed for raw-data simulation; None disables.</p> Source code in <code>src/bnode_core/config.py</code> <pre><code>@dataclass\nclass SolverClass:\n    \"\"\"\n    Configuration for simulation timing and solver behavior.\n\n    Notes:\n\n    - sequence_length is auto-computed from start/end/timestep during model validation.\n    - Changing simulationStartTime, simulationEndTime, or timestep will recompute sequence_length.\n    - all times are in seconds\n    - the timeout variable aborts the simulation if it runs longer than the specified time (in seconds) in the raw data generation. If its None, no timeout is set.\n\n    Attributes:\n        simulationStartTime (float): Start time of the simulation in seconds.\n        simulationEndTime (float): End time of the simulation in seconds.\n        timestep (float): Fixed integration step size in seconds.\n        tolerance (float): Solver tolerance used to run the co-sim FMU.\n        sequence_length (int): Number of steps including the initial state; computed after validation.\n        timeout (Optional[float]): Max wall-clock seconds allowed for raw-data simulation; None disables.\n    \"\"\"\n    simulationStartTime: float = 0.0\n    simulationEndTime: float = 1.0\n    timestep: float = 0.1\n    tolerance: float = 1e-6\n    sequence_length: int = None\n    timeout: Optional[float] = None # in seconds, if None, no timeout is set\n\n    @model_validator(mode='after')\n    def calculate_sequence_length(self):\n        # +1 because of the initial state, ceil to make sure that the last timestep is included\n        v = np.ceil((self.simulationEndTime - self.simulationStartTime) / self.timestep) + 1 \n        logging.info(f'sequence_length has been set to {int(v)}')\n        self.sequence_length = int(v)\n        return self\n</code></pre>"},{"location":"bnode_core/config/#bnode_core.config.RawDataClass","title":"<code>RawDataClass</code>","text":"<p>Raw model and sampling configuration used to generate datasets.</p> <p>Gotchas:</p> <ul> <li>fmuPath is required (and must end with .fmu) when raw_data_from_external_source is False.</li> <li>parameters entries must be length-3 lists [min, max, default]; if scalar/len-1, bounds are auto-derived   using parameters_default_lower_factor/parameters_default_upper_factor and default must lie within bounds.</li> <li>states/controls entries must be length-2 lists [min, max]; if None, class defaults are injected; lower &lt;= upper.</li> <li>When controls_sampling_strategy is ROCS/RROCS, both controls_frequency_min_in_timesteps and   controls_frequency_max_in_timesteps are required and min &lt;= max.</li> <li>When controls_sampling_strategy is 'file' or 'constantInput', controls_file_path must exist; otherwise it is ignored.</li> <li>If initial_states_include is True, dataset_prep.start_time must equal Solver.simulationStartTime (checked in base_pModelClass).      This is because initial states are sampled at the simulation start time, and excluding these times would not make sense.</li> </ul> <p>Attributes:</p> Name Type Description <code>raw_data_from_external_source</code> <code>bool</code> <p>If True, use external raw data and ignore FMU-related fields.</p> <code>raw_data_path</code> <code>Optional[str]</code> <p>Path to an existing raw-data file to consume.</p> <code>modelName</code> <code>str</code> <p>Model identifier/name.</p> <code>fmuPath</code> <code>Optional[str]</code> <p>Path to a .fmu file; required if raw_data_from_external_source is False.</p> <code>versionName</code> <code>str</code> <p>Version tag for the model/dataset.</p> <code>states_default_lower_value</code> <code>float</code> <p>Default lower bound used when a state range is None.</p> <code>states_default_upper_value</code> <code>float</code> <p>Default upper bound used when a state range is None.</p> <code>states</code> <code>Optional[Dict]</code> <p>Mapping state-name -&gt; [min, max] or None to inject defaults; list must have length 2 and min &lt;= max. E.g. {'s1': [0, 10], 's2': None}.</p> <code>initial_states_include</code> <code>bool</code> <p>Whether to include initial state sampling in data generation.</p> <code>states_der_include</code> <code>bool</code> <p>Whether to save state derivatives from the fmu, found by searching for 'der(%state_name)'.</p> <code>initial_states_sampling_strategy</code> <code>str</code> <p>Strategy for initial-state sampling (e.g., 'R' for random). See raw_data_generation.py for options.</p> <code>parameters_default_lower_factor</code> <code>float</code> <p>Factor to derive default lower bound from a scalar parameter default.</p> <code>parameters_default_upper_factor</code> <code>float</code> <p>Factor to derive default upper bound from a scalar parameter default.</p> <code>parameters</code> <code>Optional[Dict]</code> <p>Mapping parameter-name -&gt; [min, max, default] or scalar; list must have length 3 and default within [min, max].</p> <code>parameters_include</code> <code>bool</code> <p>Whether to include parameter sampling in data generation.</p> <code>parameters_sampling_strategy</code> <code>str</code> <p>Strategy for parameter sampling (e.g., 'R'). See raw_data_generation.py for options.</p> <code>controls_default_lower_value</code> <code>float</code> <p>Default lower bound for a control when unspecified.</p> <code>controls_default_upper_value</code> <code>float</code> <p>Default upper bound for a control when unspecified.</p> <code>controls</code> <code>Optional[Dict]</code> <p>Mapping control-name -&gt; [min, max], [min, max, clip_lower, clip_upper] or None to inject defaults. If 4 elements, clip values are applied after sampling.</p> <code>controls_include</code> <code>bool</code> <p>Whether to include controls in data generation.</p> <code>controls_sampling_strategy</code> <code>str</code> <p>Strategy for control sampling (e.g., 'R', 'ROCS', 'RROCS', 'file', 'constantInput').</p> <code>controls_frequency_min_in_timesteps</code> <code>Optional[int]</code> <p>Minimum control hold-frequency in time steps (required with ROCS/RROCS).</p> <code>controls_frequency_max_in_timesteps</code> <code>Optional[int]</code> <p>Maximum control hold-frequency in time steps (required with ROCS/RROCS; must be &gt;= min).</p> <code>controls_file_path</code> <code>Optional[str]</code> <p>CSV/Excel path with time and control columns when using 'file'/'constantInput'; must exist.</p> <code>controls_only_for_sampling_extract_actual_from_model</code> <code>bool</code> <p>Use controls only for sampling while extracting actual controls from the model.</p> <code>controls_from_model</code> <code>Optional[List[str]]</code> <p>Names of controls to read from the model if available.</p> <code>outputs</code> <code>Optional[List[str]]</code> <p>List of variable names to be treated as outputs.</p> <code>Solver</code> <code>SolverClass</code> <p>Nested solver configuration used for time grid and lengths.</p> <code>n_samples</code> <code>int</code> <p>Number of trajectories to generate.</p> <code>creation_date</code> <code>Optional[str]</code> <p>Optional creation date string for bookkeeping.</p> Source code in <code>src/bnode_core/config.py</code> <pre><code>@dataclass\nclass RawDataClass:\n    \"\"\"\n    Raw model and sampling configuration used to generate datasets.\n\n    Gotchas:\n\n    - fmuPath is required (and must end with .fmu) when raw_data_from_external_source is False.\n    - parameters entries must be length-3 lists [min, max, default]; if scalar/len-1, bounds are auto-derived\n      using parameters_default_lower_factor/parameters_default_upper_factor and default must lie within bounds.\n    - states/controls entries must be length-2 lists [min, max]; if None, class defaults are injected; lower &lt;= upper.\n    - When controls_sampling_strategy is ROCS/RROCS, both controls_frequency_min_in_timesteps and\n      controls_frequency_max_in_timesteps are required and min &lt;= max.\n    - When controls_sampling_strategy is 'file' or 'constantInput', controls_file_path must exist; otherwise it is ignored.\n    - If initial_states_include is True, dataset_prep.start_time must equal Solver.simulationStartTime (checked in base_pModelClass). \n        This is because initial states are sampled at the simulation start time, and excluding these times would not make sense.\n\n    Attributes:\n        raw_data_from_external_source (bool): If True, use external raw data and ignore FMU-related fields.\n        raw_data_path (Optional[str]): Path to an existing raw-data file to consume.\n        modelName (str): Model identifier/name.\n        fmuPath (Optional[str]): Path to a .fmu file; required if raw_data_from_external_source is False.\n        versionName (str): Version tag for the model/dataset.\n        states_default_lower_value (float): Default lower bound used when a state range is None.\n        states_default_upper_value (float): Default upper bound used when a state range is None.\n        states (Optional[Dict]): Mapping state-name -&gt; [min, max] or None to inject defaults; list must have length 2 and min &lt;= max. E.g. {'s1': [0, 10], 's2': None}.\n        initial_states_include (bool): Whether to include initial state sampling in data generation.\n        states_der_include (bool): Whether to save state derivatives from the fmu, found by searching for 'der(%state_name)'.\n        initial_states_sampling_strategy (str): Strategy for initial-state sampling (e.g., 'R' for random). See raw_data_generation.py for options.\n        parameters_default_lower_factor (float): Factor to derive default lower bound from a scalar parameter default.\n        parameters_default_upper_factor (float): Factor to derive default upper bound from a scalar parameter default.\n        parameters (Optional[Dict]): Mapping parameter-name -&gt; [min, max, default] or scalar; list must have length 3 and default within [min, max].\n        parameters_include (bool): Whether to include parameter sampling in data generation.\n        parameters_sampling_strategy (str): Strategy for parameter sampling (e.g., 'R'). See raw_data_generation.py for options.\n        controls_default_lower_value (float): Default lower bound for a control when unspecified.\n        controls_default_upper_value (float): Default upper bound for a control when unspecified.\n        controls (Optional[Dict]): Mapping control-name -&gt; [min, max], [min, max, clip_lower, clip_upper] or None to inject defaults. If 4 elements, clip values are applied after sampling.\n        controls_include (bool): Whether to include controls in data generation.\n        controls_sampling_strategy (str): Strategy for control sampling (e.g., 'R', 'ROCS', 'RROCS', 'file', 'constantInput').\n        controls_frequency_min_in_timesteps (Optional[int]): Minimum control hold-frequency in time steps (required with ROCS/RROCS).\n        controls_frequency_max_in_timesteps (Optional[int]): Maximum control hold-frequency in time steps (required with ROCS/RROCS; must be &gt;= min).\n        controls_file_path (Optional[str]): CSV/Excel path with time and control columns when using 'file'/'constantInput'; must exist.\n        controls_only_for_sampling_extract_actual_from_model (bool): Use controls only for sampling while extracting actual controls from the model.\n        controls_from_model (Optional[List[str]]): Names of controls to read from the model if available.\n        outputs (Optional[List[str]]): List of variable names to be treated as outputs.\n        Solver (SolverClass): Nested solver configuration used for time grid and lengths.\n        n_samples (int): Number of trajectories to generate.\n        creation_date (Optional[str]): Optional creation date string for bookkeeping.\n    \"\"\"\n    raw_data_from_external_source: bool = False # this sets the code to not use anything else from this class\n    raw_data_path: Optional[str] = None # give path to raw data file in raw_data\n    modelName: str = MISSING\n    fmuPath: Optional[str] = None\n    versionName: str = 'v1'\n    states_default_lower_value: float = -1000.0\n    states_default_upper_value: float = 1000.0\n    states: Optional[Dict] = field(default_factory=dict)\n    initial_states_include: bool = False\n    states_der_include: bool = True\n    initial_states_sampling_strategy: str = 'R'\n    parameters_default_lower_factor: float = 0.2\n    parameters_default_upper_factor: float = 5.0\n    parameters: Optional[Dict] = None\n    parameters_include: bool = False\n    parameters_sampling_strategy: str = 'R'\n    controls_default_lower_value: float = 0.2\n    controls_default_upper_value: float = 5.0\n    controls: Optional[Dict] = field(default_factory=dict)\n    controls_include: bool = False\n    controls_sampling_strategy: str = 'R'\n    controls_frequency_min_in_timesteps: Optional[int] = None\n    controls_frequency_max_in_timesteps: Optional[int] = None\n    controls_file_path: Optional[str] = None # must contains headers with time, control variable names\n    controls_only_for_sampling_extract_actual_from_model: bool = False\n    controls_from_model: Optional[List[str]] = None\n    outputs: Optional[List[str]] = None\n    Solver: SolverClass = field(default_factory=SolverClass)\n    n_samples: int = 2048\n    creation_date: Optional[str] = None\n    @field_validator('fmuPath')\n    @classmethod\n    def check_fmuPath(cls, v, info: ValidationInfo):\n        if v is None:\n            if info.data['raw_data_from_external_source'] == False:\n                raise ValueError('fmuPath must be set if raw_data_from_external_source is False')\n        else:\n            path = Path(v)\n            if path.suffix != '.fmu':\n                raise ValueError('fmuPath must be a .fmu file')\n            if not path.exists():\n                Warning('fmuPath does not exist')\n            return str(path.as_posix())\n    @field_validator('parameters')\n    @classmethod\n    def check_parameters(cls, v, info: ValidationInfo):\n        if v is None:\n            logging.info('no parameters included.')\n        else:\n            for key, value in v.items():\n                if not isinstance(value, list):\n                    value = [value]\n                if len(value) == 1:\n                    v[key] = [value[0] * info.data['parameters_default_lower_factor'], value[0] * info.data['parameters_default_upper_factor'], value[0]]\n                    logging.info(f'parameter ranges {key} has been set to \\t {[round(x, 4) for x in v[key]]}')\n                if not len(v[key]) == 3:\n                    raise ValueError(f'parameter {key} must be a dictionary of lists with 3 elements')\n                if v[key][2] &lt; v[key][0] or v[key][2] &gt; v[key][1]:\n                    raise ValueError(f'parameter {key} default value must be within the lower and upper bounds')\n        return v\n    @field_validator('controls')\n    @classmethod\n    def check_controls(cls, v, info: ValidationInfo):\n        for key, value in v.items():\n            if value is None:\n                v[key] = [info.data['controls_default_lower_value'], info.data['controls_default_upper_value']]\n                logging.info(f'control ranges {key} has been set to \\t {[round(x, 4) for x in v[key]]}')\n            if not ((len(v[key]) == 2) or (len(v[key]) == 4)):\n                raise ValueError(f'control {key} must be a dictionary of lists with 2 or 4 elements')\n            if v[key][0] &gt; v[key][1]:\n                raise ValueError(f'control {key}: first element must be smaller than second element')\n            if len(v[key]) == 4:\n                if v[key][2] &gt; v[key][3]:\n                    raise ValueError(f'control {key}: clip value lower bound must be smaller than upper bound')\n        return v\n    @field_validator('states')\n    @classmethod\n    def check_states(cls,v, info: ValidationInfo):\n        for key, value in v.items():\n            if value is None:\n                v[key] = [info.data['states_default_lower_value'], info.data['states_default_upper_value']]\n                logging.info(f'state ranges {key} has been set to \\t {[round(x, 4) for x in v[key]]}')\n            if not len(v[key]) == 2:\n                raise ValueError(f'state {key} must be a dictionary of lists with 2 elements')\n            if v[key][0] &gt; v[key][1]:\n                raise ValueError(f'state {key}: first element must be smaller than second element')\n        return v\n    @field_validator('controls_frequency_max_in_timesteps')\n    @classmethod\n    def check_controls_frequency(cls, v, info: ValidationInfo):\n        if info.data['controls_sampling_strategy'] == 'ROCS' or info.data['controls_sampling_strategy'] == 'RROCS':\n            if v is None or info.data['controls_frequency_min_in_timesteps'] is None:\n                raise ValueError('controls_frequency_min_in_timesteps and controls_frequency_max_in_timesteps must be set if controls_sampling_strategy is ROCS')\n            if v &lt; info.data['controls_frequency_min_in_timesteps']:\n                raise ValueError('controls_frequency_min_in_timesteps must be smaller than controls_frequency_max_in_timesteps')\n        return v\n    @field_validator('controls_file_path')\n    @classmethod\n    def check_controls_file_path(cls, v, info: ValidationInfo):\n        if info.data['controls_sampling_strategy'] == 'file' or info.data['controls_sampling_strategy'] == 'constantInput':\n            path = Path(v)\n            if not path.exists():\n                raise ValueError('controls_file_path does not exist')\n            return str(path.as_posix())\n        else:\n            return None\n</code></pre>"},{"location":"bnode_core/config/#bnode_core.config.base_dataset_prep_class","title":"<code>base_dataset_prep_class</code>","text":"<p>Dataset preparation settings for slicing, filtering, transforming, and splitting data.</p> <p>Gotchas:</p> <ul> <li>The lists for states/parameters/controls/outputs accept the sentinel value ['all'] to include all available items.</li> <li>filter_trajectories_limits is a dictionary where keys are variable names and values are [min, max] lists to filter out trajectories   that have variable values outside these limits.</li> <li>filter_trajectories_expression contains Python expressions as strings; all expressions in a list must evaluate to True   to filter out a trajectory. WARNING: This is is not tested yet, debug carefully before use.</li> <li>transforms is a dictionary where keys are variable names and values are strings representing Python expressions to transform the data.      example: var1: 'np.log(# + 1)' where # is replaced by the variable data.</li> <li>validation_fraction and test_fraction control split sizes; sequence_length can remain None to use model defaults.</li> </ul> <p>Attributes:</p> Name Type Description <code>dataset_suffix</code> <code>Optional[str]</code> <p>Suffix to append to dataset names/artifacts.</p> <code>n_samples</code> <code>List[int]</code> <p>Samples to include in the dataset, a list with multiple entries will generate multiple datasets, but with same \"common_test\" and \"common_validation\" sets.</p> <code>filter_trajectories_limits</code> <code>Optional[Dict[str, List]]</code> <p>Variable limits for filtering trajectories, each as [min, max].</p> <code>filter_trajectories_expression</code> <code>Optional[Dict[str, List[str]]]</code> <p>Expressions that, when True, mark trajectories for removal. Attention; not tested yet.</p> <code>transforms</code> <code>Optional[Dict[str, str]]</code> <p>Mapping variable -&gt; Python expression where '#' is replaced by the variable data.</p> <code>states</code> <code>Optional[List[str]]</code> <p>Names of states to keep, or ['all'].</p> <code>parameters</code> <code>Optional[List[str]]</code> <p>Names of parameters to keep, or ['all'].</p> <code>parameters_remove</code> <code>bool</code> <p>If True, all parameters are removed even if present in source data.</p> <code>controls</code> <code>Optional[List[str]]</code> <p>Names of controls to keep, or ['all'].</p> <code>outputs</code> <code>Optional[List[str]]</code> <p>Names of outputs to keep, or ['all'].</p> <code>start_time</code> <code>float</code> <p>Start time for slicing trajectories. E.g. used for excluding initial transients of initialization.</p> <code>end_time</code> <code>float</code> <p>End time for slicing trajectories (inf keeps full length).</p> <code>sequence_length</code> <code>Optional[int]</code> <p>Desired sequence length; None keeps source/model default.</p> <code>validation_fraction</code> <code>float</code> <p>Fraction of data reserved for validation split. Must be between 0 and 1. validation_fraction + test_fraction must be &lt; 1.</p> <code>test_fraction</code> <code>float</code> <p>Fraction of data reserved for test split. Must be between 0 and 1. validation_fraction + test_fraction must be &lt; 1.</p> <code>validation_idx_start</code> <code>Optional[int]</code> <p>Index to start validation split, leave empty, is auto-computed in dataset_preperation.py</p> <code>test_idx_start</code> <code>Optional[int]</code> <p>Index to start test split, leave empty, is auto-computed in dataset_preperation.py</p> Source code in <code>src/bnode_core/config.py</code> <pre><code>@dataclass\nclass base_dataset_prep_class:\n    \"\"\"\n    Dataset preparation settings for slicing, filtering, transforming, and splitting data.\n\n    Gotchas:\n\n    - The lists for states/parameters/controls/outputs accept the sentinel value ['all'] to include all available items.\n    - filter_trajectories_limits is a dictionary where keys are variable names and values are [min, max] lists to filter out trajectories\n      that have variable values outside these limits.\n    - filter_trajectories_expression contains Python expressions as strings; all expressions in a list must evaluate to True\n      to filter out a trajectory. WARNING: This is is not tested yet, debug carefully before use.\n    - transforms is a dictionary where keys are variable names and values are strings representing Python expressions to transform the data. \n        example: var1: 'np.log(# + 1)' where # is replaced by the variable data.\n    - validation_fraction and test_fraction control split sizes; sequence_length can remain None to use model defaults.\n\n    Attributes:\n        dataset_suffix (Optional[str]): Suffix to append to dataset names/artifacts.\n        n_samples (List[int]): Samples to include in the dataset, a list with multiple entries will generate multiple datasets, but with same \"common_test\" and \"common_validation\" sets.\n        filter_trajectories_limits (Optional[Dict[str, List]]): Variable limits for filtering trajectories, each as [min, max].\n        filter_trajectories_expression (Optional[Dict[str, List[str]]]): Expressions that, when True, mark trajectories for removal. Attention; not tested yet.\n        transforms (Optional[Dict[str, str]]): Mapping variable -&gt; Python expression where '#' is replaced by the variable data.\n        states (Optional[List[str]]): Names of states to keep, or ['all'].\n        parameters (Optional[List[str]]): Names of parameters to keep, or ['all'].\n        parameters_remove (bool): If True, all parameters are removed even if present in source data.\n        controls (Optional[List[str]]): Names of controls to keep, or ['all'].\n        outputs (Optional[List[str]]): Names of outputs to keep, or ['all'].\n        start_time (float): Start time for slicing trajectories. E.g. used for excluding initial transients of initialization.\n        end_time (float): End time for slicing trajectories (inf keeps full length).\n        sequence_length (Optional[int]): Desired sequence length; None keeps source/model default.\n        validation_fraction (float): Fraction of data reserved for validation split. Must be between 0 and 1. validation_fraction + test_fraction must be &lt; 1.\n        test_fraction (float): Fraction of data reserved for test split. Must be between 0 and 1. validation_fraction + test_fraction must be &lt; 1.\n        validation_idx_start (Optional[int]): Index to start validation split, leave empty, is auto-computed in dataset_preperation.py\n        test_idx_start (Optional[int]): Index to start test split, leave empty, is auto-computed in dataset_preperation.py\n    \"\"\"\n    dataset_suffix: Optional[str] = None\n    n_samples: List[int] = field(default_factory=list)\n    filter_trajectories_limits: Optional[Dict[str, List]] = field(default_factory=dict) \n    # e.g. stratifiedHeatFlow.partition[1].heatCapacitor.T: [10, 30]. \n    filter_trajectories_expression: Optional[Dict[str, List[str]]] = field(default_factory=dict)\n    # str can be used to define a python expression to be evaluated, \n    # the list can contain multiple expressions that all must hold true for the trajectory to be removed\n    transforms: Optional[Dict[str, str]] = field(default_factory=dict)\n    states: Optional[List[str]] = field(default_factory=lambda: ['all'])\n    parameters: Optional[List[str]] = field(default_factory=lambda: ['all'])\n    parameters_remove: bool = False\n    controls: Optional[List[str]] = field(default_factory=lambda: ['all'])\n    outputs: Optional[List[str]] = field(default_factory=lambda: ['all'])\n    start_time: float = 0\n    end_time: float = float('inf')\n    sequence_length: Optional[int] = None\n    validation_fraction: float = 0.12\n    test_fraction: float = 0.12\n    validation_idx_start: Optional[int] = None\n    test_idx_start: Optional[int] = None\n</code></pre>"},{"location":"bnode_core/config/#bnode_core.config.base_pModelClass","title":"<code>base_pModelClass</code>","text":"<p>Composite configuration that couples raw data settings and dataset preparation for a physical model.</p> <p>Gotchas:</p> <ul> <li>If RawData.initial_states_include is True, dataset_prep.start_time must equal RawData.Solver.simulationStartTime.</li> </ul> <p>Attributes:</p> Name Type Description <code>RawData</code> <code>RawDataClass</code> <p>Raw-data generation configuration (FMU, variables, sampling).</p> <code>dataset_prep</code> <code>base_dataset_prep_class</code> <p>Dataset slicing/filtering/transforms and split setup.</p> Source code in <code>src/bnode_core/config.py</code> <pre><code>@dataclass\nclass base_pModelClass:\n    \"\"\"\n    Composite configuration that couples raw data settings and dataset preparation for a physical model.\n\n    Gotchas:\n\n    - If RawData.initial_states_include is True, dataset_prep.start_time must equal RawData.Solver.simulationStartTime.\n\n    Attributes:\n        RawData (RawDataClass): Raw-data generation configuration (FMU, variables, sampling).\n        dataset_prep (base_dataset_prep_class): Dataset slicing/filtering/transforms and split setup.\n    \"\"\"\n    RawData: RawDataClass = field(default_factory=RawDataClass)\n    dataset_prep: base_dataset_prep_class = field(default_factory=base_dataset_prep_class)\n\n    @field_validator('dataset_prep')\n    @classmethod\n    def check_dataset_prep(cls, v, info: ValidationInfo):\n        # check if initial_state_include is set to True, dataset_prep.start_time must be RawData.Solver.simulationStartTime\n        if info.data['RawData'].initial_states_include == True:\n            if v.start_time != info.data['RawData'].Solver.simulationStartTime:\n                raise ValueError('dataset_prep.start_time must be RawData.Solver.simulationStartTime if RawData.initial_states_include is True')\n        return v\n</code></pre>"},{"location":"bnode_core/config/#bnode_core.config.data_gen_config","title":"<code>data_gen_config</code>","text":"<p>Top-level configuration for dataset generation and worker setup.</p> <p>Notes:</p> <ul> <li>memory_limit_per_worker expects strings like '2GiB'. A worker is restricted to this memory limit and restarts if exceeded, see https://distributed.dask.org/en/latest/api.html#distributed.LocalCluster.</li> <li>setting multiprocessing_processes to None lets the backend decide on the number of processes.</li> </ul> <p>Attributes:</p> Name Type Description <code>pModel</code> <code>base_pModelClass</code> <p>Physical-model configuration bundle to generate data for.</p> <code>multiprocessing_processes</code> <code>Optional[int]</code> <p>Number of worker processes; None lets backend choose.</p> <code>memory_limit_per_worker</code> <code>str</code> <p>Dask worker memory limit (e.g., '2GiB').</p> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite existing generated artifacts. Should be False to avoid accidental data loss.</p> Source code in <code>src/bnode_core/config.py</code> <pre><code>@dataclass\nclass data_gen_config:\n    \"\"\"\n    Top-level configuration for dataset generation and worker setup.\n\n    Notes:\n\n    - memory_limit_per_worker expects strings like '2GiB'. A worker is restricted to this memory limit and restarts if exceeded, see https://distributed.dask.org/en/latest/api.html#distributed.LocalCluster.\n    - setting multiprocessing_processes to None lets the backend decide on the number of processes.\n\n    Attributes:\n        pModel (base_pModelClass): Physical-model configuration bundle to generate data for.\n        multiprocessing_processes (Optional[int]): Number of worker processes; None lets backend choose.\n        memory_limit_per_worker (str): Dask worker memory limit (e.g., '2GiB').\n        overwrite (bool): Whether to overwrite existing generated artifacts. Should be False to avoid accidental data loss.\n    \"\"\"\n    pModel: base_pModelClass = MISSING\n    multiprocessing_processes: Optional[int] = None\n    memory_limit_per_worker: str = \"2GiB\" # specifiy as 2GiB, or auto. see https://distributed.dask.org/en/latest/api.html#distributed.LocalCluster\n    overwrite: bool = False\n</code></pre>"},{"location":"bnode_core/config/#bnode_core.config.base_network_class","title":"<code>base_network_class</code>","text":"<p>Common MLP backbone configuration shared by multiple models.</p> <p>Attributes:</p> Name Type Description <code>n_linear_layers</code> <code>int</code> <p>Number of linear layers in the backbone.</p> <code>linear_hidden_dim</code> <code>int</code> <p>Hidden dimension for linear layers.</p> <code>activation</code> <code>str</code> <p>String name of torch.nn activation module (e.g., 'torch.nn.ReLU'), is evaluated during model construction (using eval()).</p> Source code in <code>src/bnode_core/config.py</code> <pre><code>@dataclass \nclass base_network_class:\n    \"\"\"\n    Common MLP backbone configuration shared by multiple models.\n\n    Attributes:\n        n_linear_layers (int): Number of linear layers in the backbone.\n        linear_hidden_dim (int): Hidden dimension for linear layers.\n        activation (str): String name of torch.nn activation module (e.g., 'torch.nn.ReLU'), is evaluated during model construction (using eval()).\n    \"\"\"\n    n_linear_layers: int = 1\n    linear_hidden_dim: int = 128\n    activation: str = 'torch.nn.ReLU'\n</code></pre>"},{"location":"bnode_core/config/#bnode_core.config.base_training_settings_class","title":"<code>base_training_settings_class</code>","text":"<p>Generic optimization and early-stopping hyperparameters.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>Training mini-batch size.</p> <code>max_epochs</code> <code>int</code> <p>Maximum number of epochs.</p> <code>lr_start</code> <code>float</code> <p>Initial learning rate.</p> <code>beta1_adam</code> <code>float</code> <p>Adam beta1 parameter.</p> <code>beta2_adam</code> <code>float</code> <p>Adam beta2 parameter.</p> <code>weight_decay</code> <code>float</code> <p>L2 weight decay.</p> <code>clip_grad_norm</code> <code>float</code> <p>Gradient clipping norm.</p> <code>early_stopping_patience</code> <code>int</code> <p>Patience before early stopping.</p> <code>early_stopping_threshold</code> <code>float</code> <p>Improvement threshold for early stopping.</p> <code>early_stopping_threshold_mode</code> <code>str</code> <p>Threshold mode, typically 'rel' or 'abs'.</p> <code>initialization_type</code> <code>Optional[str]</code> <p>Optional weight initialization scheme identifier.</p> Source code in <code>src/bnode_core/config.py</code> <pre><code>@dataclass\nclass base_training_settings_class:\n    \"\"\"\n    Generic optimization and early-stopping hyperparameters.\n\n    Attributes:\n        batch_size (int): Training mini-batch size.\n        max_epochs (int): Maximum number of epochs.\n        lr_start (float): Initial learning rate.\n        beta1_adam (float): Adam beta1 parameter.\n        beta2_adam (float): Adam beta2 parameter.\n        weight_decay (float): L2 weight decay.\n        clip_grad_norm (float): Gradient clipping norm.\n        early_stopping_patience (int): Patience before early stopping.\n        early_stopping_threshold (float): Improvement threshold for early stopping.\n        early_stopping_threshold_mode (str): Threshold mode, typically 'rel' or 'abs'.\n        initialization_type (Optional[str]): Optional weight initialization scheme identifier.\n    \"\"\"\n    batch_size: int = 64\n    max_epochs: int = 30000\n    lr_start: float = 0.001\n    beta1_adam: Optional[float] = 0.9\n    beta2_adam: Optional[float] = 0.999\n    weight_decay: float = 0.0\n    clip_grad_norm: float = 100.0\n    early_stopping_patience: int = 1000\n    early_stopping_threshold: float = 0.000\n    early_stopping_threshold_mode: str = 'abs'\n    initialization_type: Optional[str] = None\n</code></pre>"},{"location":"bnode_core/config/#bnode_core.config.abstract_nn_model_class","title":"<code>abstract_nn_model_class</code>","text":"<p>Marker base class for neural-network configuration objects.</p> Source code in <code>src/bnode_core/config.py</code> <pre><code>@dataclass\nclass abstract_nn_model_class:\n    \"\"\"\n    Marker base class for neural-network configuration objects.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"bnode_core/config/#bnode_core.config.base_nn_model_class","title":"<code>base_nn_model_class</code>","text":"<p>               Bases: <code>abstract_nn_model_class</code></p> <p>Configuration for a simple feed-forward model with network and training settings.</p> <p>Attributes:</p> Name Type Description <code>network</code> <code>base_network_class</code> <p>Backbone network hyperparameters.</p> <code>training</code> <code>base_training_settings_class</code> <p>Training/optimization hyperparameters.</p> Source code in <code>src/bnode_core/config.py</code> <pre><code>@dataclass\nclass base_nn_model_class(abstract_nn_model_class):\n    \"\"\"\n    Configuration for a simple feed-forward model with network and training settings.\n\n    Attributes:\n        network (base_network_class): Backbone network hyperparameters.\n        training (base_training_settings_class): Training/optimization hyperparameters.\n    \"\"\"\n    network: base_network_class = field(default_factory=base_network_class)\n    training: base_training_settings_class = field(default_factory=base_training_settings_class)\n</code></pre>"},{"location":"bnode_core/config/#bnode_core.config.pels_vae_network_class","title":"<code>pels_vae_network_class</code>","text":"<p>               Bases: <code>base_network_class</code></p> <p>Network configuration for the PELS-VAE encoder/decoder.</p> <p>Gotchas:</p> <ul> <li>activation must be the string of a valid torch.nn module (e.g., 'torch.nn.ReLU'); validation will fail otherwise.</li> </ul> <p>Attributes:</p> Name Type Description <code>n_latent</code> <code>int</code> <p>Size of latent space.</p> <code>n_linear_layers</code> <code>int</code> <p>Number of linear layers in encoder/decoder.</p> <code>linear_hidden_dim</code> <code>int</code> <p>Hidden dimension in linear layers.</p> <code>dropout_rate</code> <code>float</code> <p>Dropout rate applied within the network.</p> <code>activation</code> <code>str</code> <p>Activation module as a string (validated via eval).</p> <code>params_to_decoder</code> <code>bool</code> <p>Whether to pass parameters to the decoder.</p> <code>feed_forward_nn</code> <code>bool</code> <p>If True, use a feed-forward network instead of a VAE.</p> Source code in <code>src/bnode_core/config.py</code> <pre><code>@dataclass\nclass pels_vae_network_class(base_network_class):\n    \"\"\"\n    Network configuration for the PELS-VAE encoder/decoder.\n\n    Gotchas:\n\n    - activation must be the string of a valid torch.nn module (e.g., 'torch.nn.ReLU'); validation will fail otherwise.\n\n    Attributes:\n        n_latent (int): Size of latent space.\n        n_linear_layers (int): Number of linear layers in encoder/decoder.\n        linear_hidden_dim (int): Hidden dimension in linear layers.\n        dropout_rate (float): Dropout rate applied within the network.\n        activation (str): Activation module as a string (validated via eval).\n        params_to_decoder (bool): Whether to pass parameters to the decoder.\n        feed_forward_nn (bool): If True, use a feed-forward network instead of a VAE.\n    \"\"\"\n    n_latent: int = 64\n    n_linear_layers: int = 3\n    linear_hidden_dim: int = 128\n    dropout_rate: float = 0.0\n    activation: str = 'torch.nn.ReLU'\n    params_to_decoder: bool = False\n    feed_forward_nn: bool = False # can switch to feed forward network instead of VAE\n\n    @field_validator('activation')\n    @classmethod\n    def check_if_torch_module(cls,v):\n        try:\n            import torch\n            eval(v)\n        except:\n            raise ValueError('activation must be a torch.nn.Module, \\\n                             please use the string representation of the module, e.g. torch.nn.ReLU')\n        return v\n</code></pre>"},{"location":"bnode_core/config/#bnode_core.config.pels_vae_training_settings_class","title":"<code>pels_vae_training_settings_class</code>","text":"<p>               Bases: <code>base_training_settings_class</code></p> <p>Training hyperparameters for the PELS-VAE model.</p> <p>Notes:</p> <ul> <li>Capacity terms (capacity_*) are applied only when use_capacity is True.</li> </ul> <p>Attributes:</p> Name Type Description <code>lr_min</code> <code>float</code> <p>Minimum learning rate used by scheduler.</p> <code>lr_scheduler_plateau_gamma</code> <code>float</code> <p>Multiplicative factor of LR reduction on plateau.</p> <code>lr_scheduler_plateau_patience</code> <code>int</code> <p>Epochs to wait before reducing LR on plateau.</p> <code>lr_scheduler_threshold</code> <code>float</code> <p>Threshold for measuring new optimum, to only focus on significant changes.</p> <code>lr_scheduler_threshold_mode</code> <code>str</code> <p>'rel' or 'abs' threshold mode for scheduler.</p> <code>init_bias</code> <code>float</code> <p>Initial bias for certain layers.</p> <code>beta_start</code> <code>float</code> <p>Starting beta for KL term annealing.</p> <code>clip_grad_norm</code> <code>float</code> <p>Larger clip norm typical for VAE stability.</p> <code>gamma</code> <code>float</code> <p>Extra scaling for reconstruction/regularization balance.</p> <code>use_capacity</code> <code>bool</code> <p>Enable capacity scheduling.</p> <code>capacity_patience</code> <code>int</code> <p>Patience before increasing capacity.</p> <code>capacity_start</code> <code>float</code> <p>Initial target capacity.</p> <code>capacity_increment</code> <code>float</code> <p>Step size for increasing capacity.</p> <code>capacity_increment_mode</code> <code>str</code> <p>'abs' or 'rel' increment mode.</p> <code>capacity_threshold</code> <code>float</code> <p>Threshold to trigger capacity increase.</p> <code>capacity_threshold_mode</code> <code>str</code> <p>'rel' or 'abs' threshold mode.</p> <code>capacity_max</code> <code>float</code> <p>Maximum allowed target capacity.</p> <code>count_populated_dimensions_threshold</code> <code>float</code> <p>Threshold to count active latent dimensions.</p> <code>n_passes_test</code> <code>int</code> <p>Number of stochastic passes during test.</p> <code>n_passes_train</code> <code>int</code> <p>Number of stochastic passes during training.</p> <code>test_from_regressor</code> <code>bool</code> <p>Evaluate using regressor pathway if available.</p> <code>test_with_zero_eps</code> <code>bool</code> <p>If True, set latent noise to zero during testing.</p> Source code in <code>src/bnode_core/config.py</code> <pre><code>@dataclass\nclass pels_vae_training_settings_class(base_training_settings_class):\n    \"\"\"\n    Training hyperparameters for the PELS-VAE model.\n\n    Notes:\n\n    - Capacity terms (capacity_*) are applied only when use_capacity is True.\n\n    Attributes:\n        lr_min (float): Minimum learning rate used by scheduler.\n        lr_scheduler_plateau_gamma (float): Multiplicative factor of LR reduction on plateau.\n        lr_scheduler_plateau_patience (int): Epochs to wait before reducing LR on plateau.\n        lr_scheduler_threshold (float): Threshold for measuring new optimum, to only focus on significant changes.\n        lr_scheduler_threshold_mode (str): 'rel' or 'abs' threshold mode for scheduler.\n        init_bias (float): Initial bias for certain layers.\n        beta_start (float): Starting beta for KL term annealing.\n        clip_grad_norm (float): Larger clip norm typical for VAE stability.\n        gamma (float): Extra scaling for reconstruction/regularization balance.\n        use_capacity (bool): Enable capacity scheduling.\n        capacity_patience (int): Patience before increasing capacity.\n        capacity_start (float): Initial target capacity.\n        capacity_increment (float): Step size for increasing capacity.\n        capacity_increment_mode (str): 'abs' or 'rel' increment mode.\n        capacity_threshold (float): Threshold to trigger capacity increase.\n        capacity_threshold_mode (str): 'rel' or 'abs' threshold mode.\n        capacity_max (float): Maximum allowed target capacity.\n        count_populated_dimensions_threshold (float): Threshold to count active latent dimensions.\n        n_passes_test (int): Number of stochastic passes during test.\n        n_passes_train (int): Number of stochastic passes during training.\n        test_from_regressor (bool): Evaluate using regressor pathway if available.\n        test_with_zero_eps (bool): If True, set latent noise to zero during testing.\n    \"\"\"\n    batch_size: int = 64\n    lr_start: float = 1e-5\n    lr_min: float = 1e-5\n    lr_scheduler_plateau_gamma: float = 0.5\n    lr_scheduler_plateau_patience: int = 1500\n    lr_scheduler_threshold: float = 0.1\n    lr_scheduler_threshold_mode: str = 'rel'\n    weight_decay: float = 1e-4\n    early_stopping_patience: int = 2000\n    init_bias: float = 1e-3\n    beta_start: float = 0.01\n    clip_grad_norm: float = 1e6\n    gamma: float = 1.0\n    use_capacity: bool = False\n    capacity_patience: int = 10\n    capacity_start: float = 0.1\n    capacity_increment: float = 0.05\n    capacity_increment_mode: str = 'abs'\n    capacity_threshold: float = 0.2\n    capacity_threshold_mode: str = 'rel'\n    capacity_max: float = 12.0\n    count_populated_dimensions_threshold: float = 0.1\n    n_passes_test: int = 1\n    n_passes_train: int = 1\n    test_from_regressor: bool = True\n    test_with_zero_eps: bool = False\n</code></pre>"},{"location":"bnode_core/config/#bnode_core.config.neural_ode_network_class","title":"<code>neural_ode_network_class</code>","text":"<p>               Bases: <code>base_network_class</code></p> <p>Network configuration for the neural ODE model.</p> <p>Attributes:</p> Name Type Description <code>linear_hidden_dim</code> <code>int</code> <p>Hidden dimension in dynamics MLP.</p> <code>hidden_dim_output_nn</code> <code>int</code> <p>Hidden dimension in the output head.</p> <code>n_layers_output_nn</code> <code>int</code> <p>Number of layers in the output head.</p> <code>activation</code> <code>str</code> <p>Activation module as a string (e.g., 'torch.nn.ELU').</p> <code>controls_to_output_nn</code> <code>bool</code> <p>If True, concatenate controls to the output head inputs.</p> Source code in <code>src/bnode_core/config.py</code> <pre><code>@dataclass\nclass neural_ode_network_class(base_network_class):\n    \"\"\"\n    Network configuration for the neural ODE model.\n\n    Attributes:\n        linear_hidden_dim (int): Hidden dimension in dynamics MLP.\n        hidden_dim_output_nn (int): Hidden dimension in the output head.\n        n_layers_output_nn (int): Number of layers in the output head.\n        activation (str): Activation module as a string (e.g., 'torch.nn.ELU').\n        controls_to_output_nn (bool): If True, concatenate controls to the output head inputs.\n    \"\"\"\n    linear_hidden_dim: int = 128\n    hidden_dim_output_nn: int = 12\n    n_layers_output_nn: int = 4\n    activation: str = 'torch.nn.ELU'\n    controls_to_output_nn: bool = False\n</code></pre>"},{"location":"bnode_core/config/#bnode_core.config.latent_ode_network_class","title":"<code>latent_ode_network_class</code>","text":"<p>               Bases: <code>base_network_class</code></p> <p>Network configuration and linearization modes for the latent ODE model.</p> <p>Gotchas:</p> <ul> <li>lat_ode_type must be one of ['variance_constant', 'variance_dynamic', 'vanilla'].</li> <li>linear_mode must be one of [None, 'mpc_mode', 'mpc_mode_for_controls', 'deep_koopman'].</li> <li>linear_mode choices set the *_linear flags as follows:   'mpc_mode' -&gt; all linear except state encoder; 'mpc_mode_for_controls' -&gt; linear ODE/decoder and control encoder;   'deep_koopman' -&gt; only ODE linear.</li> </ul> <p>Attributes:</p> Name Type Description <code>n_linear_layers</code> <code>int</code> <p>Number of linear layers in encoders/decoder/ODE.</p> <code>linear_hidden_dim</code> <code>int</code> <p>Hidden dimension in encoders/decoder/ODE.</p> <code>activation</code> <code>str</code> <p>Activation module as a string (e.g., 'torch.nn.ELU'). ss evaluated during model construction (using eval()).</p> <code>controls_to_decoder</code> <code>bool</code> <p>Concatenate controls to decoder inputs.</p> <code>predict_states</code> <code>bool</code> <p>If True, predict states in addition to outputs. Default True.</p> <code>lat_ode_type</code> <code>str</code> <p>Variance handling mode for latent ODE. Must be in ['variance_constant', 'variance_dynamic', 'vanilla']. 'vanilla' generates a standard latent ODE. The other options implement BNODE as described in the paper.</p> <code>include_params_encoder</code> <code>bool</code> <p>Include parameter encoder.</p> <code>params_to_state_encoder</code> <code>bool</code> <p>Feed parameters to state encoder.</p> <code>params_to_control_encoder</code> <code>bool</code> <p>Feed parameters to control encoder.</p> <code>params_to_decoder</code> <code>bool</code> <p>Feed latent parameters to decoder.</p> <code>controls_to_state_encoder</code> <code>bool</code> <p>Feed controls to state encoder.</p> <code>lat_state_mu_independent</code> <code>bool</code> <p>If True, state mean is independent from variance path. Only applicable for 'variance_dynamic' lat_ode_type.</p> <code>linear_mode</code> <code>Optional[str]</code> <p>Linearization preset; updates *_linear flags accordingly. Available options: None, 'mpc_mode', 'mpc_mode_for_controls', 'deep_koopman'. (see above)</p> <code>state_encoder_linear</code> <code>bool</code> <p>Force state encoder to be linear. Can be overridden by linear_mode.</p> <code>control_encoder_linear</code> <code>bool</code> <p>Force control encoder to be linear. Can be overridden by linear_mode.</p> <code>parameter_encoder_linear</code> <code>bool</code> <p>Force parameter encoder to be linear. Can be overridden by linear_mode.</p> <code>ode_linear</code> <code>bool</code> <p>Use a linear latent ODE. Can be overridden by linear_mode.</p> <code>decoder_linear</code> <code>bool</code> <p>Use a linear decoder. Can be overridden by linear_mode.</p> <code>lat_states_dim</code> <code>int</code> <p>Dimension of latent state space.</p> <code>lat_parameters_dim</code> <code>int</code> <p>Dimension of latent parameter space.</p> <code>lat_controls_dim</code> <code>int</code> <p>Dimension of latent control space.</p> Source code in <code>src/bnode_core/config.py</code> <pre><code>@dataclass\nclass latent_ode_network_class(base_network_class):\n    \"\"\"\n    Network configuration and linearization modes for the latent ODE model.\n\n    Gotchas:\n\n    - lat_ode_type must be one of ['variance_constant', 'variance_dynamic', 'vanilla'].\n    - linear_mode must be one of [None, 'mpc_mode', 'mpc_mode_for_controls', 'deep_koopman'].\n    - linear_mode choices set the *_linear flags as follows:\n      'mpc_mode' -&gt; all linear except state encoder; 'mpc_mode_for_controls' -&gt; linear ODE/decoder and control encoder;\n      'deep_koopman' -&gt; only ODE linear.\n\n    Attributes:\n        n_linear_layers (int): Number of linear layers in encoders/decoder/ODE.\n        linear_hidden_dim (int): Hidden dimension in encoders/decoder/ODE.\n        activation (str): Activation module as a string (e.g., 'torch.nn.ELU'). ss evaluated during model construction (using eval()).\n        controls_to_decoder (bool): Concatenate controls to decoder inputs.\n        predict_states (bool): If True, predict states in addition to outputs. Default True.\n        lat_ode_type (str): Variance handling mode for latent ODE. Must be in ['variance_constant', 'variance_dynamic', 'vanilla']. 'vanilla' generates a standard latent ODE. The other options implement BNODE as described in the paper.\n        include_params_encoder (bool): Include parameter encoder.\n        params_to_state_encoder (bool): Feed parameters to state encoder.\n        params_to_control_encoder (bool): Feed parameters to control encoder.\n        params_to_decoder (bool): Feed latent parameters to decoder.\n        controls_to_state_encoder (bool): Feed controls to state encoder.\n        lat_state_mu_independent (bool): If True, state mean is independent from variance path. Only applicable for 'variance_dynamic' lat_ode_type.\n        linear_mode (Optional[str]): Linearization preset; updates *_linear flags accordingly. Available options: None, 'mpc_mode', 'mpc_mode_for_controls', 'deep_koopman'. (see above)\n        state_encoder_linear (bool): Force state encoder to be linear. Can be overridden by linear_mode.\n        control_encoder_linear (bool): Force control encoder to be linear. Can be overridden by linear_mode.\n        parameter_encoder_linear (bool): Force parameter encoder to be linear. Can be overridden by linear_mode.\n        ode_linear (bool): Use a linear latent ODE. Can be overridden by linear_mode.\n        decoder_linear (bool): Use a linear decoder. Can be overridden by linear_mode.\n        lat_states_dim (int): Dimension of latent state space.\n        lat_parameters_dim (int): Dimension of latent parameter space.\n        lat_controls_dim (int): Dimension of latent control space.\n    \"\"\"\n    n_linear_layers: int = 4\n    linear_hidden_dim: int = 128\n    activation: str = 'torch.nn.ELU'\n    controls_to_decoder: bool = True\n    predict_states: bool = True\n    lat_ode_type: str = 'variance_constant' # must be in ['variance_constant', 'variance_dynamic', 'vanilla']\n    include_params_encoder: bool = True\n    params_to_state_encoder: bool = False\n    params_to_control_encoder: bool = False\n    params_to_decoder: bool = False\n    controls_to_state_encoder: bool = False\n\n    lat_state_mu_independent: bool = False\n\n    linear_mode: Optional[str] = None # must be one of None, 'mpc_mode', 'mpc_mode_for_controls', 'deep_koopman'\n    state_encoder_linear: bool = False\n    control_encoder_linear: bool = False\n    parameter_encoder_linear: bool = False\n    ode_linear: bool = False\n    decoder_linear: bool = False\n\n    lat_states_dim: int = 64\n    lat_parameters_dim: int = 64\n    lat_controls_dim: int = 64\n\n    # check field lat_ode_type\n    @field_validator('lat_ode_type')\n    @classmethod\n    def check_lat_ode_type(cls, v):\n        types = ['variance_constant', 'variance_dynamic', 'vanilla']\n        if v not in types:\n            raise ValueError('lat_ode_type must be in ', types)\n        return v\n\n    @model_validator(mode='after')\n    def model_validate_linear_mode(self):\n        # This method will be called after field validation for the whole model\n        # It should be registered as a model_validator in the class\n        v = self.linear_mode\n        if v is not None:\n            if v == 'mpc_mode':\n                self.state_encoder_linear = False\n                self.control_encoder_linear = True\n                self.parameter_encoder_linear = True\n                self.ode_linear = True\n                self.decoder_linear = True\n                logging.info('Setting all linear modes to True except state_encoder for mpc_mode')\n            elif v == 'mpc_mode_for_controls':\n                self.state_encoder_linear = False\n                self.control_encoder_linear = True\n                self.parameter_encoder_linear = False\n                self.ode_linear = True\n                self.decoder_linear = True\n                logging.info('Setting all linear modes to True except state_encoder and parameter_encoder for mpc_mode_for_controls')\n            elif v == 'deep_koopman':\n                self.state_encoder_linear = False\n                self.control_encoder_linear = False\n                self.parameter_encoder_linear = False\n                self.ode_linear = True\n                self.decoder_linear = False\n                logging.info('Setting only ODE to linear for deep_koopman')\n        return self\n</code></pre>"},{"location":"bnode_core/config/#bnode_core.config.base_neural_ode_pretraining_settings_class","title":"<code>base_neural_ode_pretraining_settings_class</code>","text":"<p>               Bases: <code>base_training_settings_class</code></p> <p>Pretraining settings for neural ODE components prior to main training.</p> <p>Attributes:</p> Name Type Description <code>method</code> <code>str</code> <p>Pretraining method (e.g., 'collocation').</p> <code>batch_size</code> <code>int</code> <p>Mini-batch size during pretraining.</p> <code>batches_per_epoch</code> <code>int</code> <p>Number of batches per epoch during pretraining.</p> <code>max_epochs</code> <code>int</code> <p>Maximum pretraining epochs.</p> <code>lr_start</code> <code>float</code> <p>Initial learning rate for pretraining.</p> <code>weight_decay</code> <code>float</code> <p>Weight decay during pretraining.</p> <code>early_stopping_patience</code> <code>int</code> <p>Patience for early stopping in pretraining.</p> <code>early_stopping_threshold</code> <code>float</code> <p>Threshold for early stopping in pretraining.</p> <code>early_stopping_threshold_mode</code> <code>str</code> <p>Threshold mode ('rel' or 'abs').</p> <code>load_seq_len</code> <code>Optional[int]</code> <p>If provided, sequence length to load from dataset.</p> <code>seq_len_train</code> <code>int</code> <p>Sequence length used during pretraining.</p> <code>break_after_loss_of</code> <code>Optional[float]</code> <p>Early break threshold on loss value.</p> Source code in <code>src/bnode_core/config.py</code> <pre><code>@dataclass\nclass base_neural_ode_pretraining_settings_class(base_training_settings_class):\n    \"\"\"\n    Pretraining settings for neural ODE components prior to main training.\n\n    Attributes:\n        method (str): Pretraining method (e.g., 'collocation').\n        batch_size (int): Mini-batch size during pretraining.\n        batches_per_epoch (int): Number of batches per epoch during pretraining.\n        max_epochs (int): Maximum pretraining epochs.\n        lr_start (float): Initial learning rate for pretraining.\n        weight_decay (float): Weight decay during pretraining.\n        early_stopping_patience (int): Patience for early stopping in pretraining.\n        early_stopping_threshold (float): Threshold for early stopping in pretraining.\n        early_stopping_threshold_mode (str): Threshold mode ('rel' or 'abs').\n        load_seq_len (Optional[int]): If provided, sequence length to load from dataset.\n        seq_len_train (int): Sequence length used during pretraining.\n        break_after_loss_of (Optional[float]): Early break threshold on loss value.\n    \"\"\"\n    method: str = 'collocation'\n    batch_size: int = 1024\n    batches_per_epoch: int = 12\n    max_epochs: int = 100\n    lr_start: float = 0.001\n    weight_decay: float = 1e-4\n    early_stopping_patience: int = 50\n    early_stopping_threshold: float = 0.001\n    early_stopping_threshold_mode: str = 'rel'\n    load_seq_len: Optional[int] = None\n    seq_len_train: int = 10\n    break_after_loss_of: Optional[float] = None\n</code></pre>"},{"location":"bnode_core/config/#bnode_core.config.base_time_stepper_training_settings","title":"<code>base_time_stepper_training_settings</code>","text":"<p>               Bases: <code>base_training_settings_class</code></p> <p>Per-phase training settings for time-stepped training with ODE solvers.</p> <p>Gotchas:</p> <ul> <li>If load_seq_len is not None (and not 0), then seq_len_train must be &lt;= load_seq_len.</li> <li>solver_norm must be either 'max' or 'mixed'; other values raise validation errors.</li> <li>solver_step_size None means the solver uses its internal default step size.</li> </ul> <p>Attributes:</p> Name Type Description <code>evaluate_at_control_times</code> <code>Optional[bool]</code> <p>If True, solver is forced to restart at control change times.</p> <code>batches_per_epoch</code> <code>Optional[int]</code> <p>Number of batches per epoch for this phase. Default is 12.</p> <code>reload_optimizer</code> <code>Optional[bool]</code> <p>Recreate optimizer at phase start.</p> <code>load_seq_len</code> <code>Optional[int]</code> <p>Sequence length used to load model/optimizer state.</p> <code>seq_len_train</code> <code>Optional[int]</code> <p>Training sequence length for this phase.</p> <code>seq_len_increase_in_batches</code> <code>Optional[int]</code> <p>Increase sequence length every N batches.</p> <code>seq_len_increase_abort_after_n_stable_epochs</code> <code>Optional[int]</code> <p>Stop increasing after N stable epochs. A stable epoch is counted when loss_validation &lt; 2 * loss_train</p> <code>use_adjoint</code> <code>Optional[bool]</code> <p>Use adjoint method for ODE gradients.</p> <code>solver</code> <code>Optional[str]</code> <p>ODE solver name (e.g., 'dopri5').</p> <code>solver_rtol</code> <code>Optional[float]</code> <p>Relative tolerance for adaptive step size solver.</p> <code>solver_atol</code> <code>Optional[float]</code> <p>Absolute tolerance for adaptive step size solver.</p> <code>solver_norm</code> <code>Optional[str]</code> <p>Norm used for adaptive step ('max' or 'mixed'). Default is 'mixed', that uses rmse per sample and max over batch.</p> <code>solver_step_size</code> <code>Optional[float]</code> <p>Fixed step size; None uses solver defaults. Only used for fixed-step solvers, should the dataset time step should be a multiple of this step size.</p> <code>break_after_loss_of</code> <code>Optional[float]</code> <p>Early break threshold on loss value.</p> <code>reload_model_if_loss_nan</code> <code>bool</code> <p>Reload last checkpoint if loss becomes NaN.</p> <code>activate_deterministic_mode_after_this_phase</code> <code>bool</code> <p>Activate deterministic latent dynamics after this phase.</p> <code>deterministic_mode_from_state0</code> <code>bool</code> <p>If True, deterministic mode is based on latent state at time 0; otherwise based on full trajectory.</p> <code>seq_len_epoch_start</code> <code>Optional[int]</code> <p>Internal tracker for the starting sequence length of this phase.</p> Source code in <code>src/bnode_core/config.py</code> <pre><code>@dataclass\nclass base_time_stepper_training_settings(base_training_settings_class):\n    \"\"\"\n    Per-phase training settings for time-stepped training with ODE solvers.\n\n    Gotchas:\n\n    - If load_seq_len is not None (and not 0), then seq_len_train must be &lt;= load_seq_len.\n    - solver_norm must be either 'max' or 'mixed'; other values raise validation errors.\n    - solver_step_size None means the solver uses its internal default step size.\n\n    Attributes:\n        evaluate_at_control_times (Optional[bool]): If True, solver is forced to restart at control change times.\n        batches_per_epoch (Optional[int]): Number of batches per epoch for this phase. Default is 12.\n        reload_optimizer (Optional[bool]): Recreate optimizer at phase start.\n        load_seq_len (Optional[int]): Sequence length used to load model/optimizer state.\n        seq_len_train (Optional[int]): Training sequence length for this phase.\n        seq_len_increase_in_batches (Optional[int]): Increase sequence length every N batches.\n        seq_len_increase_abort_after_n_stable_epochs (Optional[int]): Stop increasing after N stable epochs. A stable epoch is counted when loss_validation &lt; 2 * loss_train\n        use_adjoint (Optional[bool]): Use adjoint method for ODE gradients.\n        solver (Optional[str]): ODE solver name (e.g., 'dopri5').\n        solver_rtol (Optional[float]): Relative tolerance for adaptive step size solver.\n        solver_atol (Optional[float]): Absolute tolerance for adaptive step size solver.\n        solver_norm (Optional[str]): Norm used for adaptive step ('max' or 'mixed'). Default is 'mixed', that uses rmse per sample and max over batch.\n        solver_step_size (Optional[float]): Fixed step size; None uses solver defaults. Only used for fixed-step solvers, should the dataset time step should be a multiple of this step size.\n        break_after_loss_of (Optional[float]): Early break threshold on loss value.\n        reload_model_if_loss_nan (bool): Reload last checkpoint if loss becomes NaN.\n        activate_deterministic_mode_after_this_phase (bool): Activate deterministic latent dynamics after this phase.\n        deterministic_mode_from_state0 (bool): If True, deterministic mode is based on latent state at time 0; otherwise based on full trajectory.\n        seq_len_epoch_start (Optional[int]): Internal tracker for the starting sequence length of this phase.\n    \"\"\"\n    evaluate_at_control_times: Optional[bool] = False\n    batches_per_epoch: Optional[int] = 12\n    reload_optimizer: Optional[bool] = True\n    load_seq_len: Optional[int] = None\n    seq_len_train: Optional[int] = None\n    seq_len_increase_in_batches: Optional[int] = 0\n    seq_len_increase_abort_after_n_stable_epochs: Optional[int] = 10\n    use_adjoint: Optional[bool] = False\n    solver: Optional[str] = 'dopri5'\n    solver_rtol: Optional[float] = 1e-3\n    solver_atol: Optional[float] = 1e-4\n    solver_norm: Optional[str] = 'mixed' # max or mixed\n    solver_step_size: Optional[float] = None # if None, the solver will use default settings\n    break_after_loss_of: Optional[float] = None\n    reload_model_if_loss_nan: bool = True # should be always True, only set to false e.g. for writing iclr paper\n    activate_deterministic_mode_after_this_phase: bool = False # is only used for B-NODE, but for compatibility reasons it is included here\n    deterministic_mode_from_state0: bool = False # if True, the deterministic mode will be activated based on the latent state at time 0, otherwise based on the latent states over the whole trajectory\n\n    seq_len_epoch_start: Optional[int] = None # only used internally, does not need to be set. But this can be set\n    @field_validator('seq_len_train')\n    @classmethod\n    def check_seq_len_train_start(cls, v, info: ValidationInfo):\n        if info.data['load_seq_len'] != None:\n            if v &gt; info.data['load_seq_len']:\n                if info.data['load_seq_len'] == 0:\n                    pass\n                else:\n                    raise ValueError('seq_len_train must be smaller than load_seq_len')\n        return v\n    @field_validator('solver_norm')\n    @classmethod\n    def check_adaptive_step_size_norm(cls, v, info: ValidationInfo):\n        if v not in ['max', 'mixed']:\n            raise ValueError('solver_norm must be max or mixed')\n        return v\n</code></pre>"},{"location":"bnode_core/config/#bnode_core.config.base_neural_ode_training_settings_class","title":"<code>base_neural_ode_training_settings_class</code>","text":"<p>High-level training configuration and per-phase override mechanism for ODE models.</p> <p>Gotchas:</p> <ul> <li>Any *_override set here is propagated into each phase in main_training and may override non-defaults (a warning is logged).</li> <li>There is no override for fields that are intended to apply to single phases only (e.g., break_after_loss_of).</li> <li>main_training holds a sequence of phases executed in order. These are validated by the dataclass \"base_time_stepper_training_settings\".</li> </ul> <p>Attributes:</p> Name Type Description <code>pre_train</code> <code>bool</code> <p>If True, run a pretraining stage before main training.</p> <code>load_pretrained_model</code> <code>bool</code> <p>Load a pretrained model before training.</p> <code>load_trained_model_for_test</code> <code>bool</code> <p>Load a fully trained model and run testing only.</p> <code>save_predictions_in_dataset</code> <code>bool</code> <p>Save predictions back into the dataset after testing.</p> <code>test</code> <code>bool</code> <p>Enable test pass after training.</p> <code>test_save_internal_variables</code> <code>bool</code> <p>Save internal variables to dataset during testing.</p> <code>test_save_internal_variables_for</code> <code>str</code> <p>Save internal variables for this dataset split during testing. (e.g., 'common_test')</p> <code>pre_trained_model_seq_len</code> <code>Optional[int]</code> <p>Sequence length of the pretrained checkpoint to load.</p> <code>path_pretrained_model</code> <code>Optional[str]</code> <p>Path to pretrained weights. Can also be copied from mlflow web UI.</p> <code>path_trained_model</code> <code>Optional[str]</code> <p>Path to trained model weights for testing. Can also be copied from mlflow web UI.</p> <code>batch_size_test</code> <code>int</code> <p>Batch size to use during testing.</p> <code>initialization_type</code> <code>Optional[str]</code> <p>Weight initialization scheme for NN. Options: 'xavier', none.</p> <code>initialization_type_ode</code> <code>Optional[str]</code> <p>Initialization scheme for ODE-specific components. Options: 'xavier', none.</p> <code>***_override</code> <code>various</code> <p>Overrides for training hyperparameters applied to all phases in main_training.</p> <code>pre_training</code> <code>base_neural_ode_pretraining_settings_class</code> <p>Pretraining settings.</p> <code>main_training</code> <code>List[base_time_stepper_training_settings]</code> <p>Sequence of per-phase settings.</p> Source code in <code>src/bnode_core/config.py</code> <pre><code>@dataclass\nclass base_neural_ode_training_settings_class():\n    \"\"\"\n    High-level training configuration and per-phase override mechanism for ODE models.\n\n    Gotchas:\n\n    - Any *_override set here is propagated into each phase in main_training and may override non-defaults (a warning is logged).\n    - There is no override for fields that are intended to apply to single phases only (e.g., break_after_loss_of).\n    - main_training holds a sequence of phases executed in order. These are validated by the dataclass \"base_time_stepper_training_settings\".\n\n    Attributes:\n        pre_train (bool): If True, run a pretraining stage before main training.\n        load_pretrained_model (bool): Load a pretrained model before training.\n        load_trained_model_for_test (bool): Load a fully trained model and run testing only.\n        save_predictions_in_dataset (bool): Save predictions back into the dataset after testing.\n        test (bool): Enable test pass after training.\n        test_save_internal_variables (bool): Save internal variables to dataset during testing.\n        test_save_internal_variables_for (str): Save internal variables for this dataset split during testing. (e.g., 'common_test')\n        pre_trained_model_seq_len (Optional[int]): Sequence length of the pretrained checkpoint to load.\n        path_pretrained_model (Optional[str]): Path to pretrained weights. Can also be copied from mlflow web UI.\n        path_trained_model (Optional[str]): Path to trained model weights for testing. Can also be copied from mlflow web UI.\n        batch_size_test (int): Batch size to use during testing.\n        initialization_type (Optional[str]): Weight initialization scheme for NN. Options: 'xavier', none.\n        initialization_type_ode (Optional[str]): Initialization scheme for ODE-specific components. Options: 'xavier', none.\n        ***_override (various): Overrides for training hyperparameters applied to all phases in main_training.\n        pre_training (base_neural_ode_pretraining_settings_class): Pretraining settings.\n        main_training (List[base_time_stepper_training_settings]): Sequence of per-phase settings.\n    \"\"\"\n    pre_train: bool = False\n    load_pretrained_model: bool = False\n    load_trained_model_for_test: bool = False\n    save_predictions_in_dataset: bool = True\n    test: bool = True\n    test_save_internal_variables: bool = False\n    test_save_internal_variables_for: str = 'common_test'\n    pre_trained_model_seq_len: Optional[int] = None\n    path_pretrained_model: Optional[str] = None\n    path_trained_model: Optional[str] = None \n\n    batch_size_test: int = 48\n    initialization_type: Optional[str] = None\n    initialization_type_ode: Optional[str] = None\n\n    batch_size_override: Optional[int] = None\n    batches_per_epoch_override: Optional[int] = None\n    max_epochs_override: Optional[int] = None\n    lr_start_override: Optional[float] = None\n    beta1_adam_override: Optional[float] = None\n    beta2_adam_override: Optional[float] = None\n    weight_decay_override: Optional[float] = None\n    clip_grad_norm_override: Optional[float] = None\n    early_stopping_patience_override: Optional[int] = None\n    early_stopping_threshold_override: Optional[float] = None\n    early_stopping_threshold_mode_override: Optional[str] = None\n\n    reload_optimizer_override: Optional[bool] = None    \n    solver_override: Optional[str] = None\n    load_seq_len_override: Optional[int] = None\n    seq_len_train_override: Optional[int] = None\n    seq_len_increase_in_batches_override: Optional[int] = None\n    seq_len_increase_abort_after_n_stable_epochs_override: Optional[int] = None\n    use_adjoint_override: Optional[bool] = None\n    evaluate_at_control_times_override: Optional[bool] = None\n    solver_rtol_override: Optional[float] = None\n    solver_atol_override: Optional[float] = None\n    solver_step_size_override: Optional[float] = None\n    solver_norm_override: Optional[str] = None\n    # no override for break_after_loss_of as this should only used for one training phase    pre_training: base_neural_ode_pretraining_settings_class = field(default_factory=base_neural_ode_pretraining_settings_class)\n\n    pre_training: base_neural_ode_pretraining_settings_class = field(default_factory=base_neural_ode_pretraining_settings_class)\n    main_training: List[base_time_stepper_training_settings] = field(default_factory=lambda: [base_time_stepper_training_settings()])\n\n    @field_validator('main_training')\n    @classmethod\n    def set_overrides(cls, v, info: ValidationInfo):\n        default_class = base_time_stepper_training_settings()\n        for i, training_settings in enumerate(v):\n            for key in ['batch_size_override', 'batches_per_epoch_override', 'max_epochs_override', 'lr_start_override',\n                        'beta1_adam_override', 'beta2_adam_override', 'clip_grad_norm_override', \n                        'weight_decay_override', 'early_stopping_patience_override', 'early_stopping_threshold_override', \n                        'early_stopping_threshold_mode_override', 'reload_optimizer_override','solver_override', 'load_seq_len_override', \n                        'seq_len_train_override', 'use_adjoint_override', 'evaluate_at_control_times_override','solver_rtol_override', 'solver_atol_override', 'solver_step_size_override',\n                        'seq_len_increase_in_batches_override', 'seq_len_increase_abort_after_n_stable_epochs_override', 'solver_norm_override']:\n                if info.data[key] is not None:\n                    # print warning if override is set and non-default value is used\n                    if v[i].__getattribute__(key.split('_override')[0]) != default_class.__getattribute__(key.split('_override')[0]):\n                        logging.warning(f'Overriding {key.split(\"_override\")[0]} with value {info.data[key]}')\n                    v[i].__setattr__(key.split('_override')[0], info.data[key])\n        return v\n</code></pre>"},{"location":"bnode_core/config/#bnode_core.config.base_ode_nn_model_class","title":"<code>base_ode_nn_model_class</code>","text":"<p>               Bases: <code>abstract_nn_model_class</code></p> <p>Wrapper that binds a network definition to neural ODE training settings.</p> <p>Attributes:</p> Name Type Description <code>network</code> <code>base_network_class</code> <p>NN backbone used for the ODE model.</p> <code>training</code> <code>base_neural_ode_training_settings_class</code> <p>ODE training schedule and overrides.</p> Source code in <code>src/bnode_core/config.py</code> <pre><code>@dataclass\nclass base_ode_nn_model_class(abstract_nn_model_class):\n    \"\"\"\n    Wrapper that binds a network definition to neural ODE training settings.\n\n    Attributes:\n        network (base_network_class): NN backbone used for the ODE model.\n        training (base_neural_ode_training_settings_class): ODE training schedule and overrides.\n    \"\"\"\n    network: base_network_class = field(default_factory=base_network_class)\n    training: base_neural_ode_training_settings_class = field(default_factory=base_neural_ode_training_settings_class)\n</code></pre>"},{"location":"bnode_core/config/#bnode_core.config.latent_timestepper_training_settings","title":"<code>latent_timestepper_training_settings</code>","text":"<p>               Bases: <code>base_time_stepper_training_settings</code></p> <p>Time-stepper training settings extended with latent-ODE-specific losses and counters.</p> <p>Notes:</p> <ul> <li>multi_shooting_condition_multiplier controls the strength of multi-shooting consistency; 10.0 - 1.0 is a typical value.</li> </ul> <p>Attributes:</p> Name Type Description <code>beta_start</code> <code>float</code> <p>Initial beta for KL term.</p> <code>alpha_mu</code> <code>float</code> <p>Weight for addes std nois on mean (mu) term in model evaluation. Should be 1.0 in all cases.</p> <code>alpha_sigma</code> <code>float</code> <p>Weight for added std noise (sigma) term in model evaluation. Typically small (e.g., 0.001), and only used when lat_ode_type is 'variance_dynamic'.</p> <code>n_passes</code> <code>int</code> <p>Number of passes per batch/epoch for stochastic evaluation. Default 1.</p> <code>threshold_count_populated_dimensions</code> <code>float</code> <p>Threshold to count active latent dims.</p> <code>include_reconstruction_loss_state0</code> <code>bool</code> <p>Include reconstruction loss at initial state.</p> <code>include_reconstruction_loss_outputs0</code> <code>bool</code> <p>Include reconstruction loss at initial outputs.</p> <code>include_reconstruction_loss_state_der</code> <code>bool</code> <p>Include loss on state derivatives. Adds a \"state_der_decoder\". Deprecated.</p> <code>include_states_grad_loss</code> <code>bool</code> <p>Include gradient matching loss for states.</p> <code>include_outputs_grad_loss</code> <code>bool</code> <p>Include gradient matching loss for outputs.</p> <code>multi_shooting_condition_multiplier</code> <code>float</code> <p>Weight for multi-shooting consistency term. Should be somewhere between 1.0 and 10.0 if used.</p> Source code in <code>src/bnode_core/config.py</code> <pre><code>@dataclass\nclass latent_timestepper_training_settings(base_time_stepper_training_settings):\n    \"\"\"\n    Time-stepper training settings extended with latent-ODE-specific losses and counters.\n\n    Notes:\n\n    - multi_shooting_condition_multiplier controls the strength of multi-shooting consistency; 10.0 - 1.0 is a typical value.\n\n    Attributes:\n        beta_start (float): Initial beta for KL term.\n        alpha_mu (float): Weight for addes std nois on mean (mu) term in model evaluation. Should be 1.0 in all cases.\n        alpha_sigma (float): Weight for added std noise (sigma) term in model evaluation. Typically small (e.g., 0.001), and only used when lat_ode_type is 'variance_dynamic'.\n        n_passes (int): Number of passes per batch/epoch for stochastic evaluation. Default 1.\n        threshold_count_populated_dimensions (float): Threshold to count active latent dims.\n        include_reconstruction_loss_state0 (bool): Include reconstruction loss at initial state.\n        include_reconstruction_loss_outputs0 (bool): Include reconstruction loss at initial outputs.\n        include_reconstruction_loss_state_der (bool): Include loss on state derivatives. Adds a \"state_der_decoder\". Deprecated.\n        include_states_grad_loss (bool): Include gradient matching loss for states.\n        include_outputs_grad_loss (bool): Include gradient matching loss for outputs.\n        multi_shooting_condition_multiplier (float): Weight for multi-shooting consistency term. Should be somewhere between 1.0 and 10.0 if used.\n    \"\"\"\n    beta_start: float = 0.001\n    alpha_mu: float = 1.0\n    alpha_sigma: float = 0.001\n    n_passes: int = 1\n    threshold_count_populated_dimensions: float = 0.1\n    include_reconstruction_loss_state0: bool = False\n    include_reconstruction_loss_outputs0: bool = False\n    include_reconstruction_loss_state_der: bool = False\n    include_states_grad_loss: bool = False\n    include_outputs_grad_loss: bool = False\n    multi_shooting_condition_multiplier: Optional[float] = None # 10.0 seems like a good value\n</code></pre>"},{"location":"bnode_core/config/#bnode_core.config.base_latent_ode_training_settings_class","title":"<code>base_latent_ode_training_settings_class</code>","text":"<p>Training configuration and overrides for latent ODE models with multiple training phases.</p> <p>Gotchas:</p> <ul> <li>Overrides (e.g., beta_start_override, n_passes_override, solver_*) are copied into each phase of main_training. A warning is logged if   the override changes a non-default value. Variables that are intended to apply to single phases only (e.g., break_after_loss_of) do not have overrides.</li> <li>At most one phase may set activate_deterministic_mode_after_this_phase=True (enforced in validation);   consider using alpha_mu &gt;= 1.0 when enabling deterministic mode.</li> </ul> <p>Attributes:</p> Name Type Description <code>pre_train</code> <code>bool</code> <p>If True, perform a pretraining stage.</p> <code>load_pretrained_model</code> <code>bool</code> <p>Load a pretrained model before training.</p> <code>load_trained_model_for_test</code> <code>bool</code> <p>Load a trained model and run tests only.</p> <code>save_predictions_in_dataset</code> <code>bool</code> <p>Save predictions back into the dataset on test.</p> <code>test</code> <code>bool</code> <p>Enable a post-training test run.</p> <code>test_save_internal_variables</code> <code>bool</code> <p>Store internal variables during test for analysis.</p> <code>test_save_internal_variables_for</code> <code>str</code> <p>Label for the stored internal variables.</p> <code>pre_trained_model_seq_len</code> <code>Optional[int]</code> <p>Sequence length used by the pretrained checkpoint.</p> <code>path_pretrained_model</code> <code>Optional[str]</code> <p>Path to pretrained model. Can also be copied from mlflow web UI.</p> <code>path_trained_model</code> <code>Optional[str]</code> <p>Path to trained model for testing. Can also be copied from mlflow web UI.</p> <code>batch_size_test</code> <code>int</code> <p>Test-time batch size.</p> <code>initialization_type</code> <code>Optional[str]</code> <p>Weight initialization scheme for NN. Options: 'xavier', none.</p> <code>initialization_type_ode</code> <code>Optional[str]</code> <p>Initialization scheme for ODE parts. Options: 'xavier', 'move_eigvals_matrix' (only for linear ode), 'move_eigvals_net', none.</p> <code>initialization_type_ode_matrix</code> <code>Optional[str]</code> <p>Initialization for ODE matrices if applicable.</p> <code>***_override</code> <code>various</code> <p>See *_override fields to broadcast settings into each main training phase.</p> <code>pre_training</code> <code>base_neural_ode_pretraining_settings_class</code> <p>Settings for the pretraining stage.</p> <code>main_training</code> <code>List[latent_timestepper_training_settings]</code> <p>Sequence of latent ODE training phases.</p> Source code in <code>src/bnode_core/config.py</code> <pre><code>@dataclass\nclass base_latent_ode_training_settings_class:\n    \"\"\"\n    Training configuration and overrides for latent ODE models with multiple training phases.\n\n    Gotchas:\n\n    - Overrides (e.g., beta_start_override, n_passes_override, solver_*) are copied into each phase of main_training. A warning is logged if\n      the override changes a non-default value. Variables that are intended to apply to single phases only (e.g., break_after_loss_of) do not have overrides.\n    - At most one phase may set activate_deterministic_mode_after_this_phase=True (enforced in validation);\n      consider using alpha_mu &gt;= 1.0 when enabling deterministic mode.\n\n    Attributes:\n        pre_train (bool): If True, perform a pretraining stage.\n        load_pretrained_model (bool): Load a pretrained model before training.\n        load_trained_model_for_test (bool): Load a trained model and run tests only.\n        save_predictions_in_dataset (bool): Save predictions back into the dataset on test.\n        test (bool): Enable a post-training test run.\n        test_save_internal_variables (bool): Store internal variables during test for analysis.\n        test_save_internal_variables_for (str): Label for the stored internal variables.\n        pre_trained_model_seq_len (Optional[int]): Sequence length used by the pretrained checkpoint.\n        path_pretrained_model (Optional[str]): Path to pretrained model. Can also be copied from mlflow web UI.\n        path_trained_model (Optional[str]): Path to trained model for testing. Can also be copied from mlflow web UI.\n        batch_size_test (int): Test-time batch size.\n        initialization_type (Optional[str]): Weight initialization scheme for NN. Options: 'xavier', none.\n        initialization_type_ode (Optional[str]): Initialization scheme for ODE parts. Options: 'xavier', 'move_eigvals_matrix' (only for linear ode), 'move_eigvals_net', none.\n        initialization_type_ode_matrix (Optional[str]): Initialization for ODE matrices if applicable.\n        ***_override (various): See *_override fields to broadcast settings into each main training phase.\n        pre_training (base_neural_ode_pretraining_settings_class): Settings for the pretraining stage.\n        main_training (List[latent_timestepper_training_settings]): Sequence of latent ODE training phases.\n    \"\"\"\n    pre_train: bool = False\n    load_pretrained_model: bool = False\n    load_trained_model_for_test: bool = False\n    save_predictions_in_dataset: bool = True\n    test: bool = True\n    test_save_internal_variables: bool = False\n    test_save_internal_variables_for: str = 'common_test'\n    pre_trained_model_seq_len: Optional[int] = None\n    path_pretrained_model: Optional[str] = None\n    path_trained_model: Optional[str] = None \n\n    batch_size_test: int = 48\n    initialization_type: Optional[str] = None\n    initialization_type_ode: Optional[str] = None\n    initialization_type_ode_matrix: Optional[str] = None\n\n    batch_size_override: Optional[int] = None\n    batches_per_epoch_override: Optional[int] = None\n    max_epochs_override: Optional[int] = None\n    lr_start_override: Optional[float] = None\n    beta1_adam_override: Optional[float] = None\n    beta2_adam_override: Optional[float] = None\n    weight_decay_override: Optional[float] = None\n    clip_grad_norm_override: Optional[float] = None\n    early_stopping_patience_override: Optional[int] = None\n    early_stopping_threshold_override: Optional[float] = None\n    early_stopping_threshold_mode_override: Optional[str] = None\n\n    reload_optimizer_override: Optional[bool] = None  \n    solver_override: Optional[str] = None\n    load_seq_len_override: Optional[int] = None\n    seq_len_train_override: Optional[int] = None\n    seq_len_increase_in_batches_override: Optional[int] = None\n    seq_len_increase_abort_after_n_stable_epochs_override: Optional[int] = None\n    use_adjoint_override: Optional[bool] = None\n    evaluate_at_control_times_override: Optional[bool] = None\n    solver_rtol_override: Optional[float] = None\n    solver_atol_override: Optional[float] = None\n    solver_step_size_override: Optional[float] = None\n    solver_norm_override: Optional[str] = None\n    # no override for break_after_loss_of as this should only used for one training phase\n    # no override for activate_deterministic_mode_after_this_phase as this should only used for one training phase\n\n    # additional to base_neural_ode_training_settings_class\n    beta_start_override: Optional[float] = None\n    alpha_mu_override: Optional[float] = None\n    alpha_sigma_override: Optional[float] = None\n    n_passes_override: Optional[int] = None\n    threshold_count_populated_dimensions_override: Optional[float] = None\n\n    include_reconstruction_loss_state0_override: Optional[bool] = None\n    include_reconstruction_loss_outputs0_override: Optional[bool] = None\n    include_states_grad_loss_override: Optional[bool] = None\n    include_outputs_grad_loss_override: Optional[bool] = None\n    multi_shooting_condition_multiplier_override: Optional[float] = None\n\n    pre_training: base_neural_ode_pretraining_settings_class = field(default_factory=base_neural_ode_pretraining_settings_class)\n    main_training: List[latent_timestepper_training_settings] = field(\n        default_factory=lambda: [latent_timestepper_training_settings()]\n    )\n\n    @field_validator('main_training')\n    @classmethod\n    def set_overrides(cls, v, info: ValidationInfo):\n        default_class = latent_timestepper_training_settings()\n        for i, training_settings in enumerate(v):\n            for key in ['batch_size_override', 'batches_per_epoch_override', 'max_epochs_override', 'lr_start_override', \n                        'beta1_adam_override', 'beta2_adam_override', 'clip_grad_norm_override', \n                        'weight_decay_override', 'early_stopping_patience_override', 'early_stopping_threshold_override', \n                        'early_stopping_threshold_mode_override', 'reload_optimizer_override','solver_override', 'load_seq_len_override', \n                        'seq_len_train_override', 'use_adjoint_override', 'evaluate_at_control_times_override', 'solver_rtol_override', 'solver_atol_override', 'solver_step_size_override',\n                        'beta_start_override', 'alpha_mu_override', 'alpha_sigma_override',\n                        'n_passes_override', 'threshold_count_populated_dimensions_override',\n                        'include_reconstruction_loss_state0_override', 'include_reconstruction_loss_outputs0_override',\n                        'multi_shooting_condition_multiplier_override',\n                        'seq_len_increase_in_batches_override', 'seq_len_increase_abort_after_n_stable_epochs_override', 'solver_norm_override',\n                        'include_states_grad_loss_override', 'include_outputs_grad_loss_override']:\n                if info.data[key] is not None:\n                    # print warning if override is set and non-default value is used\n                    if v[i].__getattribute__(key.split('_override')[0]) != default_class.__getattribute__(key.split('_override')[0]):\n                        logging.warning(f'Overriding {key.split(\"_override\")[0]} with value {info.data[key]}')\n                    v[i].__setattr__(key.split('_override')[0], info.data[key])\n        return v\n\n    # make sure that activate_deterministic_mode_after_this_phase is only set for one phase\n    @field_validator('main_training')\n    @classmethod\n    def check_deterministic_mode(cls, v, info: ValidationInfo):\n        n = 0\n        for i, training_settings in enumerate(v):\n            if training_settings.activate_deterministic_mode_after_this_phase:\n                n += 1\n                if training_settings.alpha_mu &lt; 1.0:\n                    logging.warning('alpha_mu should be larger than some value, e.g. 1.0, when activating deterministic mode')\n                    logging.warning('otherwise excess states in latent ode are not learned to be ignored and the model fails')\n        if n &gt; 1:\n            raise ValueError('Only one phase can have activate_deterministic_mode_after_this_phase set to True')\n\n        return v\n</code></pre>"},{"location":"bnode_core/config/#bnode_core.config.base_latent_ode_nn_model_class","title":"<code>base_latent_ode_nn_model_class</code>","text":"<p>               Bases: <code>abstract_nn_model_class</code></p> <p>Wrapper that binds the latent ODE network to its training configuration.</p> <p>Attributes:</p> Name Type Description <code>network</code> <code>latent_ode_network_class</code> <p>Latent ODE network hyperparameters.</p> <code>training</code> <code>base_latent_ode_training_settings_class</code> <p>Latent ODE training configuration.</p> Source code in <code>src/bnode_core/config.py</code> <pre><code>@dataclass\nclass base_latent_ode_nn_model_class(abstract_nn_model_class):\n    \"\"\"\n    Wrapper that binds the latent ODE network to its training configuration.\n\n    Attributes:\n        network (latent_ode_network_class): Latent ODE network hyperparameters.\n        training (base_latent_ode_training_settings_class): Latent ODE training configuration.\n    \"\"\"\n    network: latent_ode_network_class = field(default_factory=latent_ode_network_class)\n    training: base_latent_ode_training_settings_class = field(default_factory=base_latent_ode_training_settings_class)\n</code></pre>"},{"location":"bnode_core/config/#bnode_core.config.train_test_config_class","title":"<code>train_test_config_class</code>","text":"<p>Runtime configuration for training/testing, hardware usage, and MLflow tracking.</p> <p>Attributes:</p> Name Type Description <code>nn_model</code> <code>abstract_nn_model_class</code> <p>Model configuration (network + training) to run.</p> <code>dataset_name</code> <code>str</code> <p>Name of the dataset configuration to use.</p> <code>mlflow_tracking_uri</code> <code>str</code> <p>MLflow tracking server URI. If None, mlflow runs without server (direct to <code>./mlruns</code>).</p> <code>mlflow_experiment_name</code> <code>str</code> <p>MLflow experiment name.</p> <code>use_amp</code> <code>bool</code> <p>Enable automatic mixed precision. Should not be used for NODE/BNODE models.</p> <code>use_cuda</code> <code>bool</code> <p>Use CUDA if available.</p> <code>raise_exception</code> <code>bool</code> <p>If True, re-raise exceptions for debugging. Otherwise, log and continue.</p> <code>batch_print_interval</code> <code>int</code> <p>Interval (in batches) for logging training progress.</p> <code>verbose</code> <code>bool</code> <p>Enable verbose logging.</p> <code>n_workers_train_loader</code> <code>int</code> <p>Number of workers for the training dataloader.</p> <code>n_workers_other_loaders</code> <code>int</code> <p>Number of workers for validation/test loaders.</p> <code>prefetch_factor</code> <code>int</code> <p>Prefetch factor for dataloaders.</p> Source code in <code>src/bnode_core/config.py</code> <pre><code>@dataclass\nclass train_test_config_class:\n    \"\"\"\n    Runtime configuration for training/testing, hardware usage, and MLflow tracking.\n\n    Attributes:\n        nn_model (abstract_nn_model_class): Model configuration (network + training) to run.\n        dataset_name (str): Name of the dataset configuration to use.\n        mlflow_tracking_uri (str): MLflow tracking server URI. If None, mlflow runs without server (direct to `./mlruns`).\n        mlflow_experiment_name (str): MLflow experiment name.\n        use_amp (bool): Enable automatic mixed precision. Should not be used for NODE/BNODE models.\n        use_cuda (bool): Use CUDA if available.\n        raise_exception (bool): If True, re-raise exceptions for debugging. Otherwise, log and continue.\n        batch_print_interval (int): Interval (in batches) for logging training progress.\n        verbose (bool): Enable verbose logging.\n        n_workers_train_loader (int): Number of workers for the training dataloader.\n        n_workers_other_loaders (int): Number of workers for validation/test loaders.\n        prefetch_factor (int): Prefetch factor for dataloaders.\n    \"\"\"\n    nn_model: abstract_nn_model_class = MISSING\n    dataset_path: Optional[str] = None\n    dataset_name: Optional[str] = None\n\n    mlflow_tracking_uri: Optional[str] = None\n    mlflow_experiment_name: str = 'Default'\n\n    use_amp: bool = False\n    use_cuda: bool = True\n    raise_exception: bool = True\n\n    batch_print_interval: int = 5\n    verbose: bool = False\n    n_workers_train_loader: int = 5\n    n_workers_other_loaders: int = 1\n    prefetch_factor: int = 2\n\n    @field_validator('dataset_path')\n    @classmethod\n    def check_dataset_path_and_name(cls, dataset_path, info: ValidationInfo):\n        if dataset_path is not None and info.data.get('dataset_name') is not None:\n            raise ValueError('Only one of dataset_path or dataset_name can be provided, not both')\n        return dataset_path\n\n    @field_validator('dataset_name')\n    @classmethod\n    def set_dataset_name_from_path(cls, v, info: ValidationInfo):\n        dataset_path = info.data.get('dataset_path')\n        if dataset_path is not None:\n            path = Path(dataset_path)\n            if not path.exists():\n                raise ValueError(f'dataset_path does not exist: {dataset_path}')\n            # Extract filename without extension as dataset_name\n            return path.stem\n        return v\n</code></pre>"},{"location":"bnode_core/config/#bnode_core.config.load_latent_ode_config_class","title":"<code>load_latent_ode_config_class</code>","text":"<p>Configuration for loading a trained latent ODE model and its associated artifacts.</p> <p>Gotchas:</p> <ul> <li>Provide either mlflow_run_id or model_directory; at least one must be set.</li> </ul> <p>Attributes:</p> Name Type Description <code>mlflow_tracking_uri</code> <code>str</code> <p>MLflow tracking server URI. If None, mlflow runs without server (direct to <code>./mlruns</code>).</p> <code>model_directory</code> <code>Optional[str]</code> <p>Local directory containing a model to load.</p> <code>mlflow_run_id</code> <code>Optional[str]</code> <p>MLflow run ID to fetch the model from tracking.</p> <code>model_checkpoint_path</code> <code>Optional[str]</code> <p>Direct path to a model checkpoint file.</p> <code>config_path</code> <code>Optional[str]</code> <p>Path to a saved Hydra config to reproduce settings.</p> <code>dataset_path</code> <code>Optional[str]</code> <p>Path to a dataset used during export/evaluation.</p> Source code in <code>src/bnode_core/config.py</code> <pre><code>@dataclass\nclass load_latent_ode_config_class:\n    \"\"\"\n    Configuration for loading a trained latent ODE model and its associated artifacts.\n\n    Gotchas:\n\n    - Provide either mlflow_run_id or model_directory; at least one must be set.\n\n    Attributes:\n        mlflow_tracking_uri (str): MLflow tracking server URI. If None, mlflow runs without server (direct to `./mlruns`).\n        model_directory (Optional[str]): Local directory containing a model to load.\n        mlflow_run_id (Optional[str]): MLflow run ID to fetch the model from tracking.\n        model_checkpoint_path (Optional[str]): Direct path to a model checkpoint file.\n        config_path (Optional[str]): Path to a saved Hydra config to reproduce settings.\n        dataset_path (Optional[str]): Path to a dataset used during export/evaluation.\n    \"\"\"\n    mlflow_tracking_uri: Optional[str] = None\n    model_directory: Optional[str] = None\n    mlflow_run_id: Optional[str] = None\n    model_checkpoint_path: Optional[str] = None\n    config_path: Optional[str] = None # should be the dataset saved from hydra, as we do not validate the dataclass\n    dataset_path: Optional[str] = None \n\n    @field_validator('mlflow_run_id')\n    @classmethod\n    def check_deterministic_mode(cls, v, info: ValidationInfo):\n        if (v == None) and (info.data['model_directory'] == None):\n            raise ValueError('Either the mlflow run id or the model directory must be provided!')\n        return v\n    pass\n</code></pre>"},{"location":"bnode_core/config/#bnode_core.config.onnx_export_config_class","title":"<code>onnx_export_config_class</code>","text":"<p>               Bases: <code>load_latent_ode_config_class</code></p> <p>Settings for exporting a latent ODE model to ONNX format.</p> <p>Attributes:</p> Name Type Description <code>output_dir</code> <code>Optional[str]</code> <p>Output directory for the exported ONNX model and assets.</p> <code>model_directory</code> <code>Optional[str]</code> <p>Local directory containing a model to load.</p> <code>mlflow_run_id</code> <code>Optional[str]</code> <p>MLflow run ID to fetch the model from tracking</p> <code>model_checkpoint_path</code> <code>Optional[str]</code> <p>Direct path to a model checkpoint file.</p> <code>config_path</code> <code>Optional[str]</code> <p>Path to a saved Hydra config to reproduce settings.</p> <code>dataset_path</code> <code>Optional[str]</code> <p>Path to a dataset used during export/evaluation.</p> Source code in <code>src/bnode_core/config.py</code> <pre><code>@dataclass\nclass onnx_export_config_class(load_latent_ode_config_class):\n    \"\"\"\n    Settings for exporting a latent ODE model to ONNX format.\n\n    Attributes:\n        output_dir (Optional[str]): Output directory for the exported ONNX model and assets.\n        model_directory (Optional[str]): Local directory containing a model to load.\n        mlflow_run_id (Optional[str]): MLflow run ID to fetch the model from tracking\n        model_checkpoint_path (Optional[str]): Direct path to a model checkpoint file.\n        config_path (Optional[str]): Path to a saved Hydra config to reproduce settings.\n        dataset_path (Optional[str]): Path to a dataset used during export/evaluation.\n    \"\"\"\n    output_dir: Optional[str] = None\n    model_directory: Optional[str] = None\n    mlflow_run_id: Optional[str] = None # which model checkpoint to load\n    mlflow_tracking_uri: Optional[str] = None\n    model_checkpoint_path: Optional[str] = None\n    config_path: Optional[str] = None\n    dataset_path: Optional[str] = None\n\n    @model_validator(mode='after')\n    def _check_exclusive_model_source(self):\n        if self.model_directory is not None and self.mlflow_run_id is not None:\n            raise ValueError('Only one of model_directory or mlflow_run_id can be provided, not both')\n        return self\n    @model_validator(mode='after')\n    def _check_mlflow_tracking_uri(self):\n        if self.mlflow_run_id is not None and self.mlflow_tracking_uri is None:\n            raise ValueError('mlflow_tracking_uri must be provided when mlflow_run_id is used')\n        return self\n</code></pre>"},{"location":"bnode_core/config/#bnode_core.config.get_config_store","title":"<code>get_config_store() -&gt; ConfigStore</code>","text":"<p>Registers all configuration dataclasses with Hydra's ConfigStore.</p> <p>Returns:</p> <pre><code>cs: ConfigStore instance with registered configurations.\n</code></pre> Source code in <code>src/bnode_core/config.py</code> <pre><code>def get_config_store() -&gt; ConfigStore:\n    \"\"\"\n    Registers all configuration dataclasses with Hydra's ConfigStore.\n\n    Returns:\n\n        cs: ConfigStore instance with registered configurations.    \n    \"\"\"\n    cs = ConfigStore.instance()\n    cs.store(name='base_data_gen', node=data_gen_config)\n    cs.store(group='pModel', name='base_pModel', node=base_pModelClass)\n\n    cs.store(name='base_train_test', node=train_test_config_class)\n    cs.store(group='nn_model', name='base_nn_model', node=base_nn_model_class)\n    #pels_vae_linear\n    cs.store(group='nn_model', name='pels_vae', \n            node = base_nn_model_class(\n                network=pels_vae_network_class(), \n                training=pels_vae_training_settings_class()\n            ),\n        )\n    # neural_ode\n    cs.store(group='nn_model', name='neural_ode_base',\n                node = base_ode_nn_model_class(\n                    network=neural_ode_network_class(),\n                    training=base_neural_ode_training_settings_class()\n                ),\n            )\n    cs.store(group='nn_model', name='latent_ode_base',\n                node = base_latent_ode_nn_model_class(\n                    network=latent_ode_network_class(),\n                    training=base_latent_ode_training_settings_class()\n                ),\n            )\n\n    # onnx export\n    cs.store(name='base_onnx_export', node=onnx_export_config_class)\n    return cs\n</code></pre>"},{"location":"bnode_core/config/#bnode_core.config.convert_cfg_to_dataclass","title":"<code>convert_cfg_to_dataclass(cfg: DictConfig) -&gt; dataclass</code>","text":"<p>Converts a hydra config object to a dataclass</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>hydra config object / that is omegaconf.dictconfig.DictConfig</p> required <p>Returns:</p> Name Type Description <code>cfg</code> <code>dataclass</code> <p>dataclass</p> Source code in <code>src/bnode_core/config.py</code> <pre><code>def convert_cfg_to_dataclass(cfg: DictConfig) -&gt; dataclass:\n    '''\n    Converts a hydra config object to a dataclass\n\n    Args:\n        cfg: hydra config object / that is omegaconf.dictconfig.DictConfig\n\n    Returns:\n        cfg: dataclass\n    '''\n    logging.info('Validating config...')\n    cfg = OmegaConf.to_object(cfg)\n    logging.info('Validatied config and converted to dataclass')\n    return cfg\n</code></pre>"},{"location":"bnode_core/config/#bnode_core.config.save_dataclass_as_yaml","title":"<code>save_dataclass_as_yaml(cfg: dataclass, path: str)</code>","text":"<p>Saves a dataclass as a yaml file</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>dataclass</code> <p>dataclass</p> required <code>path</code> <code>str</code> <p>path to save yaml file</p> required Source code in <code>src/bnode_core/config.py</code> <pre><code>def save_dataclass_as_yaml(cfg: dataclass, path: str):\n    '''\n    Saves a dataclass as a yaml file\n\n    Args:\n        cfg: dataclass\n        path: path to save yaml file\n    '''\n    with open(path, 'w') as f:\n        yaml.dump(asdict(cfg), f)\n</code></pre>"},{"location":"bnode_core/filepaths/","title":"Filepaths","text":""},{"location":"bnode_core/filepaths/#bnode_core.filepaths","title":"<code>bnode_core.filepaths</code>","text":"<p>Utilities for constructing, naming, and resolving files and directories used by bnode projects. This module centralizes path conventions and discovery for:</p> <ul> <li>Auto-recognition of the configuration directory based on the current working   directory or CLI overrides. Using little \"hint\" files, the code can determine if it is   running inside a bnode package repository or a standalone project.</li> <li>Creation and discovery of data folders for raw data and datasets.</li> <li>Canonical naming for raw-data and dataset artifacts derived from the data   generation configuration. This includes naming conventions that reflect   model name, version, if initial states, parameters, and controls are sampled,   sampling strategies itself, and dataset sizes.</li> <li>Paths to Hydra runtime output artifacts (models, optimizers, datasets).</li> <li>Resolution of MLflow artifact URIs to local filesystem paths via the   MLFLOW_ARTIFACTS_DESTINATION environment variable.</li> </ul>"},{"location":"bnode_core/filepaths/#bnode_core.filepaths.config_dir_auto_recognize","title":"<code>config_dir_auto_recognize() -&gt; Path</code>","text":"<p>Auto-recognize and return the project's configuration directory.</p> <p>The current working directory is inspected for known locations of the configuration directory.  CLI flags can be used to override discovery and delegate resolution to Hydra.</p> <p>Returns:</p> Type Description <code>Path</code> <p>Path | None: Path to the discovered configuration directory. Returns</p> <code>Path</code> <p>None when --help is requested, or when CLI flags indicate that a config</p> <code>Path</code> <p>path will be provided externally (so Hydra may handle it).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no configuration directory can be found and no CLI flag suggests that the path will be provided manually.</p> Notes <p>Search order: 1. If \"--help\" or \"-h\" present, print help and return None. 2. If \".bnode_package_repo\" exists and \"resources/config/\" exists, return it. 3. Else if \"./config/\" exists, return it. 4. Else, if \"-cp\"/\"--config-path\" and / or \"-cn/ --config-name\" are present,    return None to allow Hydra to handle the path. 5. Otherwise, log an error and raise ValueError.</p> Source code in <code>src/bnode_core/filepaths.py</code> <pre><code>def config_dir_auto_recognize() -&gt; Path:\n    \"\"\"Auto-recognize and return the project's configuration directory.\n\n    The current working directory is inspected for known locations of the configuration directory. \n    CLI flags can be used to override discovery and delegate resolution to Hydra.\n\n    Returns:\n        Path | None: Path to the discovered configuration directory. Returns\n        None when --help is requested, or when CLI flags indicate that a config\n        path will be provided externally (so Hydra may handle it).\n\n    Raises:\n        ValueError: If no configuration directory can be found and no CLI flag suggests that the path will be provided manually.\n\n    Notes:\n        Search order:\n        1. If \"--help\" or \"-h\" present, print help and return None.\n        2. If \".bnode_package_repo\" exists and \"resources/config/\" exists, return it.\n        3. Else if \"./config/\" exists, return it.\n        4. Else, if \"-cp\"/\"--config-path\" and / or \"-cn/ --config-name\" are present,\n           return None to allow Hydra to handle the path.\n        5. Otherwise, log an error and raise ValueError.\n    \"\"\"\n    msg = ''\n    return_path = None\n    if Path('.bnode_package_repo').exists():\n        if Path('resources/config/').exists():\n            return_path = Path('resources/config/')\n        else:\n            msg += 'Even though .bnode_package_repo file exists, no config directory found in resources/config/.\\n'\n    else:\n        if Path('./config/').exists():\n            return_path =  Path('./config/')\n        else:\n            msg += 'No config directory found in the standard ./config/ location.\\n'\n\n    if '--help' in sys.argv or '-h' in sys.argv or '--hydra-help' in sys.argv:\n        print('The config directory is auto-recognized based on the current working directory.')\n        print('You can override this behavior by providing the config path via --config-path or --config-dir CLI arguments.')\n        print('To access hydra help, use --hydra-help.')\n\n    if return_path is not None:\n        return return_path\n\n    # Check if user provided config path via CLI args\n    raise_error = True\n    if '-cp' in sys.argv or '--config-path' in sys.argv:\n        raise_error = False\n    elif '--config-dir' in sys.argv or '-cd' in sys.argv:\n        # we assume that the user provided also a config name in this case\n        raise_error = False\n    if raise_error:\n        msg += 'Please ensure you are in a correct working directory or provide the config path manually.'\n        logging.error(msg)\n        raise ValueError(msg)\n    else:\n        return Path(\"\")  # dummy return to satisfy function signature\n</code></pre>"},{"location":"bnode_core/filepaths/#bnode_core.filepaths.create_path","title":"<code>create_path(path: Path, log: bool) -&gt; None</code>","text":"<p>Create a directory if it does not exist.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Directory path to create.</p> required <code>log</code> <code>bool</code> <p>Whether to log the creation/exists message.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/bnode_core/filepaths.py</code> <pre><code>def create_path(path: Path, log: bool) -&gt; None:\n    \"\"\"Create a directory if it does not exist.\n\n    Args:\n        path (Path): Directory path to create.\n        log (bool): Whether to log the creation/exists message.\n\n    Returns:\n        None\n    \"\"\"\n    if not path.exists():\n        path.mkdir(parents=True)\n        if log:\n            logging.info('Created path: {}'.format(path))\n    else:\n        if log:\n            logging.info('Path already exists: {}'.format(path))\n</code></pre>"},{"location":"bnode_core/filepaths/#bnode_core.filepaths.log_overwriting_file","title":"<code>log_overwriting_file(path: Path) -&gt; None</code>","text":"<p>Log whether a file will be written or overwritten.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>File path for which to log the action.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/bnode_core/filepaths.py</code> <pre><code>def log_overwriting_file(path: Path) -&gt; None:\n    \"\"\"Log whether a file will be written or overwritten.\n\n    Args:\n        path (Path): File path for which to log the action.\n\n    Returns:\n        None\n    \"\"\"\n    if not path.exists():\n        logging.info('Writing on file: {}'.format(path))\n    else:\n        logging.info('Overwriting file: {}'.format(path))\n</code></pre>"},{"location":"bnode_core/filepaths/#bnode_core.filepaths.raw_data_name","title":"<code>raw_data_name(cfg: data_gen_config) -&gt; str</code>","text":"<p>Build the canonical base name for a raw-data artifact.</p> <p>The name is derived from the model name/version and optional sampling strategy flags present in the configuration.</p> Behavior <ul> <li>Always includes model name and version.</li> <li>Appends initial states sampling strategy if initial states are included. (E.g., '_s-R')</li> <li>Appends parameters sampling strategy if parameters are included. (E.g., '_p-R')</li> <li>Appends controls sampling strategy if controls are included. (E.g., '_c-RROCS')</li> <li>Appends '_c-Mo' suffix if controls are only for sampling and the actual used controls are extracted from the model.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>data_gen_config</code> <p>Data generation configuration.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Base name for raw-data artifacts.</p> Source code in <code>src/bnode_core/filepaths.py</code> <pre><code>def raw_data_name(cfg: data_gen_config) -&gt; str:\n    \"\"\"Build the canonical base name for a raw-data artifact.\n\n    The name is derived from the model name/version and optional sampling\n    strategy flags present in the configuration.\n\n    Behavior:\n        - Always includes model name and version.\n        - Appends initial states sampling strategy if initial states are included. (E.g., '_s-R')\n        - Appends parameters sampling strategy if parameters are included. (E.g., '_p-R')\n        - Appends controls sampling strategy if controls are included. (E.g., '_c-RROCS')\n        - Appends '_c-Mo' suffix if controls are only for sampling and the actual used controls are extracted\n        from the model.\n\n    Args:\n        cfg (data_gen_config): Data generation configuration.\n\n    Returns:\n        str: Base name for raw-data artifacts.\n    \"\"\"\n    RawData = cfg.pModel.RawData\n    name = RawData.modelName + '_' + RawData.versionName\n    if RawData.initial_states_include:\n        name = name + '_s-' + RawData.initial_states_sampling_strategy\n    if RawData.parameters_include:\n        name = name + '_p-' + RawData.parameters_sampling_strategy\n    if RawData.controls_include:\n        name = name + '_c-' + RawData.controls_sampling_strategy\n    if RawData.controls_only_for_sampling_extract_actual_from_model:\n        name = name + '_c-Mo'\n    return name\n</code></pre>"},{"location":"bnode_core/filepaths/#bnode_core.filepaths.dataset_name","title":"<code>dataset_name(cfg: data_gen_config, n_samples: int) -&gt; str</code>","text":"<p>Build the canonical dataset name for a given configuration and size.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>data_gen_config</code> <p>Data generation configuration.</p> required <code>n_samples</code> <code>int</code> <p>Number of samples in the dataset.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Dataset name including sample count and optional suffix.</p> Source code in <code>src/bnode_core/filepaths.py</code> <pre><code>def dataset_name(cfg: data_gen_config, n_samples: int) -&gt; str:\n    \"\"\"Build the canonical dataset name for a given configuration and size.\n\n    Args:\n        cfg (data_gen_config): Data generation configuration.\n        n_samples (int): Number of samples in the dataset.\n\n    Returns:\n        str: Dataset name including sample count and optional suffix.\n    \"\"\"\n    name = raw_data_name(cfg) + '__n-' + str(n_samples)\n    if cfg.pModel.dataset_prep.dataset_suffix is not None:\n        name = name + '_' + cfg.pModel.dataset_prep.dataset_suffix\n    return name\n</code></pre>"},{"location":"bnode_core/filepaths/#bnode_core.filepaths.dir_data","title":"<code>dir_data(log: bool = False) -&gt; Path</code>","text":"<p>Resolve the root data directory for the project.</p> <p>Search order: 1) If \".bnode_package_repo\" exists: \"./resources/data\". 2) Else if \"../../.surrogate_test_data_repo\" exists: \"../../data\". 3) Else: \"./data\".</p> <p>The directory is created if missing.</p> Note <p>In the future, it might be a good idea to switch to a more flexible directory assignment mechanism, e.g., via environment variables or configuration files.</p> <p>Parameters:</p> Name Type Description Default <code>log</code> <code>bool</code> <p>Whether to log directory creation/existence.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Root data directory.</p> Source code in <code>src/bnode_core/filepaths.py</code> <pre><code>def dir_data(log: bool = False) -&gt; Path:\n    \"\"\"Resolve the root data directory for the project.\n\n    Search order:\n    1) If \".bnode_package_repo\" exists: \"./resources/data\".\n    2) Else if \"../../.surrogate_test_data_repo\" exists: \"../../data\".\n    3) Else: \"./data\".\n\n    The directory is created if missing.\n\n    Note:\n        In the future, it might be a good idea to switch to a more flexible directory assignment mechanism,\n        e.g., via environment variables or configuration files.\n\n    Args:\n        log (bool): Whether to log directory creation/existence.\n\n    Returns:\n        Path: Root data directory.\n    \"\"\"\n    if Path('.bnode_package_repo').exists():\n        path = Path('./resources/data')\n    else:\n        path = Path('./data')\n    create_path(path, log)\n    return path\n</code></pre>"},{"location":"bnode_core/filepaths/#bnode_core.filepaths.dir_raw_data","title":"<code>dir_raw_data(cfg: data_gen_config, log: bool = False) -&gt; Path</code>","text":"<p>Return the directory in which raw data for the config is stored.</p> <p>The path includes a subdirectory named by <code>raw_data_name(cfg)</code> and is created on demand.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>data_gen_config</code> <p>Data generation configuration.</p> required <code>log</code> <code>bool</code> <p>Whether to log directory creation/existence.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Directory for raw data artifacts.</p> Source code in <code>src/bnode_core/filepaths.py</code> <pre><code>def dir_raw_data(cfg: data_gen_config, log: bool = False) -&gt; Path:\n    \"\"\"Return the directory in which raw data for the config is stored.\n\n    The path includes a subdirectory named by ``raw_data_name(cfg)`` and is\n    created on demand.\n\n    Args:\n        cfg (data_gen_config): Data generation configuration.\n        log (bool): Whether to log directory creation/existence.\n\n    Returns:\n        Path: Directory for raw data artifacts.\n    \"\"\"\n    path = dir_data() / 'raw_data' / raw_data_name(cfg)\n    create_path(path, log)\n    return path\n</code></pre>"},{"location":"bnode_core/filepaths/#bnode_core.filepaths.filepath_raw_data","title":"<code>filepath_raw_data(cfg: data_gen_config) -&gt; Path</code>","text":"<p>Return the path to the raw-data file for a configuration.</p> <p>If <code>cfg.pModel.RawData.raw_data_from_external_source</code> is True, the path is resolved inside the raw-data directory using the external file name. Otherwise, a default file name of the form <code>&lt;raw_data_name&gt;_raw_data.hdf5</code> is used.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>data_gen_config</code> <p>Data generation configuration.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Path to the raw-data file.</p> Source code in <code>src/bnode_core/filepaths.py</code> <pre><code>def filepath_raw_data(cfg: data_gen_config) -&gt; Path:\n    \"\"\"Return the path to the raw-data file for a configuration.\n\n    If ``cfg.pModel.RawData.raw_data_from_external_source`` is True, the path is\n    resolved inside the raw-data directory using the external file name. Otherwise,\n    a default file name of the form ``&lt;raw_data_name&gt;_raw_data.hdf5`` is used.\n\n    Args:\n        cfg (data_gen_config): Data generation configuration.\n\n    Returns:\n        Path: Path to the raw-data file.\n    \"\"\"\n    if cfg.pModel.RawData.raw_data_from_external_source:\n        file = dir_raw_data(cfg) / cfg.pModel.RawData.raw_data_path\n    else:\n        file = dir_raw_data(cfg) / (raw_data_name(cfg) + '_raw_data.hdf5')\n    return file\n</code></pre>"},{"location":"bnode_core/filepaths/#bnode_core.filepaths.filepath_raw_data_config","title":"<code>filepath_raw_data_config(cfg: data_gen_config) -&gt; Path</code>","text":"<p>Return the path to the RawData configuration YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>data_gen_config</code> <p>Data generation configuration.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Path to the YAML configuration stored alongside raw-data.</p> Source code in <code>src/bnode_core/filepaths.py</code> <pre><code>def filepath_raw_data_config(cfg: data_gen_config) -&gt; Path:\n    \"\"\"Return the path to the RawData configuration YAML file.\n\n    Args:\n        cfg (data_gen_config): Data generation configuration.\n\n    Returns:\n        Path: Path to the YAML configuration stored alongside raw-data.\n    \"\"\"\n    file = dir_raw_data(cfg) / (raw_data_name(cfg) + '_RawData_config.yaml')\n    return file\n</code></pre>"},{"location":"bnode_core/filepaths/#bnode_core.filepaths.dir_datasets","title":"<code>dir_datasets(log: bool = False) -&gt; Path</code>","text":"<p>Return the root directory for datasets, creating it if missing.</p> <p>Parameters:</p> Name Type Description Default <code>log</code> <code>bool</code> <p>Whether to log directory creation/existence.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Root datasets directory.</p> Source code in <code>src/bnode_core/filepaths.py</code> <pre><code>def dir_datasets(log: bool = False) -&gt; Path:\n    \"\"\"Return the root directory for datasets, creating it if missing.\n\n    Args:\n        log (bool): Whether to log directory creation/existence.\n\n    Returns:\n        Path: Root datasets directory.\n    \"\"\"\n    path = dir_data() / 'datasets'\n    create_path(path, log)\n    return path\n</code></pre>"},{"location":"bnode_core/filepaths/#bnode_core.filepaths.dir_specific_dataset_from_cfg","title":"<code>dir_specific_dataset_from_cfg(cfg: data_gen_config, n_samples: int, log: bool = False) -&gt; Path</code>","text":"<p>Return the directory for a specific dataset derived from a config.</p> <p>The directory name is computed via :func:<code>dataset_name</code> and created if missing.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>data_gen_config</code> <p>Data generation configuration.</p> required <code>n_samples</code> <code>int</code> <p>Number of samples in the dataset.</p> required <code>log</code> <code>bool</code> <p>Whether to log directory creation/existence.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Directory for the specific dataset.</p> Source code in <code>src/bnode_core/filepaths.py</code> <pre><code>def dir_specific_dataset_from_cfg(cfg: data_gen_config, n_samples: int, log: bool = False) -&gt; Path:\n    \"\"\"Return the directory for a specific dataset derived from a config.\n\n    The directory name is computed via :func:`dataset_name` and created if missing.\n\n    Args:\n        cfg (data_gen_config): Data generation configuration.\n        n_samples (int): Number of samples in the dataset.\n        log (bool): Whether to log directory creation/existence.\n\n    Returns:\n        Path: Directory for the specific dataset.\n    \"\"\"\n    path = dir_datasets() / dataset_name(cfg, n_samples)\n    create_path(path, log)\n    return path\n</code></pre>"},{"location":"bnode_core/filepaths/#bnode_core.filepaths.dir_specific_dataset_from_name","title":"<code>dir_specific_dataset_from_name(name: str, log: bool = False) -&gt; Path</code>","text":"<p>Return the directory for a specific dataset by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Dataset name.</p> required <code>log</code> <code>bool</code> <p>Unused flag kept for API symmetry; directory is not created here.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Directory path for the dataset name.</p> Source code in <code>src/bnode_core/filepaths.py</code> <pre><code>def dir_specific_dataset_from_name(name: str, log: bool = False) -&gt; Path:\n    \"\"\"Return the directory for a specific dataset by name.\n\n    Args:\n        name (str): Dataset name.\n        log (bool): Unused flag kept for API symmetry; directory is not created here.\n\n    Returns:\n        Path: Directory path for the dataset name.\n    \"\"\"\n    path = dir_datasets() / name\n    return path\n</code></pre>"},{"location":"bnode_core/filepaths/#bnode_core.filepaths.filepath_dataset","title":"<code>filepath_dataset(cfg: data_gen_config, n_samples: int) -&gt; Path</code>","text":"<p>Return the path to a dataset file for a given config and size.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>data_gen_config</code> <p>Data generation configuration.</p> required <code>n_samples</code> <code>int</code> <p>Number of samples in the dataset.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>HDF5 dataset file path.</p> Source code in <code>src/bnode_core/filepaths.py</code> <pre><code>def filepath_dataset(cfg: data_gen_config, n_samples: int) -&gt; Path:\n    \"\"\"Return the path to a dataset file for a given config and size.\n\n    Args:\n        cfg (data_gen_config): Data generation configuration.\n        n_samples (int): Number of samples in the dataset.\n\n    Returns:\n        Path: HDF5 dataset file path.\n    \"\"\"\n    file = dir_specific_dataset_from_cfg(cfg, n_samples) / (dataset_name(cfg, n_samples) + '_dataset.hdf5')\n    return file\n</code></pre>"},{"location":"bnode_core/filepaths/#bnode_core.filepaths.filepath_dataset_config","title":"<code>filepath_dataset_config(cfg: data_gen_config, n_samples: int) -&gt; Path</code>","text":"<p>Return the path to the pModel configuration YAML for the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>data_gen_config</code> <p>Data generation configuration.</p> required <code>n_samples</code> <code>int</code> <p>Number of samples in the dataset.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>YAML configuration file path stored with the dataset.</p> Source code in <code>src/bnode_core/filepaths.py</code> <pre><code>def filepath_dataset_config(cfg: data_gen_config, n_samples: int) -&gt; Path:\n    \"\"\"Return the path to the pModel configuration YAML for the dataset.\n\n    Args:\n        cfg (data_gen_config): Data generation configuration.\n        n_samples (int): Number of samples in the dataset.\n\n    Returns:\n        Path: YAML configuration file path stored with the dataset.\n    \"\"\"\n    file = dir_specific_dataset_from_cfg(cfg, n_samples) / (dataset_name(cfg, n_samples) + '_pModel_config.yaml')\n    return file\n</code></pre>"},{"location":"bnode_core/filepaths/#bnode_core.filepaths.filepath_dataset_from_name","title":"<code>filepath_dataset_from_name(name: str) -&gt; Path</code>","text":"<p>Return the HDF5 dataset file path for a given dataset name. Assumes that the dataset is located in the standard dataset directory structure of this package.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Dataset name.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>HDF5 dataset file path.</p> Source code in <code>src/bnode_core/filepaths.py</code> <pre><code>def filepath_dataset_from_name(name: str) -&gt; Path:\n    \"\"\"Return the HDF5 dataset file path for a given dataset name. Assumes that the dataset is located\n    in the standard dataset directory structure of this package.\n\n    Args:\n        name (str): Dataset name.\n\n    Returns:\n        Path: HDF5 dataset file path.\n    \"\"\"\n    return dir_specific_dataset_from_name(name) / (name + '_dataset.hdf5')\n</code></pre>"},{"location":"bnode_core/filepaths/#bnode_core.filepaths.filepath_dataset_from_config","title":"<code>filepath_dataset_from_config(dataset_name: str, dataset_path: str) -&gt; Path</code>","text":"<p>Return the HDF5 dataset file path for a given dataset name or explicit path.</p> <p>If <code>dataset_path</code> is provided, it is used directly. Otherwise, the path is constructed based on the standard dataset directory structure.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>Dataset name.</p> required <code>dataset_path</code> <code>str | None</code> <p>Optional explicit dataset file path.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>HDF5 dataset file path.</p> Source code in <code>src/bnode_core/filepaths.py</code> <pre><code>def filepath_dataset_from_config(dataset_name: str, dataset_path: str) -&gt; Path:\n    \"\"\"Return the HDF5 dataset file path for a given dataset name or explicit path.\n\n    If ``dataset_path`` is provided, it is used directly. Otherwise, the path is\n    constructed based on the standard dataset directory structure.\n\n    Args:\n        dataset_name (str): Dataset name.\n        dataset_path (str | None): Optional explicit dataset file path.\n\n    Returns:\n        Path : HDF5 dataset file path.\n    \"\"\"\n    if dataset_path is not None:\n        return Path(dataset_path)\n    else:\n        return filepath_dataset_from_name(dataset_name)\n</code></pre>"},{"location":"bnode_core/filepaths/#bnode_core.filepaths.filepath_dataset_config_from_name","title":"<code>filepath_dataset_config_from_name(name: str) -&gt; Path</code>","text":"<p>Return the pModel configuration YAML path for a dataset name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Dataset name.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>YAML configuration file path.</p> Source code in <code>src/bnode_core/filepaths.py</code> <pre><code>def filepath_dataset_config_from_name(name: str) -&gt; Path:\n    \"\"\"Return the pModel configuration YAML path for a dataset name.\n\n    Args:\n        name (str): Dataset name.\n\n    Returns:\n        Path: YAML configuration file path.\n    \"\"\"\n    file = dir_specific_dataset_from_name(name) / (name + '_pModel_config.yaml')\n    return file\n</code></pre>"},{"location":"bnode_core/filepaths/#bnode_core.filepaths.dir_current_hydra_output","title":"<code>dir_current_hydra_output() -&gt; Path</code>","text":"<p>Return the current Hydra runtime output directory.</p> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Path to Hydra's current run output directory.</p> Source code in <code>src/bnode_core/filepaths.py</code> <pre><code>def dir_current_hydra_output() -&gt; Path:\n    \"\"\"Return the current Hydra runtime output directory.\n\n    Returns:\n        Path: Path to Hydra's current run output directory.\n    \"\"\"\n    return Path(hydra.core.hydra_config.HydraConfig.get().runtime.output_dir)\n</code></pre>"},{"location":"bnode_core/filepaths/#bnode_core.filepaths.filepath_model_current_hydra_output","title":"<code>filepath_model_current_hydra_output(phase: int | None = None) -&gt; Path</code>","text":"<p>Return the model checkpoint path in the current Hydra output directory.</p> <p>Parameters:</p> Name Type Description Default <code>phase</code> <code>int | None</code> <p>Optional training phase index. When provided, the filename is \"model_phase_{phase}.pt\"; otherwise \"model.pt\".</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Model checkpoint file path.</p> Source code in <code>src/bnode_core/filepaths.py</code> <pre><code>def filepath_model_current_hydra_output(phase: int | None = None) -&gt; Path:\n    \"\"\"Return the model checkpoint path in the current Hydra output directory.\n\n    Args:\n        phase (int | None): Optional training phase index. When provided,\n            the filename is \"model_phase_{phase}.pt\"; otherwise \"model.pt\".\n\n    Returns:\n        Path: Model checkpoint file path.\n    \"\"\"\n    if phase is not None:\n        return dir_current_hydra_output() / 'model_phase_{}.pt'.format(phase)\n    else:\n        return dir_current_hydra_output() / 'model.pt'\n</code></pre>"},{"location":"bnode_core/filepaths/#bnode_core.filepaths.filepath_pretrained_model_current_hydra_output","title":"<code>filepath_pretrained_model_current_hydra_output() -&gt; Path</code>","text":"<p>Return the pretrained model file path in the current Hydra output dir.</p> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>\"model_pretrained.pt\" path.</p> Source code in <code>src/bnode_core/filepaths.py</code> <pre><code>def filepath_pretrained_model_current_hydra_output() -&gt; Path:\n    \"\"\"Return the pretrained model file path in the current Hydra output dir.\n\n    Returns:\n        Path: \"model_pretrained.pt\" path.\n    \"\"\"\n    return dir_current_hydra_output() / 'model_pretrained.pt'\n</code></pre>"},{"location":"bnode_core/filepaths/#bnode_core.filepaths.filepath_dataset_current_hydra_output","title":"<code>filepath_dataset_current_hydra_output() -&gt; Path</code>","text":"<p>Return the dataset file path in the current Hydra output directory.</p> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>\"dataset.hdf5\" path.</p> Source code in <code>src/bnode_core/filepaths.py</code> <pre><code>def filepath_dataset_current_hydra_output() -&gt; Path:\n    \"\"\"Return the dataset file path in the current Hydra output directory.\n\n    Returns:\n        Path: \"dataset.hdf5\" path.\n    \"\"\"\n    return dir_current_hydra_output() / 'dataset.hdf5'\n</code></pre>"},{"location":"bnode_core/filepaths/#bnode_core.filepaths.filepath_optimizer_current_hydra_output","title":"<code>filepath_optimizer_current_hydra_output(phase: int | None = None) -&gt; Path</code>","text":"<p>Return the optimizer state dict path in the current Hydra output dir.</p> <p>Parameters:</p> Name Type Description Default <code>phase</code> <code>int | None</code> <p>Optional training phase index. When provided, the filename is \"optimizer_phase_{phase}.pt\"; otherwise \"optimizer.pt\".</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Optimizer checkpoint file path.</p> Source code in <code>src/bnode_core/filepaths.py</code> <pre><code>def filepath_optimizer_current_hydra_output(phase: int | None = None) -&gt; Path:\n    \"\"\"Return the optimizer state dict path in the current Hydra output dir.\n\n    Args:\n        phase (int | None): Optional training phase index. When provided,\n            the filename is \"optimizer_phase_{phase}.pt\"; otherwise \"optimizer.pt\".\n\n    Returns:\n        Path: Optimizer checkpoint file path.\n    \"\"\"\n    if phase is not None:\n        return dir_current_hydra_output() / 'optimizer_phase_{}.pt'.format(phase)\n    else:\n        return dir_current_hydra_output() / 'optimizer.pt'\n</code></pre>"},{"location":"bnode_core/filepaths/#bnode_core.filepaths.filepath_from_ml_artifacts_uri","title":"<code>filepath_from_ml_artifacts_uri(mlflow_uri: str) -&gt; Path</code>","text":"<p>Resolve an MLflow artifacts URI to a local filesystem path.</p> <p>The base directory is read from the <code>MLFLOW_ARTIFACTS_DESTINATION</code> environment variable. The leading \"file://\" is stripped from the env var value if present. The \"mlflow-artifacts:/\" prefix in <code>mlflow_uri</code> is also removed before joining.</p> <p>Parameters:</p> Name Type Description Default <code>mlflow_uri</code> <code>str</code> <p>An MLflow artifacts URI (e.g., \"mlflow-artifacts:///artifacts/...\"), or a relative path component under the artifacts root. required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Resolved local filesystem path.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>MLFLOW_ARTIFACTS_DESTINATION</code> is not set.</p> Source code in <code>src/bnode_core/filepaths.py</code> <pre><code>def filepath_from_ml_artifacts_uri(mlflow_uri: str) -&gt; Path:\n    \"\"\"Resolve an MLflow artifacts URI to a local filesystem path.\n\n    The base directory is read from the ``MLFLOW_ARTIFACTS_DESTINATION``\n    environment variable. The leading \"file://\" is stripped from the env var\n    value if present. The \"mlflow-artifacts:/\" prefix in ``mlflow_uri`` is also\n    removed before joining.\n\n    Args:\n        mlflow_uri (str): An MLflow artifacts URI (e.g.,\n            \"mlflow-artifacts:/&lt;experiment&gt;/&lt;run&gt;/artifacts/...\"), or a relative\n            path component under the artifacts root.\n\n    Returns:\n        Path: Resolved local filesystem path.\n\n    Raises:\n        ValueError: If ``MLFLOW_ARTIFACTS_DESTINATION`` is not set.\n    \"\"\"\n    try:\n        # try resolving the path from the environment variable\n        _dir = os.environ['MLFLOW_ARTIFACTS_DESTINATION']\n        _dir = _dir.replace('file://', '')\n    except:\n        logging.error('MLFLOW_ARTIFACTS_DESTINATION not set')\n        logging.error('please set the environment variable MLFLOW_ARTIFACTS_DESTINATION to the path where the mlflow artifacts are stored')\n        logging.error('or provide the full path to the dataset')\n        raise ValueError('MLFLOW_ARTIFACTS_DESTINATION not set as environment variable')\n    mlflow_uri = _dir + os.sep + mlflow_uri.replace('mlflow-artifacts:/', '')\n    mlflow_uri = Path(mlflow_uri)\n    return mlflow_uri\n</code></pre>"},{"location":"bnode_core/filepaths/#bnode_core.filepaths.filepath_from_local_or_ml_artifacts","title":"<code>filepath_from_local_or_ml_artifacts(mlflow_path: str) -&gt; Path</code>","text":"<p>Return a local Path from either a local path or an MLflow artifacts URI. If the provided path starts with \"mlflow-artifacts:\", it is resolved via :func:<code>filepath_from_ml_artifacts_uri</code>. Otherwise, it is treated as a local filesystem path.</p> <p>Parameters:</p> Name Type Description Default <code>mlflow_path</code> <code>str</code> <p>Local filesystem path or an MLflow artifacts URI beginning with \"mlflow-artifacts:\".</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Local filesystem path.</p> Source code in <code>src/bnode_core/filepaths.py</code> <pre><code>def filepath_from_local_or_ml_artifacts(mlflow_path: str) -&gt; Path:\n    \"\"\"Return a local Path from either a local path or an MLflow artifacts URI.\n    If the provided path starts with \"mlflow-artifacts:\", it is resolved via\n    :func:`filepath_from_ml_artifacts_uri`. Otherwise, it is treated as a local\n    filesystem path.\n\n    Args:\n        mlflow_path (str): Local filesystem path or an MLflow artifacts URI\n            beginning with \"mlflow-artifacts:\".\n\n    Returns:\n        Path: Local filesystem path.\n    \"\"\"\n    if mlflow_path.startswith('mlflow-artifacts:'):\n        _path = filepath_from_ml_artifacts_uri(mlflow_path)\n    else:\n        _path = mlflow_path\n    return Path(_path)\n</code></pre>"},{"location":"bnode_core/data_generation/data_preperation/","title":"Data Preparation","text":""},{"location":"bnode_core/data_generation/data_preperation/#bnode_core.data_generation.data_preperation","title":"<code>bnode_core.data_generation.data_preperation</code>","text":"<p>Dataset preparation module for BNODE raw data processing.</p>"},{"location":"bnode_core/data_generation/data_preperation/#bnode_core.data_generation.data_preperation--module-description","title":"Module Description","text":"<p>This module processes raw FMU simulation data and creates prepared dataset files with  train/validation/test splits. It applies transformations, filters trajectories, selects  variables, and generates multiple dataset sizes from a single raw data source.</p>"},{"location":"bnode_core/data_generation/data_preperation/#bnode_core.data_generation.data_preperation--command-line-usage","title":"Command-line Usage","text":"<pre><code>With uv (recommended):\n    uv run data_preperation [hydra_overrides]\n\nIn activated (uv) virtual environment:\n    data_preperation [hydra_overrides]\n\nDirect Python execution:\n    python -m bnode_core.data_generation.data_preperation [hydra_overrides]\n</code></pre>"},{"location":"bnode_core/data_generation/data_preperation/#bnode_core.data_generation.data_preperation--example-commands","title":"Example Commands","text":"<pre><code># Generate datasets with 128, 512, and 2048 samples\nuv run data_preperation pModel.dataset_prep.n_samples=[128,512,2048]\n\n# Apply temperature conversion transform\nuv run data_preperation pModel.dataset_prep.transforms.temperature=temperature_k_to_degC\n\n# Change validation/test fractions\nuv run data_preperation pModel.dataset_prep.validation_fraction=0.15 pModel.dataset_prep.test_fraction=0.15\n</code></pre>"},{"location":"bnode_core/data_generation/data_preperation/#bnode_core.data_generation.data_preperation--what-this-module-does","title":"What This Module Does","text":"<pre><code>1. Loads raw data HDF5 file and validates configuration\n2. Removes failed simulation runs and filters trajectories by limits/expressions\n3. Applies transformations (unit conversions, numerical derivatives, custom operations)\n4. Selects requested variables (states, controls, outputs, parameters)\n5. Extracts requested time window from trajectories\n6. Creates train/validation/test splits with consistent common sets across dataset sizes\n7. Generates multiple HDF5 dataset files (one per n_samples value)\n8. Saves dataset-specific YAML configs alongside each HDF5 file\n</code></pre> <p>See main() function for entry point and run_data_preperation() for the complete pipeline.</p>"},{"location":"bnode_core/data_generation/data_preperation/#bnode_core.data_generation.data_preperation--key-features","title":"Key Features","text":"<pre><code>- Safely modifies data in temporary file before creating final datasets\n- Common validation/test sets ensure fair comparison across dataset sizes\n- Incremental dataset creation allows generating multiple sizes efficiently\n- Comprehensive logging of filtering, transformation, and split statistics\n- Supports external raw data sources (skip config validation)\n</code></pre>"},{"location":"bnode_core/data_generation/data_preperation/#bnode_core.data_generation.data_preperation--configuration","title":"Configuration","text":"<pre><code>Uses Hydra for configuration management. Config loaded from 'data_generation.yaml'.\nKey config sections: pModel.RawData (raw data metadata) and pModel.dataset_prep \n(preparation settings).\n</code></pre>"},{"location":"bnode_core/data_generation/data_preperation/#bnode_core.data_generation.data_preperation.load_and_validate_raw_data","title":"<code>load_and_validate_raw_data(cfg: data_gen_config) -&gt; Tuple[h5py.File, Optional[RawDataClass]]</code>","text":"<p>Load raw data HDF5 file and validate its configuration against current config.</p> <p>Loads the raw data file and its companion YAML config, validates the config structure,  and compares it to the current configuration. If differences are found (excluding  creation_date), logs warnings and overwrites the current config with the loaded one  to ensure consistency.</p> <p>For external raw data sources (cfg.pModel.RawData.raw_data_from_external_source=True),  skips config loading and validation.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>data_gen_config</code> <p>Data generation configuration containing:</p> <ul> <li>pModel.RawData.pModel_name: model identifier for file paths</li> <li>pModel.RawData.version: dataset version identifier</li> <li>pModel.RawData.raw_data_from_external_source: if True, skip config validation</li> </ul> required <p>Returns:</p> Type Description <code>Tuple[File, Optional[RawDataClass]]</code> <p>Tuple of (raw_data, raw_data_config) where:</p> <ul> <li>raw_data: Open h5py.File handle to the raw data HDF5 file</li> <li>raw_data_config: RawDataClass dataclass instance with validated config,    or None if raw_data_from_external_source is True</li> </ul> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If raw data file or config file does not exist (unless external source).</p> Notes <ul> <li>Config comparison excludes creation_date field to avoid false mismatches.</li> <li>If config mismatch is detected, logs which keys differ and overwrites cfg.pModel.RawData.</li> <li>The raw data file is returned open; caller is responsible for closing it.</li> </ul> Source code in <code>src/bnode_core/data_generation/data_preperation.py</code> <pre><code>def load_and_validate_raw_data(cfg: data_gen_config) -&gt; Tuple[h5py.File, Optional[RawDataClass]]:\n    \"\"\"Load raw data HDF5 file and validate its configuration against current config.\n\n    Loads the raw data file and its companion YAML config, validates the config structure, \n    and compares it to the current configuration. If differences are found (excluding \n    creation_date), logs warnings and overwrites the current config with the loaded one \n    to ensure consistency.\n\n    For external raw data sources (cfg.pModel.RawData.raw_data_from_external_source=True), \n    skips config loading and validation.\n\n    Args:\n        cfg: Data generation configuration containing:\n\n            - pModel.RawData.pModel_name: model identifier for file paths\n            - pModel.RawData.version: dataset version identifier\n            - pModel.RawData.raw_data_from_external_source: if True, skip config validation\n\n    Returns:\n        Tuple of (raw_data, raw_data_config) where:\n\n            - raw_data: Open h5py.File handle to the raw data HDF5 file\n            - raw_data_config: RawDataClass dataclass instance with validated config, \n              or None if raw_data_from_external_source is True\n\n    Raises:\n        FileNotFoundError: If raw data file or config file does not exist (unless external source).\n\n    Notes:\n        - Config comparison excludes creation_date field to avoid false mismatches.\n        - If config mismatch is detected, logs which keys differ and overwrites cfg.pModel.RawData.\n        - The raw data file is returned open; caller is responsible for closing it.\n    \"\"\"\n    path_raw_data = filepath_raw_data(cfg)\n    path_raw_data_config = filepath_raw_data_config(cfg)\n\n    if not path_raw_data.exists():\n        raise FileNotFoundError(f'Raw data file does not exist: {path_raw_data}')\n    if not path_raw_data_config.exists() and not cfg.pModel.RawData.raw_data_from_external_source:\n        raise FileNotFoundError(f'Raw data config file does not exist: {path_raw_data_config}')\n\n    # load raw data config\n    if not cfg.pModel.RawData.raw_data_from_external_source:\n        logging.info('Loading raw data config from {}'.format(path_raw_data_config))\n        _raw_data_config_dict = OmegaConf.load(path_raw_data_config)\n        _raw_data_config_dict = OmegaConf.to_object(_raw_data_config_dict) # make dict\n        raw_data_config = RawDataClass(**_raw_data_config_dict) # validate and convert to dataclass\n        logging.info('Validated raw data config from {}'.format(path_raw_data_config))\n\n        # compare raw data config to actual config and raise errors / warnings\n        logging.info('Comparing raw data config to current config. Creating copy of raw data config without creation date for comparison.')\n        _raw_data_config_wo_creation_date = RawDataClass(**_raw_data_config_dict)\n        _raw_data_config_wo_creation_date.creation_date = None\n        _flag = False\n        if cfg.pModel.RawData != _raw_data_config_wo_creation_date:\n            for key in cfg.pModel.RawData.__dict__.keys():\n                if cfg.pModel.RawData.__dict__[key] != _raw_data_config_wo_creation_date.__dict__[key]:\n                    logging.warning(f'Raw data config does not match current config. Specifically key {key} does not match.')\n                    _flag = True\n        if _flag:\n            logging.info('Overwriting raw data config with raw data config loaded from {}'.format(path_raw_data_config))\n            cfg.pModel.RawData = raw_data_config\n        else: \n            logging.info('Current config matches loaded raw data config. No overwriting of raw data config.')\n    else:\n        raw_data_config = None\n        logging.info('No raw data given as data from external source. Skipping loading raw data config.')\n\n    # load raw data\n    raw_data = h5py.File(path_raw_data, 'r')\n    logging.info('Loaded raw data from {}'.format(path_raw_data))\n\n    return raw_data, raw_data_config\n</code></pre>"},{"location":"bnode_core/data_generation/data_preperation/#bnode_core.data_generation.data_preperation.get_position_in_raw_data_file","title":"<code>get_position_in_raw_data_file(variable: str, temp_raw_data: h5py.File) -&gt; list</code>","text":"<p>Find the dataset and index position of a variable in the raw data HDF5 file.</p> <p>Searches all '*_names' datasets in the HDF5 file to locate the specified variable.  Returns the dataset name (without '_names' suffix) and the index within that dataset.</p> <p>Parameters:</p> Name Type Description Default <code>variable</code> <code>str</code> <p>Name of the variable to find (e.g., 'temperature', 'control_1').</p> required <code>temp_raw_data</code> <code>File</code> <p>Open h5py.File handle to the raw data file.</p> required <p>Returns:</p> Type Description <code>list</code> <p>List [dataset_name, idx] where:</p> <ul> <li>dataset_name: Name of the dataset (e.g., 'states', 'controls', 'outputs')</li> <li>idx: Integer index of the variable within that dataset's second dimension</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If variable is not found in any dataset, or if found in multiple datasets.</p> Source code in <code>src/bnode_core/data_generation/data_preperation.py</code> <pre><code>def get_position_in_raw_data_file(variable: str, temp_raw_data: h5py.File) -&gt; list:\n    \"\"\"Find the dataset and index position of a variable in the raw data HDF5 file.\n\n    Searches all '*_names' datasets in the HDF5 file to locate the specified variable. \n    Returns the dataset name (without '_names' suffix) and the index within that dataset.\n\n    Args:\n        variable: Name of the variable to find (e.g., 'temperature', 'control_1').\n        temp_raw_data: Open h5py.File handle to the raw data file.\n\n    Returns:\n        List [dataset_name, idx] where:\n\n            - dataset_name: Name of the dataset (e.g., 'states', 'controls', 'outputs')\n            - idx: Integer index of the variable within that dataset's second dimension\n\n    Raises:\n        ValueError: If variable is not found in any dataset, or if found in multiple datasets.\n\n    \"\"\"\n    # returns dataset name and position in dataset\n    search_datasets = [key for key in temp_raw_data.keys() if key.endswith('names')]\n    temp = []\n    for dataset in search_datasets:\n        _temp = np.array(temp_raw_data[dataset], dtype=str)\n        if variable in _temp:\n            temp.append([dataset, np.where(_temp == variable)[0][0]])\n    if len(temp) == 0:\n        raise ValueError(f'Variable {variable} not found in raw data file.')\n    elif len(temp) &gt; 1:\n        raise ValueError(f'Variable {variable} found in multiple datasets in raw data file.')\n    else:\n        temp[0][0] = temp[0][0].replace('_names', '')\n        return temp[0]\n</code></pre>"},{"location":"bnode_core/data_generation/data_preperation/#bnode_core.data_generation.data_preperation.transform_raw_data","title":"<code>transform_raw_data(cfg: data_gen_config, temp_raw_data: h5py.File, raw_data_config: RawDataClass) -&gt; None</code>","text":"<p>Apply configured transformations to variables in the raw data file.</p> <p>Performs in-place transformations on raw data variables according to the transforms  specified in cfg.pModel.dataset_prep.transforms. Each variable can have one transform  applied. The function modifies the data directly in the temp_raw_data HDF5 file.</p> Supported transforms <ul> <li>'temperature_k_to_degC': Convert from Kelvin to Celsius (subtract 273.15)</li> <li>'power_w_to_kw': Convert from Watts to kilowatts (divide by 1000)</li> <li>'differentiate': Compute numerical derivative using Akima interpolation.    Also updates states_der if present. Logs interpolation error statistics.</li> <li>'evaluate_python_': Evaluate arbitrary Python expression where '#' is    replaced with the data array reference (e.g., 'evaluate_python_#/100' divides by 100) <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>data_gen_config</code> <p>Data generation configuration. cfg.pModel.dataset_prep.transforms is a dict mapping variable names to transform names.</p> required <code>temp_raw_data</code> <code>File</code> <p>Open h5py.File handle to the temporary raw data file (modified in-place).</p> required <code>raw_data_config</code> <code>RawDataClass</code> <p>Raw data configuration (used for context, not directly modified).</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If an unsupported transform name is specified.</p> Notes <ul> <li>For 'differentiate': Uses scipy.interpolate.Akima1DInterpolator with 'makima' method.   Computes 0th, 1st, and optionally 2nd derivatives. Logs mean and max interpolation    errors normalized by standard deviation.</li> <li>For 'evaluate_python_': The command is split on '#' and reconstructed with    <code>temp_raw_data[dataset_name][:, idx]</code> as the data reference. Use caution with    arbitrary code execution.</li> <li>Transform operations are logged for each variable.</li> </ul> Source code in <code>src/bnode_core/data_generation/data_preperation.py</code> <pre><code>def transform_raw_data(cfg: data_gen_config, temp_raw_data: h5py.File, raw_data_config: RawDataClass) -&gt; None:\n    \"\"\"Apply configured transformations to variables in the raw data file.\n\n    Performs in-place transformations on raw data variables according to the transforms \n    specified in cfg.pModel.dataset_prep.transforms. Each variable can have one transform \n    applied. The function modifies the data directly in the temp_raw_data HDF5 file.\n\n    Supported transforms:\n        - 'temperature_k_to_degC': Convert from Kelvin to Celsius (subtract 273.15)\n        - 'power_w_to_kw': Convert from Watts to kilowatts (divide by 1000)\n        - 'differentiate': Compute numerical derivative using Akima interpolation. \n          Also updates states_der if present. Logs interpolation error statistics.\n        - 'evaluate_python_&lt;command&gt;': Evaluate arbitrary Python expression where '#' is \n          replaced with the data array reference (e.g., 'evaluate_python_#/100' divides by 100)\n\n    Args:\n        cfg: Data generation configuration.\n            cfg.pModel.dataset_prep.transforms is a dict mapping variable names to transform names.\n        temp_raw_data: Open h5py.File handle to the temporary raw data file (modified in-place).\n        raw_data_config: Raw data configuration (used for context, not directly modified).\n\n    Raises:\n        NotImplementedError: If an unsupported transform name is specified.\n\n    Notes:\n        - For 'differentiate': Uses scipy.interpolate.Akima1DInterpolator with 'makima' method.\n          Computes 0th, 1st, and optionally 2nd derivatives. Logs mean and max interpolation \n          errors normalized by standard deviation.\n        - For 'evaluate_python_': The command is split on '#' and reconstructed with \n          `temp_raw_data[dataset_name][:, idx]` as the data reference. Use caution with \n          arbitrary code execution.\n        - Transform operations are logged for each variable.\n    \"\"\"\n\n    for variable in cfg.pModel.dataset_prep.transforms.keys():\n        dataset_name, idx = get_position_in_raw_data_file(variable, temp_raw_data)\n\n        if cfg.pModel.dataset_prep.transforms[variable] == 'temperature_k_to_degC':\n            temp_raw_data[dataset_name][:,idx] = temp_raw_data[dataset_name][:,idx] - 273.15\n\n        elif cfg.pModel.dataset_prep.transforms[variable] == 'power_w_to_kw':\n            temp_raw_data[dataset_name][:,idx] = temp_raw_data[dataset_name][:,idx] / 1000\n\n        elif cfg.pModel.dataset_prep.transforms[variable] == 'differentiate':\n            # derivative present in raw data\n            _states_der_in_dataset = False\n            if 'states_der' in temp_raw_data.keys():\n                state_der_names = np.array(temp_raw_data['states_der_names'], dtype=str).tolist()\n                if 'der({})'.format(variable) in state_der_names:\n                    _states_der_in_dataset = True\n            # generate interpolating lagrange polynomial\n            x = temp_raw_data['time'][:]\n            y = temp_raw_data[dataset_name][:,idx]\n            y_std = np.std(y)\n            y_der_std = np.std(temp_raw_data['states_der'][:, idx]) if _states_der_in_dataset else None\n            # allocate dicts for statistics\n            error_mae = {'0th': [], '1st': []}\n            error_max = {'0th': [], '1st': []}\n            logging.info(f'Calculating derivative for variable {variable} in dataset {dataset_name}')\n            # loop over all samples, calculate derivative and replace in dataset\n            for i in tqdm.tqdm(range(y.shape[0]), desc=f'Calculating derivative for {variable}'):\n                interpolator = scipy.interpolate.Akima1DInterpolator(x, y[i, :], method='makima')\n                der = [interpolator(x, 0), interpolator(x, 1)]\n                if _states_der_in_dataset:\n                    der.append(interpolator(x, 2))\n                # calculate error statistics\n                error_mae['0th'].append(np.mean(np.abs(y[i] - der[0])/ y_std))\n                error_max['0th'].append(np.max(np.abs(y[i] - der[0]))/ y_std)\n                if _states_der_in_dataset:\n                    error_mae['1st'].append(np.mean(np.abs(temp_raw_data['states_der'][i, idx] - der[1])/ y_der_std))\n                    error_max['1st'].append(np.max(np.abs(temp_raw_data['states_der'][i, idx] - der[1])/ y_der_std))\n                # replace in dataset\n                temp_raw_data[dataset_name][i, idx, :] = der[1]\n                if _states_der_in_dataset:\n                    temp_raw_data['states_der'][i, idx, :] = der[2]\n            # print error statistics\n            logging.info(f'  Error statistics for differentiating variable {variable} in dataset {dataset_name}, normalized by std:')\n            logging.info(f'    Mean Absolute Error (0th): {np.mean(error_mae[\"0th\"])}, std: {np.std(error_mae[\"0th\"])}')\n            logging.info(f'    Max Error (0th): {np.max(error_max[\"0th\"])}')\n            if _states_der_in_dataset:\n                logging.info(f'    Mean Absolute Error (1st): {np.mean(error_mae[\"1st\"])}, std: {np.std(error_mae[\"1st\"])}')\n                logging.info(f'    Max Error (1st): {np.max(error_max[\"1st\"])}')\n        elif cfg.pModel.dataset_prep.transforms[variable].startswith('evaluate_python_'):\n            \"\"\"\n            Transform is a python command that evaluates the variable temp_raw_data[dataset_name][:,idx]\n            It should be in the format 'evaluate_python_&lt;command&gt;'\n            where &lt;command&gt; is a python command that takes the variable as input.\n            denote with # a placeholder for temp_raw_data[dataset_name][:,idx].\n            \"\"\"\n            command = cfg.pModel.dataset_prep.transforms[variable].replace('evaluate_python_', '')\n            commands = command.split('#')\n            command = commands[0] + 'temp_raw_data[dataset_name][:,idx]' + commands[1]\n            logging.info(f'Transforming variable {variable} in dataset {dataset_name} with python command: {command}')\n            temp_raw_data[dataset_name][:,idx] = eval(command)\n        else:\n            raise NotImplementedError(f'Transform {cfg.pModel.dataset_prep.transforms[variable]} not implemented.')\n        logging.info(f'Transformed variable {variable} in dataset {dataset_name} with transform {cfg.pModel.dataset_prep.transforms[variable]}')\n    pass\n</code></pre>"},{"location":"bnode_core/data_generation/data_preperation/#bnode_core.data_generation.data_preperation.replace_hdf5_dataset","title":"<code>replace_hdf5_dataset(dataset_name: str, raw_data: h5py.File, data: np.ndarray, remove: bool = False)</code>","text":"<p>Replace or remove a dataset in an HDF5 file.</p> <p>Updates an existing dataset in the HDF5 file with new data. If the new data has a  different shape than the existing dataset, or if remove=True, deletes the old dataset.  If remove=True, no new dataset is created.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>Name of the dataset to replace (e.g., 'states', 'controls').</p> required <code>raw_data</code> <code>File</code> <p>Open h5py.File handle to the HDF5 file.</p> required <code>data</code> <code>ndarray</code> <p>New data array to write (ignored if remove=True).</p> required <code>remove</code> <code>bool</code> <p>If True, delete the dataset without creating a replacement.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dataset_name does not exist in the HDF5 file.</p> Notes <ul> <li>If shapes match and remove=False, overwrites data in-place using [...] assignment.</li> <li>If shapes differ, deletes old dataset and creates new one with the provided data.</li> <li>If remove=True, only deletes the dataset (used for cleanup).</li> </ul> Source code in <code>src/bnode_core/data_generation/data_preperation.py</code> <pre><code>def replace_hdf5_dataset(dataset_name: str, raw_data: h5py.File, data: np.ndarray, remove: bool = False):\n    \"\"\"Replace or remove a dataset in an HDF5 file.\n\n    Updates an existing dataset in the HDF5 file with new data. If the new data has a \n    different shape than the existing dataset, or if remove=True, deletes the old dataset. \n    If remove=True, no new dataset is created.\n\n    Args:\n        dataset_name: Name of the dataset to replace (e.g., 'states', 'controls').\n        raw_data: Open h5py.File handle to the HDF5 file.\n        data: New data array to write (ignored if remove=True).\n        remove: If True, delete the dataset without creating a replacement.\n\n    Raises:\n        ValueError: If dataset_name does not exist in the HDF5 file.\n\n    Notes:\n        - If shapes match and remove=False, overwrites data in-place using [...] assignment.\n        - If shapes differ, deletes old dataset and creates new one with the provided data.\n        - If remove=True, only deletes the dataset (used for cleanup).\n    \"\"\"\n    if dataset_name not in raw_data.keys():\n        raise ValueError(f'Dataset {dataset_name} not found in raw data file.')\n    if remove:\n        del raw_data[dataset_name]\n    else:\n        if data.shape != raw_data[dataset_name].shape:\n            del raw_data[dataset_name]\n            raw_data.create_dataset(dataset_name, data=data)           \n        else:\n            raw_data[dataset_name][...] = data\n</code></pre>"},{"location":"bnode_core/data_generation/data_preperation/#bnode_core.data_generation.data_preperation.run_data_preperation","title":"<code>run_data_preperation(cfg: data_gen_config)</code>","text":"<p>Main orchestration function for dataset preparation pipeline.</p> <p>Complete dataset preparation workflow:</p> <ol> <li>Load and validate raw data HDF5 file and config</li> <li>Copy raw data to temporary file for safe manipulation</li> <li>Remove failed simulation runs</li> <li>Filter trajectories based on configured limits and expressions</li> <li>Apply transformations (unit conversions, derivatives, etc.)</li> <li>Select only requested variables (states, controls, outputs, parameters)</li> <li>Select requested time window</li> <li>Create common validation and test sets</li> <li>Generate multiple dataset files with different sample counts</li> <li>Save dataset-specific configs and clean up temporary files</li> </ol> <p>The function creates one or more dataset HDF5 files (based on cfg.pModel.dataset_prep.n_samples  list) with train/validation/test splits plus common_validation and common_test sets that are  consistent across all dataset sizes.</p> <p>This is the Hydra-decorated entry point called by main().</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>data_gen_config</code> <p>Data generation configuration (automatically populated by Hydra from YAML + CLI args). Key settings include:</p> <ul> <li>pModel.RawData: raw data generation parameters</li> <li>pModel.dataset_prep.transforms: dict of variable -&gt; transform mappings</li> <li>pModel.dataset_prep.filter_trajectories_limits: dict of variable -&gt; [min, max] bounds</li> <li>pModel.dataset_prep.filter_trajectories_expression: dict of variable -&gt; expression list</li> <li>pModel.dataset_prep.states/controls/outputs/parameters: variable lists to keep</li> <li>pModel.dataset_prep.parameters_remove: whether to exclude parameters from datasets</li> <li>pModel.dataset_prep.start_time/end_time: time window for trajectory slicing</li> <li>pModel.dataset_prep.validation_fraction/test_fraction: split proportions</li> <li>pModel.dataset_prep.n_samples: list of dataset sizes to generate</li> </ul> required Workflow details <ul> <li>Failed runs (from 'failed_idx' or 'logs/completed') are removed.</li> <li>Filtering can exclude trajectories based on min/max limits or Python expressions.</li> <li>Transforms are applied in the order specified in the config.</li> <li>Variables not in the selection lists are removed to reduce file size.</li> <li>Time window selection adjusts sequence_length accordingly.</li> <li>Common test/validation sets are created from the full raw data and stored in each    dataset file. Smaller datasets use proportionally fewer train samples but keep the    same validation/test examples.</li> <li>Each dataset file gets a companion YAML config with the pModel settings and    dataset-specific n_samples.</li> <li>Temporary HDF5 file is deleted after all datasets are created.</li> </ul> Notes <ul> <li>The temporary file (temp_raw_data.hdf5) is created in the current directory.</li> <li>Dataset file paths determined by filepath_dataset(cfg, n_samples).</li> <li>Config paths determined by filepath_dataset_config(cfg, n_samples).</li> <li>Creation date is recorded in each dataset file's 'creation_date' attribute.</li> <li>If n_samples in the list exceeds raw data sample count, it's clamped and logged.</li> </ul> Source code in <code>src/bnode_core/data_generation/data_preperation.py</code> <pre><code>def run_data_preperation(cfg: data_gen_config):\n    \"\"\"Main orchestration function for dataset preparation pipeline.\n\n    Complete dataset preparation workflow:\n\n    1. Load and validate raw data HDF5 file and config\n    2. Copy raw data to temporary file for safe manipulation\n    3. Remove failed simulation runs\n    4. Filter trajectories based on configured limits and expressions\n    5. Apply transformations (unit conversions, derivatives, etc.)\n    6. Select only requested variables (states, controls, outputs, parameters)\n    7. Select requested time window\n    8. Create common validation and test sets\n    9. Generate multiple dataset files with different sample counts\n    10. Save dataset-specific configs and clean up temporary files\n\n    The function creates one or more dataset HDF5 files (based on cfg.pModel.dataset_prep.n_samples \n    list) with train/validation/test splits plus common_validation and common_test sets that are \n    consistent across all dataset sizes.\n\n    This is the Hydra-decorated entry point called by main().\n\n    Args:\n        cfg: Data generation configuration (automatically populated by Hydra from YAML + CLI args).\n            Key settings include:\n\n            - pModel.RawData: raw data generation parameters\n            - pModel.dataset_prep.transforms: dict of variable -&gt; transform mappings\n            - pModel.dataset_prep.filter_trajectories_limits: dict of variable -&gt; [min, max] bounds\n            - pModel.dataset_prep.filter_trajectories_expression: dict of variable -&gt; expression list\n            - pModel.dataset_prep.states/controls/outputs/parameters: variable lists to keep\n            - pModel.dataset_prep.parameters_remove: whether to exclude parameters from datasets\n            - pModel.dataset_prep.start_time/end_time: time window for trajectory slicing\n            - pModel.dataset_prep.validation_fraction/test_fraction: split proportions\n            - pModel.dataset_prep.n_samples: list of dataset sizes to generate\n\n    Workflow details:\n        - Failed runs (from 'failed_idx' or 'logs/completed') are removed.\n        - Filtering can exclude trajectories based on min/max limits or Python expressions.\n        - Transforms are applied in the order specified in the config.\n        - Variables not in the selection lists are removed to reduce file size.\n        - Time window selection adjusts sequence_length accordingly.\n        - Common test/validation sets are created from the full raw data and stored in each \n          dataset file. Smaller datasets use proportionally fewer train samples but keep the \n          same validation/test examples.\n        - Each dataset file gets a companion YAML config with the pModel settings and \n          dataset-specific n_samples.\n        - Temporary HDF5 file is deleted after all datasets are created.\n\n    Notes:\n        - The temporary file (temp_raw_data.hdf5) is created in the current directory.\n        - Dataset file paths determined by filepath_dataset(cfg, n_samples).\n        - Config paths determined by filepath_dataset_config(cfg, n_samples).\n        - Creation date is recorded in each dataset file's 'creation_date' attribute.\n        - If n_samples in the list exceeds raw data sample count, it's clamped and logged.\n    \"\"\"\n    cfg = convert_cfg_to_dataclass(cfg)\n\n    # load and validate raw data, copy data to temp file\n    raw_data, raw_data_cfg = load_and_validate_raw_data(cfg)\n    temp_raw_data_path = Path('temp_raw_data.hdf5')\n    temp_raw_data = h5py.File(temp_raw_data_path, 'w')\n    for key in raw_data.keys():\n        raw_data.copy(key, temp_raw_data)\n    logging.info('Copied raw data to temporary file {}'.format(temp_raw_data_path))\n\n    # remove failed runs from raw data\n    # get idx of logs/completed that are 0\n    remove_runs = np.where(raw_data['logs/completed'][:] == 0)[0]\n    logging.info('Removing failed runs from raw data: {}'.format(remove_runs))\n\n    _remove_runs = []\n    if len(cfg.pModel.dataset_prep.filter_trajectories_limits) &gt; 0:\n        logging.info('Filtering trajectories in raw data according to filter_trajectories config.')\n        for key, value in cfg.pModel.dataset_prep.filter_trajectories_limits.items():\n            dataset_name, idx = get_position_in_raw_data_file(key, temp_raw_data)\n            if type(value) is list: # apply min/max filter\n                if len(value) != 2:\n                    raise ValueError(f'Filter for {key} must be a list of length 2, got {value}.')\n                logging.info(f'Filtering {dataset_name} for {key} with min {value[0]} and max {value[1]}.')\n                idx_lower = (temp_raw_data[dataset_name][:, idx] &lt; value[0])\n                idx_upper = (temp_raw_data[dataset_name][:, idx] &gt; value[1])\n                idx = np.logical_or(idx_lower, idx_upper)\n                if np.sum(idx) &gt; 0:\n                    logging.info(f'Found {np.sum(idx)} runs that do not match the filter for {key}. Removing them.')\n                    _remove_runs.append(np.nonzero(idx)[0])\n    if len(cfg.pModel.dataset_prep.filter_trajectories_expression) &gt; 0:\n        raise NotImplementedError('Filtering by expression is implementd, but not yet tested. Please use with caution.')\n        logging.info('Filtering trajectories in raw data according to filter_trajectories_by_expression config.')\n        for key, args in cfg.pModel.dataset_prep.filter_trajectories_by_expression.items():\n            dataset_name, idx = get_position_in_raw_data_file(key, temp_raw_data)\n            if type(args) is not list:\n                raise ValueError(f'Filter for {key} must be a list of expressions, got {args}.')\n            logging.info(f'Filtering {dataset_name} for {key} with expressions {args}.')\n            _conditions = []\n            for arg in args:\n                # evaluate the expression\n                arg = arg.replace('#', 'temp_raw_data[dataset_name][:, idx]')\n                _conditions.append(eval(arg))\n            idx = np.logical_or.reduce(_conditions)\n            _remove_runs.append(np.nonzero(idx)[0])\n\n    # remove runs from raw data\n    if len(_remove_runs) &gt; 0 or len(remove_runs) &gt; 0:\n        _remove_runs = np.concatenate(_remove_runs) if len(_remove_runs) &gt; 0 else np.array([], dtype=int)\n        remove_runs = np.unique(np.concatenate([remove_runs, _remove_runs]))\n        logging.info('Found {} runs to remove from raw data: {}'.format(len(remove_runs), remove_runs))\n        remove_runs = np.sort(remove_runs)\n        for key in ['states', 'states_der', 'controls', 'outputs', 'parameters']:\n            if key in temp_raw_data.keys():\n                _temp= np.delete(temp_raw_data[key][:], remove_runs, axis=0)\n                replace_hdf5_dataset(key, temp_raw_data, data = _temp)\n                logging.info('\\tRemoved runs from {}'.format(key))\n            else:\n                logging.info('\\tNo {} in raw data. Skipping removal of failed runs.'.format(key))\n        raw_data_cfg.n_samples = raw_data_cfg.n_samples - len(remove_runs)\n        logging.info('Updated n_samples in raw_data_config to {}'.format(raw_data_cfg.n_samples))\n    else:\n        logging.info('No runs to remove from raw data.')\n\n    # perform transforms on raw data\n    if not cfg.pModel.RawData.raw_data_from_external_source:\n        transform_raw_data(cfg, temp_raw_data, raw_data_cfg)\n\n    # only select variables of interest / states, controls, outputs, parameters\n\n    # helper functions\n    def get_idx(names_list: h5py.Dataset, chosen_variables: list):\n        if chosen_variables == ['all'] or chosen_variables == ['der(all)'] or chosen_variables == None:\n            return np.arange(len(names_list))\n        else:\n            names_list = np.array(names_list, dtype=str).tolist()\n            return [names_list.index(variable) for variable in chosen_variables]\n\n    def select_variables_of_interest(type: str, variables: list, remove: bool = False):\n        # type is states, states_der, controls, outputs, parameters\n        if variables == None:\n            remove = True\n        if type in temp_raw_data.keys():\n            _type_with_names = f'{type}_names' # type_names_str = 'states_names' or 'states_der_names' or 'controls_names' or 'outputs_names' or 'parameters_names'\n            idx = get_idx(temp_raw_data[_type_with_names], variables) # get idx\n            replace_hdf5_dataset(type, temp_raw_data, data = temp_raw_data[type][:,idx], remove=remove) # replace dataset\n            replace_hdf5_dataset(_type_with_names, temp_raw_data, data = temp_raw_data[_type_with_names][idx], remove=remove) # replace dataset\n            if remove:\n                logging.info(f'Removed dataset {type} from raw data.')\n            else:\n                logging.info(f'Selected {type} {variables} from raw data.') \n        else:\n            logging.info(f'No {type} in raw data. Skipping selection of {type}.')\n\n    logging.info('... Selecting variables of interest in raw data.')\n    select_variables_of_interest('states', cfg.pModel.dataset_prep.states)\n    select_variables_of_interest('states_der', ['der({})'.format(state) for state in cfg.pModel.dataset_prep.states])\n    select_variables_of_interest('controls', cfg.pModel.dataset_prep.controls)\n    select_variables_of_interest('outputs', cfg.pModel.dataset_prep.outputs)\n    select_variables_of_interest('parameters', cfg.pModel.dataset_prep.parameters, remove = cfg.pModel.dataset_prep.parameters_remove)\n\n    # only select certain timeframe\n    def idx_timeframe(time: np.ndarray, start_time: float, end_time: float):\n        idx = np.where((time &gt;= start_time) &amp; (time &lt;= end_time))[0]\n        logging.info(f'... Selecting timeframe from {start_time} to {end_time} in raw data.')\n        return idx\n\n    def replace_timeseries_if_exist(idx, dataset_name: str, raw_data: h5py.File):\n        if dataset_name in raw_data.keys():\n            replace_hdf5_dataset(dataset_name, raw_data, data = raw_data[dataset_name][:,:,idx])\n            logging.info(f'Selected timeframe from {dataset_name} in raw data.')\n        else:\n            logging.info(f'No {dataset_name} in raw data. Skipping selection of {dataset_name}.')\n\n    idx = idx_timeframe(temp_raw_data['time'][:], cfg.pModel.dataset_prep.start_time, cfg.pModel.dataset_prep.end_time)\n    cfg.pModel.dataset_prep.sequence_length = len(idx)\n    replace_hdf5_dataset('time', temp_raw_data, data = temp_raw_data['time'][idx])\n    replace_timeseries_if_exist(idx, 'states', temp_raw_data)\n    replace_timeseries_if_exist(idx, 'states_der', temp_raw_data)\n    replace_timeseries_if_exist(idx, 'controls', temp_raw_data)\n    replace_timeseries_if_exist(idx, 'outputs', temp_raw_data)\n\n    #############################################################\n    # special routines, e.g. chunking together from 0 to N for each time-series\n\n    # could be added here\n\n    #############################################################\n\n    # save common test and validation sets to temporary raw data file\n    logging.info('opening common test and validation sets')\n    temp_raw_data.create_group('common_test')\n    temp_raw_data.create_group('common_validation')\n    raw_data_n_samples = raw_data_cfg.n_samples if not cfg.pModel.RawData.raw_data_from_external_source else temp_raw_data['states'].shape[0]\n\n    # determine idx in raw data set of test and validation sets\n    validation_idx_start_total = int(np.floor(raw_data_n_samples * (1 - cfg.pModel.dataset_prep.validation_fraction - cfg.pModel.dataset_prep.test_fraction)))\n    test_idx_start_total = int(np.floor(raw_data_n_samples * (1 - cfg.pModel.dataset_prep.test_fraction)))\n\n    # to accomendate cases where validation fraction is 0, just ensure to add one element to validation set\n    validation_idx_end_total = test_idx_start_total if test_idx_start_total &gt; validation_idx_start_total else validation_idx_start_total + 1\n\n    # save idx to cfg\n    cfg.pModel.dataset_prep.validation_idx_start = validation_idx_start_total\n    cfg.pModel.dataset_prep.test_idx_start = test_idx_start_total\n    logging.info('set validation_idx_start to {}, test_idx_start to {} in cfg.'.format(validation_idx_start_total, test_idx_start_total))\n\n    # save common validation and test sets\n    for key in ['states', 'states_der', 'controls', 'outputs', 'parameters']:\n        if key in temp_raw_data.keys():\n            temp_raw_data.create_dataset('common_validation/' + key, data=temp_raw_data[key][validation_idx_start_total:validation_idx_end_total])\n            temp_raw_data.create_dataset('common_test/' + key, data=temp_raw_data[key][test_idx_start_total:])\n            logging.info('Saved common test and validation sets for {} to temporary raw data file.'.format(key))\n        else:\n            logging.info('No {} in raw data. Skipping saving common test and validation sets for {}.'.format(key, key))\n\n    # add generation date to datasets\n    creation_date = datetime.now()\n    temp_raw_data.attrs['creation_date'] = str(creation_date)\n\n    _reached_max_samples = False\n    # sample dataset sizes and save datasets\n    for n_samples_dataset in cfg.pModel.dataset_prep.n_samples:\n        if _reached_max_samples:\n            logging.warning('Reached maximum number of samples in raw data. Skipping further dataset creation.')\n            break\n        if n_samples_dataset &gt; raw_data_n_samples:\n            logging.warning('n_samples_dataset must be smaller than n_samples in raw data. Setting n_samples_dataset={} to n_samples={}'.format(n_samples_dataset, raw_data_n_samples))\n            n_samples_dataset = raw_data_n_samples\n            _reached_max_samples = True\n        path_dataset = filepath_dataset(cfg, n_samples_dataset)\n        log_overwriting_file(path_dataset)\n        dataset_file = h5py.File(path_dataset, 'w')\n        dataset_file.create_dataset('time', data=temp_raw_data['time'])\n        for key in ['states', 'states_der', 'controls', 'outputs', 'parameters']:\n            if key in temp_raw_data.keys():\n                if n_samples_dataset &gt; raw_data_n_samples:\n                    raise ValueError('n_samples_dataset must be smaller than n_samples in raw data. Reaching this line should not happen.')\n                dataset_file.create_dataset(key + '_names', data=temp_raw_data[key + '_names'])\n                # get idx\n                train_idx_stop = int((n_samples_dataset/raw_data_n_samples) * validation_idx_start_total)\n                if not train_idx_stop &gt; 0:\n                    train_idx_stop = 1\n                common_validation_idx_stop = int((n_samples_dataset/raw_data_n_samples) * len(temp_raw_data['common_validation/' + key]))\n                common_test_idx_stop = int((n_samples_dataset/raw_data_n_samples) * len(temp_raw_data['common_test/' + key]))\n\n                # train, validate, test\n                dataset_file.create_dataset('train/' + key, data = temp_raw_data[key][:train_idx_stop])\n                dataset_file.create_dataset('validation/' + key, data=temp_raw_data['common_validation/' + key][:common_validation_idx_stop])\n                dataset_file.create_dataset('test/' + key, data=temp_raw_data['common_test/' + key][:common_test_idx_stop])\n\n                logging.info('Saved {} data with {} samples to {}'.format(key, n_samples_dataset, path_dataset))\n                # add common datasets\n                dataset_file.create_dataset('common_validation/' + key, data=temp_raw_data['common_validation/' + key])\n                dataset_file.create_dataset('common_test/' + key, data=temp_raw_data['common_test/' + key])\n                logging.info('Added common test and validation sets for {} to {} dataset.'.format(key, path_dataset))\n            else:\n                logging.info('No {} in raw data. Skipping saving {}'.format(key, key))\n        # save config: create new config object, set raw data config, set n_samples to n_samples_dataset, add preparation info\n        _conf = OmegaConf.create(cfg)\n        if not cfg.pModel.RawData.raw_data_from_external_source:\n            _conf.pModel.RawData = raw_data_cfg\n        _conf.pModel.dataset_prep.n_samples = [n_samples_dataset]\n        # add preparation info\n        _conf.pModel.dataset_prep = cfg.pModel.dataset_prep\n        path_dataset_config = filepath_dataset_config(cfg, n_samples_dataset)\n        log_overwriting_file(path_dataset_config)\n        OmegaConf.save(_conf.pModel, path_dataset_config)\n        logging.info('Saved pModel config to {}'.format(path_dataset_config))\n        # close dataset\n        dataset_file.attrs['creation_date'] = str(creation_date)\n        dataset_file.close()\n        logging.info('Closed dataset {}'.format(path_dataset))\n\n    # delete temporary file\n    temp_raw_data.close()\n    os.remove(temp_raw_data_path)\n    pass\n</code></pre>"},{"location":"bnode_core/data_generation/data_preperation/#bnode_core.data_generation.data_preperation.main","title":"<code>main()</code>","text":"<p>CLI entry point for dataset preparation.</p> <p>Sets up Hydra configuration management and launches run_data_preperation().  Displays help message if --help or -h is provided.</p> <p>Hydra automatically:</p> <ul> <li>Loads the data_generation.yaml config from the auto-detected config directory</li> <li>Parses command-line overrides</li> <li>Creates a working directory for outputs</li> <li>Injects the composed config into run_data_preperation()</li> </ul> Usage <p><code>python data_preperation.py [overrides]</code></p> <p><code>python data_preperation.py --help</code></p> <p><code>python data_preperation.py --hydra-help</code></p> <p>Examples:</p> <pre><code>python data_preperation.py pModel.dataset_prep.n_samples=[128,512,2048]\npython data_preperation.py pModel=SHF pModel.dataset_prep.validation_fraction=0.15\npython data_preperation.py pModel.dataset_prep.transforms.temperature=temperature_k_to_degC\n</code></pre> Notes <ul> <li>The standard config file used is 'data_generation.yaml' (same as raw_data_generation.py).</li> <li>Config directory is auto-detected using config_dir_auto_recognize().</li> <li>You can override config path and direcotory using Hydra CLI options \"-cp\"/\"--config-path\" and/or \"-cd\"/\"--config-dir\" </li> <li>Hydra overrides can modify any config field from the command line.</li> </ul> Source code in <code>src/bnode_core/data_generation/data_preperation.py</code> <pre><code>def main():\n    \"\"\"CLI entry point for dataset preparation.\n\n    Sets up Hydra configuration management and launches run_data_preperation(). \n    Displays help message if --help or -h is provided.\n\n    Hydra automatically:\n\n    - Loads the data_generation.yaml config from the auto-detected config directory\n    - Parses command-line overrides\n    - Creates a working directory for outputs\n    - Injects the composed config into run_data_preperation()\n\n    Usage:\n        ```python data_preperation.py [overrides]```\n\n        ```python data_preperation.py --help```\n\n        ```python data_preperation.py --hydra-help```\n\n    Examples:\n\n        python data_preperation.py pModel.dataset_prep.n_samples=[128,512,2048]\n        python data_preperation.py pModel=SHF pModel.dataset_prep.validation_fraction=0.15\n        python data_preperation.py pModel.dataset_prep.transforms.temperature=temperature_k_to_degC\n\n    Notes:\n        - The standard config file used is 'data_generation.yaml' (same as raw_data_generation.py).\n        - Config directory is auto-detected using config_dir_auto_recognize().\n        - You can override config path and direcotory using Hydra CLI options \"-cp\"/\"--config-path\" and/or \"-cd\"/\"--config-dir\" \n        - Hydra overrides can modify any config field from the command line.\n    \"\"\"\n    if '--help' in sys.argv or '-h' in sys.argv:\n        print('Usage: data_preperation [--cfg_path &lt;path_to_config_file&gt;]')\n        print('If --cfg_path is not provided, the default config file \"data_generation.yaml in the \"conf\" directory is used.')\n        print('The remainder of the command line arguments are passed to and provided by Hydra.')\n    cs = get_config_store()\n    config_dir = config_dir_auto_recognize()\n    config_name = 'data_generation'\n    hydra.main(config_path=str(config_dir.absolute()), config_name=config_name, version_base=None)(run_data_preperation)()\n</code></pre>"},{"location":"bnode_core/data_generation/introduction/","title":"Introduction","text":""},{"location":"bnode_core/data_generation/introduction/#overview-raw-data-generation-and-dataset-preparation","title":"Overview: Raw Data Generation and Dataset Preparation","text":"<p>This project separates raw data generation (heavy numerical simulation) from dataset preparation (conversions, filtering, slicing and dataset packaging). The separation exists because raw-data generation is often computationally expensive (for example FMU simulations run in parallel). Doing the expensive simulations once and storing the raw outputs lets you repeat cheaper, deterministic preparation steps (unit conversions, derivatives, filtering, variable selection, time-window selection, and creation of train/validation/test splits) many times without re-running the simulators.</p> <p>A second benefit of the split is reproducible dataset sizing: you can generate a single large raw dataset and then prepare multiple dataset files with different sample counts (n_samples) while keeping the same selection logic for train/validation/test. To support this, <code>data_preperation.py</code> creates per-dataset train/validation/test subsets while also storing full \"common\" validation/test sets (prefixed with <code>common_</code>) so that smaller dataset sizes preserve the same validation/test elements (just fewer train samples).</p> <p>Below is a concise reference of what is produced and saved by the two scripts.</p>"},{"location":"bnode_core/data_generation/introduction/#raw-data-file-created-by-raw_data_generationpy","title":"Raw Data File (created by <code>raw_data_generation.py</code>)","text":"<p>When <code>run_data_generation</code> finishes it writes a single HDF5 raw data file (path from <code>filepath_raw_data(cfg)</code> of the <code>bnode_core.filepaths</code> module) and a companion YAML config (path from <code>filepath_raw_data_config(cfg)</code>). The raw HDF5 stores simulation inputs, outputs and run-level metadata. The script also writes a YAML copy of the <code>pModel</code> raw-data config to the filesystem.</p>"},{"location":"bnode_core/data_generation/introduction/#fields-and-groups-written-to-the-raw-hdf5-file","title":"Fields and groups written to the raw HDF5 file","text":""},{"location":"bnode_core/data_generation/introduction/#attributes","title":"Attributes","text":"<ul> <li><code>creation_date</code> \u2014 string with the file creation timestamp (YYYY-MM-DD HH:MM:SS). </li> <li><code>config</code> \u2014 the raw-data <code>pModel.RawData</code> configuration serialized to YAML (OmegaConf.to_yaml(cfg.pModel.RawData)). This is a snapshot of the configuration used for generation. </li> </ul>"},{"location":"bnode_core/data_generation/introduction/#datasets-high-level","title":"Datasets (high-level)","text":"<ul> <li><code>time</code> \u2014 1D array containing the simulation time vector used for all time-series (from simulationStartTime to simulationEndTime with the configured timestep).</li> <li><code>parameters</code> (optional) \u2014 shape (n_samples, n_parameters). Sampled parameter values when <code>parameters_include</code> is enabled. Else: the default model parameter values used for all samples.</li> <li><code>parameters_names</code> \u2014 array of parameter names (stored as bytes/strings).</li> <li><code>controls</code> (optional) \u2014 shape (n_samples, n_controls, sequence_length) if controls are stored directly. If the sampled controls are only used for sampling (and later extracted from the model) different storage rules apply (see config options).</li> <li><code>controls_names</code> \u2014 array of control names (bytes/strings).</li> <li><code>states</code> \u2014 shape (n_samples, n_states, len(time)). Time series of states for each sample. The initial state at time=0 corresponds to the sampled initial states (if any).</li> <li><code>states_names</code> \u2014 array of state names (bytes/strings).</li> <li><code>states_der</code> (optional) \u2014 shape (n_samples, n_states, len(time)) if <code>states_der_include</code> is True (time derivatives of states).</li> <li><code>states_der_names</code> \u2014 array of derivative names (e.g. <code>der(state_name)</code>) when derivatives are included.</li> <li><code>outputs</code> (optional) \u2014 shape (n_samples, n_outputs, len(time)) for model outputs if configured.</li> <li><code>outputs_names</code> \u2014 array of output names.</li> </ul>"},{"location":"bnode_core/data_generation/introduction/#logs-progress-information","title":"Logs / progress information","text":"<ul> <li><code>logs</code> group \u2014 created to track simulation progress and per-sample status:</li> <li><code>logs/completed</code> \u2014 boolean array (n_samples,) marking which sample runs completed successfully.</li> <li><code>logs/sim_failed</code> \u2014 boolean array (n_samples,) marking runs that failed.</li> <li><code>logs/timedout</code> \u2014 boolean array (n_samples,) marking runs that timed out.</li> <li><code>logs/processing_time</code> \u2014 per-sample processing times (float) recorded while generating data.</li> <li>This log is written incrementally during data generation to allow usage of the raw data file even if generation is interrupted.</li> <li><code>failed_idx</code> \u2014 integer array with indices of runs that did not complete (added at the end for backward compatibility). </li> </ul>"},{"location":"bnode_core/data_generation/introduction/#dataset-files-created-by-data_preperationpy","title":"Dataset Files (created by <code>data_preperation.py</code>)","text":"<p><code>data_preperation.py</code> reads a raw HDF5 file and produces one or more dataset HDF5 files sized to requested sample counts (specified by a list for the config entry <code>n_samples</code>). Each dataset file groups data into train/validation/test splits (and includes <code>common_</code> groups so small datasets keep the same validation/test elements as larger ones).</p>"},{"location":"bnode_core/data_generation/introduction/#what-data_preperation-writes-into-each-dataset-file","title":"What <code>data_preperation</code> writes into each dataset file","text":""},{"location":"bnode_core/data_generation/introduction/#file-level-attribute","title":"File-level attribute","text":"<ul> <li><code>creation_date</code> \u2014 timestamp when the dataset file was created. TODO: AI, is this correct? I cannot find it in the dataset files.</li> </ul>"},{"location":"bnode_core/data_generation/introduction/#datasets","title":"Datasets","text":"<ul> <li><code>time</code> \u2014 copied from the raw file (already trimmed to the requested timeframe).</li> <li>For each key in <code>['states', 'states_der', 'controls', 'outputs', 'parameters']</code> present in the raw data:</li> <li><code>&lt;key&gt;_names</code> \u2014 names corresponding to that dataset (e.g. <code>states_names</code>).</li> <li><code>train/&lt;key&gt;</code> \u2014 train subset samples (shape depends on <code>n_samples_dataset</code> and selection logic).</li> <li><code>validation/&lt;key&gt;</code> \u2014 validation subset samples.</li> <li><code>test/&lt;key&gt;</code> \u2014 test subset samples.</li> <li><code>common_validation/&lt;key&gt;</code> \u2014 full common validation set copied from the temporary raw file (used across dataset sizes to ensure the same validation elements).</li> <li><code>common_test/&lt;key&gt;</code> \u2014 full common test set copied from the temporary raw file.</li> </ul>"},{"location":"bnode_core/data_generation/introduction/#side-files","title":"Side files","text":"<ul> <li>A dataset-specific YAML config is saved for each dataset file path (via <code>filepath_dataset_config(cfg, n_samples_dataset)</code>), containing a copy of the <code>pModel</code> config with <code>n_samples</code> set to that dataset size and any dataset-prep metadata (the script saves <code>_conf.pModel</code> using OmegaConf.save).</li> </ul>"},{"location":"bnode_core/data_generation/introduction/#key-behaviors-of-data_preperation","title":"Key behaviors of data_preperation","text":""},{"location":"bnode_core/data_generation/introduction/#transforms-and-filtering","title":"Transforms and filtering","text":"<ul> <li><code>data_preperation</code> performs deterministic transforms and filtering:</li> <li>Unit conversions (e.g., <code>temperature_k_to_degC</code>, <code>power_w_to_kw</code>).</li> <li>Numerical differentiation using an Akima interpolator (if <code>differentiate</code> transform is requested) and error-statistics logging.</li> <li>Arbitrary Python-evaluated transforms (prefix <code>evaluate_python_</code>) \u2014 applied to a variable using an expression with <code>#</code> as a placeholder for the timeseries slice.</li> </ul>"},{"location":"bnode_core/data_generation/introduction/#filtering","title":"Filtering","text":"<ul> <li>Runs flagged as failed (or listed in <code>failed_idx</code>) can be removed.</li> <li>Additional filters based on per-variable min/max or expressions can exclude samples.</li> <li>Time-window trimming selects only the requested timeframe; the <code>sequence_length</code> is adjusted accordingly.</li> </ul>"},{"location":"bnode_core/data_generation/introduction/#variable-selection","title":"Variable selection","text":"<ul> <li>Only the requested variables are kept per dataset (states, controls, outputs, parameters); others are removed to reduce dataset size.</li> </ul>"},{"location":"bnode_core/data_generation/raw_data_generation/","title":"Raw Data Generation","text":""},{"location":"bnode_core/data_generation/raw_data_generation/#bnode_core.data_generation.raw_data_generation","title":"<code>bnode_core.data_generation.raw_data_generation</code>","text":"<p>Raw data generation module for parallel FMU simulation.</p>"},{"location":"bnode_core/data_generation/raw_data_generation/#bnode_core.data_generation.raw_data_generation--module-description","title":"Module Description","text":"<p>This module generates raw simulation data by running FMU (Functional Mock-up Unit) models  in parallel with sampled inputs (initial states, parameters, controls). It uses Dask for  distributed computing and writes results to HDF5 files with comprehensive logging.</p>"},{"location":"bnode_core/data_generation/raw_data_generation/#bnode_core.data_generation.raw_data_generation--command-line-usage","title":"Command-line Usage","text":"<pre><code>With uv (recommended):\n    uv run raw_data_generation [overrides]\n\nIn activated virtual environment:\n    raw_data_generation [overrides]\n\nDirect Python execution:\n    python -m bnode_core.data_generation.raw_data_generation [overrides]\n</code></pre>"},{"location":"bnode_core/data_generation/raw_data_generation/#bnode_core.data_generation.raw_data_generation--example-commands","title":"Example Commands","text":"<pre><code># Generate 1000 samples with default config\nuv run raw_data_generation pModel.RawData.n_samples=1000\n\n# Use specific pModel config and allow overwriting\nuv run raw_data_generation pModel=SHF overwrite=true\n\n# Change control sampling strategy to RROCS\nuv run raw_data_generation pModel.RawData.controls_sampling_strategy=RROCS\n\n# Adjust parallel workers and timeout\nuv run raw_data_generation multiprocessing_processes=8 pModel.RawData.Solver.timeout=120\n\n# Adjust config path and name\nuv run raw_data_generation --config-path=resources/config --config-name=data_generation_custom\n</code></pre>"},{"location":"bnode_core/data_generation/raw_data_generation/#bnode_core.data_generation.raw_data_generation--what-this-module-does","title":"What This Module Does","text":"<ol> <li>Loads and validates configuration (FMU path, sampling strategies, solver settings)</li> <li>Sets reproducibility seed (np.random.seed(42))</li> <li>Creates HDF5 raw data file with pre-allocated datasets</li> <li>Samples input values (initial states, parameters, controls) using configured strategies</li> <li>Writes sampled inputs and metadata to HDF5 file</li> <li>Sets up Dask distributed cluster for parallel FMU simulation</li> <li>Submits simulation tasks in batches with timeout monitoring</li> <li>Incrementally writes simulation results (states, outputs, derivatives) to HDF5</li> <li>Logs completion status, failures, timeouts, and processing times per sample</li> <li>Saves configuration YAML file alongside raw data</li> </ol> <p>See main() function for entry point and run_data_generation() for the complete pipeline.</p>"},{"location":"bnode_core/data_generation/raw_data_generation/#bnode_core.data_generation.raw_data_generation--key-features","title":"Key Features","text":"<ul> <li>Parallel execution using Dask LocalCluster with configurable workers</li> <li>Per-simulation timeout enforcement via ThreadPoolExecutor</li> <li>Automatic worker restart on repeated timeouts</li> <li>Incremental result writing (partial data available if interrupted)</li> <li>Comprehensive logging: completed, failed, timed-out simulations</li> <li>Multiple control sampling strategies (R, RO, ROCS, RROCS, RS, RF, file, Excel)</li> <li>Reproducible sampling (fixed seed since 2024-11-23)</li> <li>Dask dashboard for monitoring: http://localhost:8787</li> </ul>"},{"location":"bnode_core/data_generation/raw_data_generation/#bnode_core.data_generation.raw_data_generation--sampling-strategies","title":"Sampling Strategies","text":"<pre><code>Parameters: 'R' (random uniform)\nInitial states: 'R' (random uniform)\nControls: 'R' (random uniform), 'RO' (random with offset), 'ROCS' (cubic splines with \n          clipping), 'RROCS' (cubic splines with random rescaling), 'RS' (random steps), \n          'RF' (frequency sweep), 'file' (from CSV), 'constantInput' (from Excel)\n</code></pre>"},{"location":"bnode_core/data_generation/raw_data_generation/#bnode_core.data_generation.raw_data_generation--configuration","title":"Configuration","text":"<pre><code>Uses Hydra for configuration management. Config loaded from 'data_generation.yaml'.\nKey config sections: pModel.RawData (all generation parameters including FMU path, bounds, \nsolver settings, sampling strategies), multiprocessing_processes (worker count), \nmemory_limit_per_worker (per-worker memory limit).\n</code></pre>"},{"location":"bnode_core/data_generation/raw_data_generation/#bnode_core.data_generation.raw_data_generation--output-files","title":"Output Files","text":"<pre><code>- Raw data HDF5 file: Contains time, states, controls, outputs, parameters, logs\n- Config YAML file: Snapshot of pModel.RawData configuration used for generation\nBoth file paths determined by bnode_core.filepaths functions.\n</code></pre>"},{"location":"bnode_core/data_generation/raw_data_generation/#bnode_core.data_generation.raw_data_generation.random_sampling_parameters","title":"<code>random_sampling_parameters(cfg: data_gen_config) -&gt; np.ndarray</code>","text":"<p>Sample parameter values uniformly within configured bounds.</p> <p>Generates a 2D array of parameter values by sampling uniformly from the bounds  specified in cfg.pModel.RawData.parameters for each parameter.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>data_gen_config</code> <p>Data generation configuration containing parameter bounds and n_samples. cfg.pModel.RawData.parameters is a dict where each key maps to [lower_bound, upper_bound]. cfg.pModel.RawData.n_samples specifies the number of parameter sets to generate.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Parameter values with shape (n_samples, n_parameters). Each row is one  sampled parameter set.</p> Source code in <code>src/bnode_core/data_generation/raw_data_generation.py</code> <pre><code>def random_sampling_parameters(cfg: data_gen_config) -&gt; np.ndarray:\n    \"\"\"Sample parameter values uniformly within configured bounds.\n\n    Generates a 2D array of parameter values by sampling uniformly from the bounds \n    specified in cfg.pModel.RawData.parameters for each parameter.\n\n    Args:\n        cfg: Data generation configuration containing parameter bounds and n_samples.\n            cfg.pModel.RawData.parameters is a dict where each key maps to [lower_bound, upper_bound].\n            cfg.pModel.RawData.n_samples specifies the number of parameter sets to generate.\n\n    Returns:\n        np.ndarray: Parameter values with shape (n_samples, n_parameters). Each row is one \n            sampled parameter set.\n    \"\"\"\n    bounds = [[cfg.pModel.RawData.parameters[key][0], cfg.pModel.RawData.parameters[key][1]] for key in cfg.pModel.RawData.parameters.keys()]\n    param_values = np.zeros((cfg.pModel.RawData.n_samples, len(cfg.pModel.RawData.parameters.keys())))\n    for i in range(len(cfg.pModel.RawData.parameters.keys())):\n        param_values[:, i] = np.random.uniform(bounds[i][0], bounds[i][1], cfg.pModel.RawData.n_samples)\n    return param_values\n</code></pre>"},{"location":"bnode_core/data_generation/raw_data_generation/#bnode_core.data_generation.raw_data_generation.random_sampling_controls","title":"<code>random_sampling_controls(cfg: data_gen_config) -&gt; np.ndarray</code>","text":"<p>Sample control input values uniformly within configured bounds.</p> <p>Generates a 3D array of control trajectories by sampling uniformly from the bounds  specified in cfg.pModel.RawData.controls for each control variable at each timestep. Each control trajectory is independently sampled (no temporal correlation).</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>data_gen_config</code> <p>Data generation configuration containing control bounds, n_samples, and sequence_length. cfg.pModel.RawData.controls is a dict where each key maps to [lower_bound, upper_bound]. cfg.pModel.RawData.n_samples specifies the number of control trajectories to generate. cfg.pModel.RawData.Solver.sequence_length specifies the number of timesteps.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Control values with shape (n_samples, n_controls, sequence_length).  Each element is independently sampled from uniform distributions.</p> Source code in <code>src/bnode_core/data_generation/raw_data_generation.py</code> <pre><code>def random_sampling_controls(cfg: data_gen_config) -&gt; np.ndarray:\n    \"\"\"Sample control input values uniformly within configured bounds.\n\n    Generates a 3D array of control trajectories by sampling uniformly from the bounds \n    specified in cfg.pModel.RawData.controls for each control variable at each timestep.\n    Each control trajectory is independently sampled (no temporal correlation).\n\n    Args:\n        cfg: Data generation configuration containing control bounds, n_samples, and sequence_length.\n            cfg.pModel.RawData.controls is a dict where each key maps to [lower_bound, upper_bound].\n            cfg.pModel.RawData.n_samples specifies the number of control trajectories to generate.\n            cfg.pModel.RawData.Solver.sequence_length specifies the number of timesteps.\n\n    Returns:\n        np.ndarray: Control values with shape (n_samples, n_controls, sequence_length). \n            Each element is independently sampled from uniform distributions.\n    \"\"\"\n    bounds = [[cfg.pModel.RawData.controls[key][0], cfg.pModel.RawData.controls[key][1]] for key in cfg.pModel.RawData.controls.keys()]\n    ctrl_values = np.zeros((cfg.pModel.RawData.n_samples, len(cfg.pModel.RawData.controls.keys()), cfg.pModel.RawData.Solver.sequence_length))\n    for i in range(len(cfg.pModel.RawData.controls.keys())):\n        ctrl_values[:, i, :] = np.random.uniform(bounds[i][0], bounds[i][1], (cfg.pModel.RawData.n_samples, cfg.pModel.RawData.Solver.sequence_length))\n    # last control input is not used.\n    return ctrl_values\n</code></pre>"},{"location":"bnode_core/data_generation/raw_data_generation/#bnode_core.data_generation.raw_data_generation.random_sampling_controls_w_offset","title":"<code>random_sampling_controls_w_offset(cfg: data_gen_config, seq_len: Optional[int] = None, n_samples: Optional[int] = None) -&gt; np.ndarray</code>","text":"<p>Sample control trajectories with random offset and bounded amplitude.</p> <p>For each control trajectory, first samples a random offset within the control bounds,  then samples an amplitude that ensures the trajectory stays within bounds. Each timestep  is sampled uniformly within [offset - amplitude_lower, offset + amplitude_upper].</p> <p>This produces control trajectories that vary around a central offset value rather than  exploring the full control space independently at each timestep.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>data_gen_config</code> <p>Data generation configuration containing control bounds. cfg.pModel.RawData.controls is a dict where each key maps to [lower_bound, upper_bound]. cfg.pModel.RawData.n_samples and cfg.pModel.RawData.Solver.sequence_length are used  as defaults if n_samples or seq_len are not provided.</p> required <code>seq_len</code> <code>Optional[int]</code> <p>Optional sequence length override. If None, uses cfg.pModel.RawData.Solver.sequence_length.</p> <code>None</code> <code>n_samples</code> <code>Optional[int]</code> <p>Optional sample count override. If None, uses cfg.pModel.RawData.n_samples.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Control values with shape (n_samples, n_controls, seq_len). Each trajectory  varies around a sampled offset with bounded amplitude.</p> Source code in <code>src/bnode_core/data_generation/raw_data_generation.py</code> <pre><code>def random_sampling_controls_w_offset(cfg: data_gen_config, seq_len: Optional[int] = None, n_samples: Optional[int] = None) -&gt; np.ndarray:\n    \"\"\"Sample control trajectories with random offset and bounded amplitude.\n\n    For each control trajectory, first samples a random offset within the control bounds, \n    then samples an amplitude that ensures the trajectory stays within bounds. Each timestep \n    is sampled uniformly within [offset - amplitude_lower, offset + amplitude_upper].\n\n    This produces control trajectories that vary around a central offset value rather than \n    exploring the full control space independently at each timestep.\n\n    Args:\n        cfg: Data generation configuration containing control bounds.\n            cfg.pModel.RawData.controls is a dict where each key maps to [lower_bound, upper_bound].\n            cfg.pModel.RawData.n_samples and cfg.pModel.RawData.Solver.sequence_length are used \n            as defaults if n_samples or seq_len are not provided.\n        seq_len: Optional sequence length override. If None, uses cfg.pModel.RawData.Solver.sequence_length.\n        n_samples: Optional sample count override. If None, uses cfg.pModel.RawData.n_samples.\n\n    Returns:\n        np.ndarray: Control values with shape (n_samples, n_controls, seq_len). Each trajectory \n            varies around a sampled offset with bounded amplitude.\n    \"\"\"\n    bounds = [[cfg.pModel.RawData.controls[key][0], cfg.pModel.RawData.controls[key][1]] for key in cfg.pModel.RawData.controls.keys()]\n    ctrl_values = np.zeros((cfg.pModel.RawData.n_samples if n_samples is None else n_samples, len(cfg.pModel.RawData.controls.keys()), cfg.pModel.RawData.Solver.sequence_length if seq_len is None else seq_len))\n    for j in range(ctrl_values.shape[0]):\n        for i in range(len(cfg.pModel.RawData.controls.keys())):\n            # get offset\n            offset = np.random.uniform(bounds[i][0], bounds[i][1])\n            # get amplitude\n            amplitude = np.random.uniform(0, bounds[i][1] - bounds[i][0])\n            # reduce amplitude if offset is close to bounds\n            amplitude_upper = amplitude if bounds[i][1] - amplitude &gt; offset else bounds[i][1] - offset\n            amplitude_lower = amplitude if bounds[i][0] + amplitude &lt; offset else offset - bounds[i][0]\n            ctrl_values[j, i, :] = np.random.uniform(offset - amplitude_lower, offset + amplitude_upper, ctrl_values.shape[2])\n    # last control input is not used.\n    return ctrl_values\n</code></pre>"},{"location":"bnode_core/data_generation/raw_data_generation/#bnode_core.data_generation.raw_data_generation.random_sampling_controls_w_offset_cubic_splines_old_clip_manual","title":"<code>random_sampling_controls_w_offset_cubic_splines_old_clip_manual(cfg: data_gen_config) -&gt; np.ndarray</code>","text":"<p>Sample control trajectories using cubic spline interpolation with manual clipping (ROCS).</p> <p>Also known as ROCS (Random Offset Cubic Splines). Generates smooth control trajectories by:</p> <ol> <li>Sampling control values at random intervals</li> <li>Interpolating with cubic splines</li> <li>Normalizing to fit within bounds via manual clipping</li> </ol> <p>ROCS fills the control space more than RROCS because values exceeding bounds are clipped  to the bounds rather than rescaled.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>data_gen_config</code> <p>Data generation configuration. cfg.pModel.RawData.controls_frequency_min_in_timesteps: minimum interval between samples. cfg.pModel.RawData.controls_frequency_max_in_timesteps: maximum interval between samples. cfg.pModel.RawData.controls: dict of control bounds [lower, upper].</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Control values with shape (n_samples, n_controls, sequence_length). Smooth trajectories that fill the control space via clipping.</p> Source code in <code>src/bnode_core/data_generation/raw_data_generation.py</code> <pre><code>def random_sampling_controls_w_offset_cubic_splines_old_clip_manual(cfg: data_gen_config) -&gt; np.ndarray:\n    \"\"\"Sample control trajectories using cubic spline interpolation with manual clipping (ROCS).\n\n    Also known as ROCS (Random Offset Cubic Splines). Generates smooth control trajectories by:\n\n    1. Sampling control values at random intervals\n    2. Interpolating with cubic splines\n    3. Normalizing to fit within bounds via manual clipping\n\n    ROCS fills the control space more than RROCS because values exceeding bounds are clipped \n    to the bounds rather than rescaled.\n\n    Args:\n        cfg: Data generation configuration.\n            cfg.pModel.RawData.controls_frequency_min_in_timesteps: minimum interval between samples.\n            cfg.pModel.RawData.controls_frequency_max_in_timesteps: maximum interval between samples.\n            cfg.pModel.RawData.controls: dict of control bounds [lower, upper].\n\n    Returns:\n        np.ndarray: Control values with shape (n_samples, n_controls, sequence_length).\n            Smooth trajectories that fill the control space via clipping.\n    \"\"\"\n    freq_sequence = np.random.choice(np.arange(cfg.pModel.RawData.controls_frequency_min_in_timesteps, cfg.pModel.RawData.controls_frequency_max_in_timesteps + 1), cfg.pModel.RawData.n_samples)\n    # find out at which entry we reached the sequence length\n    seq_len_sampling = np.where(np.cumsum(freq_sequence) &gt; cfg.pModel.RawData.Solver.sequence_length)[0][0] + 1\n    # sample data\n    ctrl_values_sampled = random_sampling_controls_w_offset(cfg, seq_len_sampling+1)\n    # create cubic splines\n    x = np.concatenate((np.array([0]),\n                       np.cumsum(freq_sequence[:seq_len_sampling]))\n                       )\n    xnew = np.arange(cfg.pModel.RawData.Solver.sequence_length)\n    ctrl_values = CubicSpline(x, ctrl_values_sampled, axis=2)(xnew)\n    # normalize values to bounds\n    bounds = [[cfg.pModel.RawData.controls[key][0], cfg.pModel.RawData.controls[key][1]] for key in cfg.pModel.RawData.controls.keys()]\n    for i in range(ctrl_values.shape[0]):\n        for j in range(ctrl_values.shape[1]):\n            min_val = np.min(ctrl_values[i, j, :])\n            max_val = np.max(ctrl_values[i, j, :])\n\n            exceeds_bounds = max_val - min_val &gt; bounds[j][1] - bounds[j][0]\n            delta = max_val - min_val if  exceeds_bounds else bounds[j][1] - bounds[j][0]\n\n            # calculate base:\n            if exceeds_bounds:\n                base = bounds[j][0]\n            elif min_val &lt; bounds[j][0]:\n                base = bounds[j][0]\n            elif max_val &gt; bounds[j][1]:\n                base = bounds[j][1] - delta\n            else:\n                base = min_val\n            ctrl_values[i, j, :] = (ctrl_values[i, j, :] - min_val) / delta * (bounds[j][1] - bounds[j][0]) + base\n            if ctrl_values[i, j, :].min() &lt; bounds[j][0] or ctrl_values[i, j, :].max() &gt; bounds[j][1]:\n                print('error in random_sampling_controls_w_offset_cubic_splines')\n    return ctrl_values\n</code></pre>"},{"location":"bnode_core/data_generation/raw_data_generation/#bnode_core.data_generation.raw_data_generation.random_sampling_controls_w_offset_cubic_splines_clip_random","title":"<code>random_sampling_controls_w_offset_cubic_splines_clip_random(cfg: data_gen_config) -&gt; np.ndarray</code>","text":"<p>Sample control trajectories using cubic spline interpolation with random rescaling (RROCS).</p> <p>Also known as RROCS (Randomly Rescaled Offset Cubic Splines). Generates smooth control  trajectories by:</p> <ol> <li>For each control and sample, sampling values at random intervals (e.g. different frequencies),  with sampled amplitudes and offsets</li> <li>Interpolating with cubic splines</li> <li>Normalizing to [0, 1] and rescaling with randomly sampled base and delta</li> <li>Optionally clipping to tighter bounds if specified</li> </ol> <p>RROCS fills the control space less uniformly than ROCS because values are rescaled to fit within bounds rather than clipped. This means, that typically at the sampling bounds, less samples are present.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>data_gen_config</code> <p>Data generation configuration. cfg.pModel.RawData.controls_frequency_min_in_timesteps: minimum interval between samples. cfg.pModel.RawData.controls_frequency_max_in_timesteps: maximum interval between samples. cfg.pModel.RawData.controls: dict where each key maps to [lower, upper] or      [lower, upper, clip_lower, clip_upper] for optional tighter clipping bounds.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Control values with shape (n_samples, n_controls, sequence_length). Smooth trajectories with diverse amplitude and offset characteristics.</p> Source code in <code>src/bnode_core/data_generation/raw_data_generation.py</code> <pre><code>def random_sampling_controls_w_offset_cubic_splines_clip_random(cfg: data_gen_config) -&gt; np.ndarray:\n    \"\"\"Sample control trajectories using cubic spline interpolation with random rescaling (RROCS).\n\n    Also known as RROCS (Randomly Rescaled Offset Cubic Splines). Generates smooth control \n    trajectories by:\n\n    1. For each control and sample, sampling values at random intervals (e.g. different frequencies), \n    with sampled amplitudes and offsets\n    2. Interpolating with cubic splines\n    3. Normalizing to [0, 1] and rescaling with randomly sampled base and delta\n    4. Optionally clipping to tighter bounds if specified\n\n    RROCS fills the control space less uniformly than ROCS because values are rescaled to fit\n    within bounds rather than clipped. This means, that typically at the sampling bounds, less\n    samples are present.\n\n    Args:\n        cfg: Data generation configuration.\n            cfg.pModel.RawData.controls_frequency_min_in_timesteps: minimum interval between samples.\n            cfg.pModel.RawData.controls_frequency_max_in_timesteps: maximum interval between samples.\n            cfg.pModel.RawData.controls: dict where each key maps to [lower, upper] or \n                [lower, upper, clip_lower, clip_upper] for optional tighter clipping bounds.\n\n    Returns:\n        np.ndarray: Control values with shape (n_samples, n_controls, sequence_length).\n            Smooth trajectories with diverse amplitude and offset characteristics.\n    \"\"\"\n    # freq_sequence = np.random.choice(np.arange(cfg.pModel.RawData.controls_frequency_min_in_timesteps, cfg.pModel.RawData.controls_frequency_max_in_timesteps + 1), cfg.pModel.RawData.n_samples)\n    # # find out at which entry we reached the sequence length\n    # seq_len_sampling = np.where(np.cumsum(freq_sequence) &gt; cfg.pModel.RawData.Solver.sequence_length)[0][0] + 1\n    # # sample data\n    # ctrl_values_sampled = random_sampling_controls_w_offset(cfg, seq_len_sampling+1)\n    # # create cubic splines\n    # x = np.concatenate((np.array([0]),\n    #                    np.cumsum(freq_sequence[:seq_len_sampling]))\n    #                    )\n    # xnew = np.arange(cfg.pModel.RawData.Solver.sequence_length)\n    # ctrl_values = CubicSpline(x, ctrl_values_sampled, axis=2)(xnew)\n\n    # normalize values to bounds\n    bounds = [[cfg.pModel.RawData.controls[key][0], cfg.pModel.RawData.controls[key][1]] for key in cfg.pModel.RawData.controls.keys()]\n    # get clip values, if available\n    clip_bounds = [cfg.pModel.RawData.controls[key][2:] if len(cfg.pModel.RawData.controls[key]) == 4 else None for key in cfg.pModel.RawData.controls.keys()]\n    for j, clip in enumerate(clip_bounds):\n        if clip is not None:\n            logging.info('control {}: clip values provided: {}'.format(list(cfg.pModel.RawData.controls.keys())[j], clip))\n\n    ctrl_values = np.zeros((cfg.pModel.RawData.n_samples, len(cfg.pModel.RawData.controls.keys()), cfg.pModel.RawData.Solver.sequence_length))\n    # loop over samples\n    for i in range(ctrl_values.shape[0]):\n        # loop over controls\n        for j in range(ctrl_values.shape[1]):\n            freq_sequence = np.random.choice(np.arange(cfg.pModel.RawData.controls_frequency_min_in_timesteps, cfg.pModel.RawData.controls_frequency_max_in_timesteps + 1), cfg.pModel.RawData.Solver.sequence_length)\n            # find out at which entry we reached the sequence length\n            seq_len_sampling = np.where(np.cumsum(freq_sequence) &gt; cfg.pModel.RawData.Solver.sequence_length)[0][0] + 1\n            # sample data\n            ctrl_values_sampled = random_sampling_controls_w_offset(cfg, seq_len_sampling+1, n_samples=1)\n            # create cubic splines\n            x = np.concatenate((np.array([0]),\n                            np.cumsum(freq_sequence[:seq_len_sampling]))\n                            )\n            xnew = np.arange(cfg.pModel.RawData.Solver.sequence_length)\n            ctrl_values[i, j, :] = CubicSpline(x, ctrl_values_sampled[0, j])(xnew)\n\n            # normalize values to bounds\n            min_val = np.min(ctrl_values[i, j, :])\n            max_val = np.max(ctrl_values[i, j, :])\n            # normalize data to min 0 and max 1\n            _values = (ctrl_values[i, j, :] - min_val) / (max_val - min_val)\n            # randomly samply base and delta\n            base = np.random.uniform(bounds[j][0], bounds[j][1])\n            delta = np.random.uniform(0, bounds[j][1]-bounds[j][0])\n            # calculate new base if delta is too large\n            if base + delta &gt; bounds[j][1]:\n                base = bounds[j][1] - delta\n            elif base - delta &lt; bounds[j][0]:\n                base = bounds[j][0]\n            # calculate new values\n            ctrl_values[i, j, :] = _values * delta + base\n            # clip to clip bounds if available\n            if clip_bounds[j] is not None:\n                ctrl_values[i, j, :] = np.clip(ctrl_values[i, j, :], clip_bounds[j][0], clip_bounds[j][1])\n            # if ctrl_values[i, j, :].min() &lt; bounds[j][0] or ctrl_values[i, j, :].max() &gt; bounds[j][1]:\n            #     print('error in random_sampling_controls_w_offset_cubic_splines')\n    return ctrl_values\n</code></pre>"},{"location":"bnode_core/data_generation/raw_data_generation/#bnode_core.data_generation.raw_data_generation.random_steps_sampling_controls","title":"<code>random_steps_sampling_controls(cfg: data_gen_config) -&gt; np.ndarray</code>","text":"<p>Sample step-change control trajectories for system response testing.</p> <p>Generates control trajectories with a single step change at the midpoint. Each control  starts at a randomly sampled value and steps to another randomly sampled value halfway  through the sequence. Useful for testing system step response characteristics.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>data_gen_config</code> <p>Data generation configuration. cfg.pModel.RawData.controls: dict of control bounds [lower, upper]. cfg.pModel.RawData.n_samples: number of step trajectories to generate. cfg.pModel.RawData.Solver.sequence_length: total trajectory length.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Control values with shape (n_samples, n_controls, sequence_length). Each trajectory has a step change at sequence_length // 2.</p> Source code in <code>src/bnode_core/data_generation/raw_data_generation.py</code> <pre><code>def random_steps_sampling_controls(cfg: data_gen_config) -&gt; np.ndarray:\n    \"\"\"Sample step-change control trajectories for system response testing.\n\n    Generates control trajectories with a single step change at the midpoint. Each control \n    starts at a randomly sampled value and steps to another randomly sampled value halfway \n    through the sequence. Useful for testing system step response characteristics.\n\n    Args:\n        cfg: Data generation configuration.\n            cfg.pModel.RawData.controls: dict of control bounds [lower, upper].\n            cfg.pModel.RawData.n_samples: number of step trajectories to generate.\n            cfg.pModel.RawData.Solver.sequence_length: total trajectory length.\n\n    Returns:\n        np.ndarray: Control values with shape (n_samples, n_controls, sequence_length).\n            Each trajectory has a step change at sequence_length // 2.\n    \"\"\"\n    bounds = [[cfg.pModel.RawData.controls[key][0], cfg.pModel.RawData.controls[key][1]] for key in cfg.pModel.RawData.controls.keys()]\n    ctrl_values = np.zeros((cfg.pModel.RawData.n_samples, len(cfg.pModel.RawData.controls.keys()), cfg.pModel.RawData.Solver.sequence_length))\n\n    i_step = cfg.pModel.RawData.Solver.sequence_length // 2\n    for i in range(len(cfg.pModel.RawData.controls.keys())):\n        #ctrl_values[:, i, :] = np.random.uniform(bounds[i][0], bounds[i][1], (cfg.pModel.RawData.n_samples, cfg.pModel.RawData.Solver.sequence_length))\n        _signal_value_before_step = np.random.uniform(bounds[i][0], bounds[i][1], cfg.pModel.RawData.n_samples)\n        _signal_value_after_step = np.random.uniform(bounds[i][0], bounds[i][1], cfg.pModel.RawData.n_samples)\n        ctrl_values[:, i, :i_step] = _signal_value_before_step[:, None]\n        ctrl_values[:, i, i_step:] = _signal_value_after_step[:, None]\n\n    # last control input is not used.\n    return ctrl_values\n</code></pre>"},{"location":"bnode_core/data_generation/raw_data_generation/#bnode_core.data_generation.raw_data_generation.random_frequency_response_sampling_controls","title":"<code>random_frequency_response_sampling_controls(cfg: data_gen_config) -&gt; np.ndarray</code>","text":"<p>Sample frequency-sweep control trajectories for system identification.</p> <p>Generates control trajectories with a chirp (frequency sweep) starting at the midpoint.  The first half is constant, and the second half contains a sine wave with linearly  increasing frequency from min to max. Useful for system identification and frequency  response analysis.</p> <p>The frequency sweep goes from _min_frequency (low) to _max_frequency (high), calculated  based on the configured control frequency bounds (multiplied by 4 since these represent  half-periods).</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>data_gen_config</code> <p>Data generation configuration. cfg.pModel.RawData.controls: dict of control bounds [lower, upper]. cfg.pModel.RawData.controls_frequency_min_in_timesteps: base for max sweep frequency. cfg.pModel.RawData.controls_frequency_max_in_timesteps: base for min sweep frequency. cfg.pModel.RawData.n_samples: number of trajectories to generate. cfg.pModel.RawData.Solver.sequence_length: total trajectory length.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Control values with shape (n_samples, n_controls, sequence_length). First half constant, second half contains frequency sweep.</p> Source code in <code>src/bnode_core/data_generation/raw_data_generation.py</code> <pre><code>def random_frequency_response_sampling_controls(cfg: data_gen_config) -&gt; np.ndarray:\n    \"\"\"Sample frequency-sweep control trajectories for system identification.\n\n    Generates control trajectories with a chirp (frequency sweep) starting at the midpoint. \n    The first half is constant, and the second half contains a sine wave with linearly \n    increasing frequency from min to max. Useful for system identification and frequency \n    response analysis.\n\n    The frequency sweep goes from _min_frequency (low) to _max_frequency (high), calculated \n    based on the configured control frequency bounds (multiplied by 4 since these represent \n    half-periods).\n\n    Args:\n        cfg: Data generation configuration.\n            cfg.pModel.RawData.controls: dict of control bounds [lower, upper].\n            cfg.pModel.RawData.controls_frequency_min_in_timesteps: base for max sweep frequency.\n            cfg.pModel.RawData.controls_frequency_max_in_timesteps: base for min sweep frequency.\n            cfg.pModel.RawData.n_samples: number of trajectories to generate.\n            cfg.pModel.RawData.Solver.sequence_length: total trajectory length.\n\n    Returns:\n        np.ndarray: Control values with shape (n_samples, n_controls, sequence_length).\n            First half constant, second half contains frequency sweep.\n    \"\"\"\n    bounds = [[cfg.pModel.RawData.controls[key][0], cfg.pModel.RawData.controls[key][1]] for key in cfg.pModel.RawData.controls.keys()]\n    ctrl_values = np.zeros((cfg.pModel.RawData.n_samples, len(cfg.pModel.RawData.controls.keys()), cfg.pModel.RawData.Solver.sequence_length))\n\n    _max_frequency = cfg.pModel.RawData.controls_frequency_min_in_timesteps * 4 # because this is only half the frequency\n    _min_frequency = cfg.pModel.RawData.controls_frequency_max_in_timesteps * 4 \n\n    i_step = cfg.pModel.RawData.Solver.sequence_length // 2\n    len_frequency = cfg.pModel.RawData.Solver.sequence_length - i_step\n\n    freq_fun = lambda x: _min_frequency + (_max_frequency - _min_frequency) * (x/len_frequency)\n    turns = np.zeros(len_frequency)\n    for i in range(1,len_frequency):\n        turns[i] = turns[i-1] + (1/freq_fun(i))\n    phi = turns * (2 * np.pi) \n    sine = np.sin(phi)\n    for i in range(cfg.pModel.RawData.n_samples):\n        for j in range(len(cfg.pModel.RawData.controls.keys())):\n            _signal_value_start = np.random.uniform(bounds[j][0], bounds[j][1],1)\n            ctrl_values[i, j, :i_step] = _signal_value_start[:, None]\n            _amplitude = np.random.uniform(0, bounds[j][1] - bounds[j][0])\n            if bounds[j][1]  &lt; _signal_value_start + _amplitude:\n                _amplitude = bounds[j][1] - _signal_value_start\n            if bounds[j][0] &gt; _signal_value_start - _amplitude:\n                _amplitude = _signal_value_start - bounds[j][0]\n            assert bounds[j][0] + _amplitude &lt;= _signal_value_start &lt;= bounds[j][1] - _amplitude\n            _signal_value_end = _signal_value_start + sine * _amplitude\n            ctrl_values[i, j, i_step:] = _signal_value_end[:]\n    return ctrl_values\n</code></pre>"},{"location":"bnode_core/data_generation/raw_data_generation/#bnode_core.data_generation.raw_data_generation.load_controls_from_file","title":"<code>load_controls_from_file(cfg: data_gen_config) -&gt; np.ndarray</code>","text":"<p>Load control trajectories from a CSV file and resample to simulation time vector.</p> <p>Reads control values from a CSV file where columns match control variable names from the  config. The CSV must include a 'time' column. Control values are resampled via linear  interpolation to match the simulation timestep, then replicated for all samples.</p> <p>TODO: could be extended to load multiple trajectories for different samples.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>data_gen_config</code> <p>Data generation configuration. cfg.pModel.RawData.controls_file_path: path to CSV file with time and control columns. cfg.pModel.RawData.controls: dict of control names (used as column names). cfg.pModel.RawData.Solver: simulation time parameters (start, end, timestep). cfg.pModel.RawData.n_samples: number of times to replicate the loaded trajectory.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Control values with shape (n_samples, n_controls, sequence_length). Same trajectory replicated across all samples.</p> Source code in <code>src/bnode_core/data_generation/raw_data_generation.py</code> <pre><code>def load_controls_from_file(cfg: data_gen_config) -&gt; np.ndarray:\n    \"\"\"Load control trajectories from a CSV file and resample to simulation time vector.\n\n    Reads control values from a CSV file where columns match control variable names from the \n    config. The CSV must include a 'time' column. Control values are resampled via linear \n    interpolation to match the simulation timestep, then replicated for all samples.\n\n    TODO: could be extended to load multiple trajectories for different samples.\n\n    Args:\n        cfg: Data generation configuration.\n            cfg.pModel.RawData.controls_file_path: path to CSV file with time and control columns.\n            cfg.pModel.RawData.controls: dict of control names (used as column names).\n            cfg.pModel.RawData.Solver: simulation time parameters (start, end, timestep).\n            cfg.pModel.RawData.n_samples: number of times to replicate the loaded trajectory.\n\n    Returns:\n        np.ndarray: Control values with shape (n_samples, n_controls, sequence_length).\n            Same trajectory replicated across all samples.\n    \"\"\"\n    # load controls from file by control variable name\n    _df = pd.read_csv(cfg.pModel.RawData.controls_file_path)\n    _list = []\n    for key in cfg.pModel.RawData.controls.keys():\n        # append to list column that matches the key\n        _list.append(_df[key].values)\n    time_ctrls = _df['time'].values\n    # resample to time vector TODO: better make time vector only once\n    time = np.arange(cfg.pModel.RawData.Solver.simulationStartTime, cfg.pModel.RawData.Solver.simulationEndTime + cfg.pModel.RawData.Solver.timestep, cfg.pModel.RawData.Solver.timestep)\n    ctrl_values = [np.interp(time, time_ctrls, ctrl) for ctrl in _list]\n    ctrl_values = np.array(ctrl_values)\n    ctrl_values = np.expand_dims(ctrl_values, axis=0)\n    ctrl_values = np.repeat(ctrl_values, cfg.pModel.RawData.n_samples, axis=0)\n    return ctrl_values\n</code></pre>"},{"location":"bnode_core/data_generation/raw_data_generation/#bnode_core.data_generation.raw_data_generation.constant_input_simulation_from_excel","title":"<code>constant_input_simulation_from_excel(cfg: data_gen_config) -&gt; np.ndarray</code>","text":"<p>Load constant control values from an Excel file for steady-state simulations.</p> <p>Reads an Excel file with a sheet named 'Tabelle1' where each row defines one simulation  with constant control values. Control columns must be named to match config control names.  Each row's values are held constant for the entire sequence length.</p> <p>Useful for steady-state simulations or parameter sweeps with constant inputs.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>data_gen_config</code> <p>Data generation configuration. cfg.pModel.RawData.controls_file_path: path to Excel file. cfg.pModel.RawData.controls: dict of control names (must match column names in Excel). cfg.pModel.RawData.Solver.sequence_length: length to replicate constant values.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Control values with shape (n_rows, n_controls, sequence_length). Each row from Excel becomes one sample with constant control values.</p> Notes <p>Excel file structure: - Sheet name: 'Tabelle1' - First row: column headers matching control variable names - Each subsequent row: one set of constant control values for one simulation</p> Source code in <code>src/bnode_core/data_generation/raw_data_generation.py</code> <pre><code>def constant_input_simulation_from_excel(cfg: data_gen_config) -&gt; np.ndarray:\n    \"\"\"Load constant control values from an Excel file for steady-state simulations.\n\n    Reads an Excel file with a sheet named 'Tabelle1' where each row defines one simulation \n    with constant control values. Control columns must be named to match config control names. \n    Each row's values are held constant for the entire sequence length.\n\n    Useful for steady-state simulations or parameter sweeps with constant inputs.\n\n    Args:\n        cfg: Data generation configuration.\n            cfg.pModel.RawData.controls_file_path: path to Excel file.\n            cfg.pModel.RawData.controls: dict of control names (must match column names in Excel).\n            cfg.pModel.RawData.Solver.sequence_length: length to replicate constant values.\n\n    Returns:\n        np.ndarray: Control values with shape (n_rows, n_controls, sequence_length).\n            Each row from Excel becomes one sample with constant control values.\n\n    Notes:\n        Excel file structure:\n        - Sheet name: 'Tabelle1'\n        - First row: column headers matching control variable names\n        - Each subsequent row: one set of constant control values for one simulation\n    \"\"\"\n    file = pd.ExcelFile(cfg.pModel.RawData.controls_file_path)\n    _df = file.parse(sheet_name='Tabelle1')\n    _list = []\n    for key in cfg.pModel.RawData.controls.keys():\n        _list.append(_df[key].values)\n    ctrl_values = np.array(_list).transpose()\n    ctrl_values = np.expand_dims(ctrl_values, axis=2)\n    ctrl_values = np.repeat(ctrl_values, (cfg.pModel.RawData.Solver.sequence_length), axis=2)\n    return ctrl_values\n</code></pre>"},{"location":"bnode_core/data_generation/raw_data_generation/#bnode_core.data_generation.raw_data_generation.random_sampling_initial_states","title":"<code>random_sampling_initial_states(cfg: data_gen_config) -&gt; np.ndarray</code>","text":"<p>Sample initial state values uniformly within configured bounds.</p> <p>Generates a 2D array of initial state values by sampling uniformly from the bounds  specified in cfg.pModel.RawData.states for each state variable.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>data_gen_config</code> <p>Data generation configuration containing state bounds and n_samples. cfg.pModel.RawData.states is a dict where each key maps to [lower_bound, upper_bound]. cfg.pModel.RawData.n_samples specifies the number of initial state sets to generate.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Initial state values with shape (n_samples, n_states). Each row is one  sampled initial state vector.</p> Source code in <code>src/bnode_core/data_generation/raw_data_generation.py</code> <pre><code>def random_sampling_initial_states(cfg: data_gen_config) -&gt; np.ndarray:\n    \"\"\"Sample initial state values uniformly within configured bounds.\n\n    Generates a 2D array of initial state values by sampling uniformly from the bounds \n    specified in cfg.pModel.RawData.states for each state variable.\n\n    Args:\n        cfg: Data generation configuration containing state bounds and n_samples.\n            cfg.pModel.RawData.states is a dict where each key maps to [lower_bound, upper_bound].\n            cfg.pModel.RawData.n_samples specifies the number of initial state sets to generate.\n\n    Returns:\n        np.ndarray: Initial state values with shape (n_samples, n_states). Each row is one \n            sampled initial state vector.\n    \"\"\"\n    bounds = [[cfg.pModel.RawData.states[key][0], cfg.pModel.RawData.states[key][1]] for key in cfg.pModel.RawData.states.keys()]\n    initial_state_values = np.zeros((cfg.pModel.RawData.n_samples, len(cfg.pModel.RawData.states.keys())))\n    for i in range(len(cfg.pModel.RawData.states.keys())):\n        initial_state_values[:, i] = np.random.uniform(bounds[i][0], bounds[i][1], cfg.pModel.RawData.n_samples)\n    return initial_state_values\n</code></pre>"},{"location":"bnode_core/data_generation/raw_data_generation/#bnode_core.data_generation.raw_data_generation.progress_string","title":"<code>progress_string(progress: float, length: int = 10) -&gt; str</code>","text":"<p>Generate a visual progress bar string for logging.</p> <p>Returns a visual progress string of the form '|||||.....' for a given progress value in [0, 1].</p> <p>Parameters:</p> Name Type Description Default <code>progress</code> <code>float</code> <p>Progress value between 0 and 1.</p> required <code>length</code> <code>int</code> <p>Total length of the progress string.</p> <code>10</code> <p>Returns:</p> Type Description <code>str</code> <p>Progress bar string with '|' for completed portion and '.' for remaining.</p> Source code in <code>src/bnode_core/data_generation/raw_data_generation.py</code> <pre><code>def progress_string(progress: float, length: int = 10) -&gt; str:\n    \"\"\"Generate a visual progress bar string for logging.\n\n    Returns a visual progress string of the form '|||||.....' for a given progress value in [0, 1].\n\n    Args:\n        progress: Progress value between 0 and 1.\n        length: Total length of the progress string.\n\n    Returns:\n        Progress bar string with '|' for completed portion and '.' for remaining.\n    \"\"\"\n    progress = max(0, min(1, progress))  # Clamp to [0, 1]\n    n_complete = int(round(progress * length))\n    n_remaining = length - n_complete\n    return '|' * n_complete + '.' * n_remaining\n</code></pre>"},{"location":"bnode_core/data_generation/raw_data_generation/#bnode_core.data_generation.raw_data_generation.data_generation","title":"<code>data_generation(cfg: data_gen_config, initial_state_values: np.ndarray = None, param_values: np.ndarray = None, ctrl_values: np.ndarray = None)</code>","text":"<p>Execute parallel FMU simulations and write results to raw data HDF5 file.</p> <p>Core data generation function that:</p> <ol> <li>Sets up a Dask distributed cluster for parallel FMU simulation</li> <li>Submits simulation tasks for each sample in batches</li> <li>Monitors task completion and handles timeouts/failures</li> <li>Incrementally writes results to the raw data HDF5 file</li> <li>Logs completion status, failures, and timing information</li> </ol> <p>The function uses ThreadPoolExecutor to enforce per-simulation timeouts and Dask's  LocalCluster for parallel execution across multiple workers. Results are written  incrementally so partial data is available even if generation is interrupted.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>data_gen_config</code> <p>Data generation configuration containing: - FMU path and simulation parameters - Solver settings (timestep, tolerance, timeout) - Multiprocessing and memory settings - Output file paths</p> required <code>initial_state_values</code> <code>ndarray</code> <p>Optional array of shape (n_samples, n_states) with initial states.</p> <code>None</code> <code>param_values</code> <code>ndarray</code> <p>Optional array of shape (n_samples, n_parameters) with parameter values.</p> <code>None</code> <code>ctrl_values</code> <code>ndarray</code> <p>Optional array of shape (n_samples, n_controls, sequence_length) with controls.</p> <code>None</code> Notes <ul> <li>The raw data HDF5 file must already exist with pre-allocated datasets.</li> <li>Dask worker memory limits and allowed failures are configured from cfg settings.</li> <li>Progress is logged via the Dask diagnostic dashboard at http://localhost:8787.</li> <li>Per-sample logs (completed, sim_failed, timedout, processing_time) are written    incrementally to the HDF5 file.</li> <li>If a worker's tasks timeout repeatedly, the worker is restarted automatically.</li> <li>For large numbers of samples, tasks are submitted in \"submission rounds\" (batches of 10,000 simulations)    to avoid overwhelming the scheduler.</li> </ul> <p>Raises:</p> Type Description <code>BaseException</code> <p>Any exception during generation is caught to ensure partial results  are saved before re-raising.</p> Source code in <code>src/bnode_core/data_generation/raw_data_generation.py</code> <pre><code>def data_generation(cfg: data_gen_config,\n                    initial_state_values: np.ndarray = None,\n                    param_values: np.ndarray = None,\n                    ctrl_values: np.ndarray = None):\n    \"\"\"Execute parallel FMU simulations and write results to raw data HDF5 file.\n\n    Core data generation function that:\n\n    1. Sets up a Dask distributed cluster for parallel FMU simulation\n    2. Submits simulation tasks for each sample in batches\n    3. Monitors task completion and handles timeouts/failures\n    4. Incrementally writes results to the raw data HDF5 file\n    5. Logs completion status, failures, and timing information\n\n    The function uses ThreadPoolExecutor to enforce per-simulation timeouts and Dask's \n    LocalCluster for parallel execution across multiple workers. Results are written \n    incrementally so partial data is available even if generation is interrupted.\n\n    Args:\n        cfg: Data generation configuration containing:\n            - FMU path and simulation parameters\n            - Solver settings (timestep, tolerance, timeout)\n            - Multiprocessing and memory settings\n            - Output file paths\n        initial_state_values: Optional array of shape (n_samples, n_states) with initial states.\n        param_values: Optional array of shape (n_samples, n_parameters) with parameter values.\n        ctrl_values: Optional array of shape (n_samples, n_controls, sequence_length) with controls.\n\n    Notes:\n        - The raw data HDF5 file must already exist with pre-allocated datasets.\n        - Dask worker memory limits and allowed failures are configured from cfg settings.\n        - Progress is logged via the Dask diagnostic dashboard at http://localhost:8787.\n        - Per-sample logs (completed, sim_failed, timedout, processing_time) are written \n          incrementally to the HDF5 file.\n        - If a worker's tasks timeout repeatedly, the worker is restarted automatically.\n        - For large numbers of samples, tasks are submitted in \"submission rounds\" (batches of 10,000 simulations) \n          to avoid overwhelming the scheduler.\n\n    Raises:\n        BaseException: Any exception during generation is caught to ensure partial results \n            are saved before re-raising.\n    \"\"\"\n    from bnode_core.data_generation.utils.fmu_simulate import fmu_simulate # import here to avoid circular import\n\n    # wrap fmu_simulate to include idx and catch exceptions. Time out simulations by using ThreadPoolExecutor.\n    def fmu_simulate_wrapped(idx, *args, **kwargs): \n        t0 = time()\n        with ThreadPoolExecutor(max_workers=1) as executor:\n            future = executor.submit(fmu_simulate, *args, **kwargs)\n            try:\n                res = future.result(timeout=cfg.pModel.RawData.Solver.timeout)\n                res['timeout'] = False\n            except TimeoutError:\n                res = {'success': False, 'error_messages': ['fmu_simulate timed out limit of {}s'.format(cfg.pModel.RawData.Solver.timeout)], 'timeout': True}\n        res ['idx'], res['time'] = idx, time() - t0\n        return res\n\n    # create dask client\n    from dask.distributed import Client, as_completed, LocalCluster, wait\n    _n_workers = os.cpu_count()-2 if cfg.multiprocessing_processes is None else cfg.multiprocessing_processes\n    logging.info('Setting up dask client with {} workers'.format(_n_workers))\n    # increasing the allowed failures helps dealing with fmus that do not clean up memory usage\n    # if this does not help, set it manually with \"export DASK_DISTRIBUTED__SCHEDULER__ALLOWED_FAILURES=35\" in the terminal\n    dask.config.set({'distributed.scheduler.allowed-failures': _n_workers + 4})\n    logging.info('set distributed.scheduler.allowed-failures to {}'.format(_n_workers + 4))\n    # trim memory usage\n    dask.config.set({'distributed.worker.memory.target': 0.95})\n    dask.config.set({'distributed.worker.memory.spill': 0.95})\n    dask.config.set({'distributed.worker.memory.pause': 0.95}) # this stops assigning new tasks to the worker\n    dask.config.set({'distributed.worker.memory.terminate': 0.90})\n    # set logging level to info\n    logging.getLogger('distributed.nanny').setLevel(logging.INFO)\n    logging.info('set distributed.worker.memory.target, distributed.worker.memory.spill, distributed.worker.memory.pause, distributed.worker.memory.terminate to 0.95')\n    cluster = LocalCluster(n_workers = _n_workers,\n                           threads_per_worker = 1, \n                           processes = True, \n                            memory_limit = cfg.memory_limit_per_worker,\n                            )\n    client = Client(cluster)\n    # set logging level to warning\n    logging.getLogger('distributed.worker').setLevel(logging.CRITICAL)\n    logging.info(client)\n    futures = []\n    t0 = time()\n    logging.info('view diagnostic dashboard at: http://localhost:8787')\n    logging.info('view per worker diagnostics at: http://127.0.0.1:8787/info/main/workers.html')\n    logging.info('\\t logs on this page show fmu simulation progress')\n    client.forward_logging(level=logging.WARNING)\n\n    # open raw data file\n    raw_data = h5py.File(filepath_raw_data(cfg), 'a')\n\n    # counters for logging\n    _n_completed = 0\n    _n_failed = 0\n    _n_timedout = 0\n    _n_finished = 0\n\n    # categories for results: started, completed, failed, timemout, processing time\n    raw_data.create_group('logs')\n    raw_data.create_dataset('logs/completed', data=np.zeros((cfg.pModel.RawData.n_samples,), dtype=bool))\n    raw_data.create_dataset('logs/sim_failed', data=np.zeros((cfg.pModel.RawData.n_samples,), dtype=bool))\n    raw_data.create_dataset('logs/timedout', data=np.zeros((cfg.pModel.RawData.n_samples,), dtype=bool))\n    raw_data.create_dataset('logs/processing_time', (cfg.pModel.RawData.n_samples,))\n\n    step_tasks_i = min(10000, cfg.pModel.RawData.n_samples)\n    max_submission_rounds = cfg.pModel.RawData.n_samples // step_tasks_i \n    for submission_round, max_submission_i in enumerate(range(0, cfg.pModel.RawData.n_samples, step_tasks_i)):\n        # submit simulation as futures to dask client (the computation does not block the main thread)\n        min_tasks_i = max_submission_i\n        max_tasks_i = max_submission_i + step_tasks_i\n        logging.info('submission round {}/{}: submitting and computing tasks {}-{} of {}'.format(submission_round +1, max_submission_rounds, min_tasks_i, max_tasks_i, cfg.pModel.RawData.n_samples))\n        for i in range(min_tasks_i, max_tasks_i):\n            futures.append(client.submit(fmu_simulate_wrapped, i,\n                    fmu_path = str(Path(cfg.pModel.RawData.fmuPath).resolve()),\n                    state_names = cfg.pModel.RawData.states.keys(),\n                    get_state_derivatives = cfg.pModel.RawData.states_der_include,\n                    initial_state_values = initial_state_values[i] if initial_state_values is not None else None,\n                    parameter_names = cfg.pModel.RawData.parameters.keys() if cfg.pModel.RawData.parameters is not None else None,\n                    parameter_values = param_values[i] if param_values is not None else None,\n                    control_names = cfg.pModel.RawData.controls.keys(),\n                    control_values = ctrl_values[i] if ctrl_values is not None else None,\n                    control_from_model_names = cfg.pModel.RawData.controls_from_model if cfg.pModel.RawData.controls_only_for_sampling_extract_actual_from_model else None,\n                    output_names = cfg.pModel.RawData.outputs,\n                    start_time = cfg.pModel.RawData.Solver.simulationStartTime, \n                    stop_time = cfg.pModel.RawData.Solver.simulationEndTime, \n                    fmu_simulate_step_size = cfg.pModel.RawData.Solver.timestep,\n                    fmu_simulate_tolerance = cfg.pModel.RawData.Solver.tolerance,\n                    key = i,\n                )\n            )\n\n        # time logging variables, create new for every submission round (to avoid too large dict)\n        start_time_futures = {}\n        if cfg.pModel.RawData.Solver.timeout is not None:\n            _timeout_worker_restart = min(1.2 * cfg.pModel.RawData.Solver.timeout, cfg.pModel.RawData.Solver.timeout + 30)\n        _runtime_per_future = [0.01] * _n_workers # to avoid too many requests to the scheduler, we will sleep for the average runtime of a future divided by the number of workers\n\n        # progressively process the incoming results, catch exception and save if necessary\n        try: # for catching all exceptions and saving the data that was generated so far\n            while not len(futures) == 0:\n                # determine which futures run too long and restart their workers\n                worker_states = client.run(lambda dask_worker: dask_worker.state.tasks)\n                _workers_to_restart = []\n                for worker, tasks in worker_states.items():\n                    _restart_worker = False\n                    for key, task_state in tasks.items():\n                        if task_state.state == 'executing':\n                            if key not in start_time_futures:\n                                start_time_futures[key] = time()\n                            else:\n                                if cfg.pModel.RawData.Solver.timeout is not None:\n                                    if time() - start_time_futures[key] &gt; _timeout_worker_restart:\n                                        logging.warning('fmu {} is running for more than {}s, we will restart its worker {}'.format(key, _timeout_worker_restart, worker))\n                                        _restart_worker = True\n                                        # also remove the future from the list of futures\n                                        for future in futures:\n                                            if future.key == key:\n                                                future.cancel() # cancel the future to avoid further processing\n                                                future.release()\n                                                futures.remove(future)\n                                                _n_timedout += 1\n                                                raw_data['logs/timedout'][key] = True\n                    if _restart_worker:\n                        _workers_to_restart.append(worker)\n                client.restart_workers(workers=_workers_to_restart)\n                # loop over futures and check if they are done\n                for future in futures:\n                    if future.done():\n                        if future.cancelled():\n                            logging.error('fmu {} was cancelled. This should not happen!'.format(future.key))\n                            # print reason\n                            logging.error('Reason: ')\n                            logging.error(future.exception())\n                            logging.error('Traceback: ')\n                            logging.error(future.traceback())\n                            raise Exception('fmu {} was cancelled. This should not happen!'.format(future.key))\n                        # get id of result\n                        res = future.result()\n                        idx = res['idx']\n\n                        # handle counters and save logs\n                        raw_data['logs/processing_time'][idx] = res['time']\n\n                        if res['success'] is False:\n                            if not res['timeout']:\n                                logging.error('fmu {} simulation failed, due to the following errors'.format(res['idx']))\n                                for error in res['error_messages']:\n                                    logging.error(error)\n                                raw_data['logs/sim_failed'][idx] = True\n                                _n_failed += 1\n                            else:\n                                logging.error('fmu {} timed out after {}s'.format(res['idx'], cfg.pModel.RawData.Solver.timeout))\n                                raw_data['logs/timedout'][idx] = True\n                                _n_timedout += 1\n                        else: # if completed\n                            raw_data['logs/completed'][idx] = True\n                            _n_completed += 1\n\n                        # unpack results\n                        if res['timeout'] is False:\n                            outputs, states, states_der, controls_from_model = res['outputs'], res['states'], res['states_der'], res['controls_from_model']\n                            if cfg.pModel.RawData.outputs is not None:\n                                raw_data['outputs'][idx] = outputs\n                            if cfg.pModel.RawData.controls_only_for_sampling_extract_actual_from_model is True:\n                                raw_data['controls'][idx] = controls_from_model\n                            raw_data['states'][idx] = states\n                            if cfg.pModel.RawData.states_der_include:\n                                raw_data['states_der'][idx] = states_der\n\n                        # mark future as done\n                        future.release() # especially necessary when simulating ClaRa\n                        futures.remove(future) # remove future from list of futures\n                        _n_finished += 1\n\n                        _str0 = 'Progress: '\n                        _str1 = progress_string(_n_finished / cfg.pModel.RawData.n_samples)\n                        _str2 = ' \\t - \\tfinished {}/{} ({}%)\\t {} ({}%) successful, {} ({}%) failed, {} ({}%) timed out \\t fmu {} took {} sec'.format(\n                            _n_finished, cfg.pModel.RawData.n_samples, round(_n_finished / cfg.pModel.RawData.n_samples * 100, 1),\n                            _n_completed, round(_n_completed / _n_finished * 100, 2),\n                            _n_failed, round(_n_failed / _n_finished * 100, 2),\n                            _n_timedout, round(_n_timedout / _n_finished * 100, 2),\n                            idx, round(res['time'], 3),\n                            )\n                        logging.info(_str0 + _str1 + _str2)\n                        _runtime_per_future.append(res['time'])\n\n                sleep(np.mean(_runtime_per_future)/_n_workers) # sleep for the average runtime of a future to avoid too many requests to the scheduler\n\n        except BaseException as e:\n            logging.error('Error in data generation: {}'.format(e))\n            logging.error(e)\n            logging.error('catching exception to save the data that was generated so far')\n            raise e\n\n    logging.info('multiprocessing time: {}'.format(time() - t0))\n\n    # close raw data file\n    raw_data.close()\n    logging.info('closed raw data file, all data saved. Proceeding errors have no influence on the data.')\n    for future in futures:\n        future.release()\n    client.shutdown()\n    cluster.close()\n</code></pre>"},{"location":"bnode_core/data_generation/raw_data_generation/#bnode_core.data_generation.raw_data_generation.sample_all_values","title":"<code>sample_all_values(cfg: data_gen_config) -&gt; Tuple[Optional[np.ndarray], Optional[np.ndarray], Optional[np.ndarray]]</code>","text":"<p>Sample all input values (initial states, parameters, controls) according to config.</p> <p>Orchestrates sampling for all simulation inputs based on the configured sampling strategies. Returns None for any input category not included in the config. For parameters, if sampling  is disabled, returns default parameter values for all samples.</p> Supported sampling strategies <ul> <li>Initial states: 'R' (random uniform)</li> <li>Controls: 'R', 'RO' (random with offset), 'ROCS', 'RROCS', 'RS' (random steps),    'RF' (frequency response), 'file' (from CSV), 'constantInput' (from Excel)</li> <li>Parameters: 'R' (random uniform)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>data_gen_config</code> <p>Data generation configuration containing:</p> <ul> <li>cfg.pModel.RawData.initial_states_include: whether to sample initial states</li> <li>cfg.pModel.RawData.initial_states_sampling_strategy: 'R' for random uniform</li> <li>cfg.pModel.RawData.controls_include: whether to sample controls</li> <li>cfg.pModel.RawData.controls_sampling_strategy: strategy name (see above)</li> <li>cfg.pModel.RawData.parameters_include: whether to sample parameters</li> <li>cfg.pModel.RawData.parameters_sampling_strategy: 'R' for random uniform</li> <li>cfg.pModel.RawData.parameters: dict with parameter bounds and defaults</li> <li>cfg.pModel.RawData.n_samples: number of samples to generate</li> </ul> required <p>Returns:</p> Type Description <code>Tuple[Optional[ndarray], Optional[ndarray], Optional[ndarray]]</code> <p>Tuple of (initial_state_values, param_values, ctrl_values) where:</p> <ul> <li>initial_state_values: np.ndarray (n_samples, n_states) or None</li> <li>param_values: np.ndarray (n_samples, n_parameters) or None</li> <li>ctrl_values: np.ndarray (n_samples, n_controls, sequence_length) or None</li> </ul> Source code in <code>src/bnode_core/data_generation/raw_data_generation.py</code> <pre><code>def sample_all_values(cfg: data_gen_config) -&gt; Tuple[Optional[np.ndarray], Optional[np.ndarray], Optional[np.ndarray]]:\n    \"\"\"Sample all input values (initial states, parameters, controls) according to config.\n\n    Orchestrates sampling for all simulation inputs based on the configured sampling strategies.\n    Returns None for any input category not included in the config. For parameters, if sampling \n    is disabled, returns default parameter values for all samples.\n\n    Supported sampling strategies:\n        - Initial states: 'R' (random uniform)\n        - Controls: 'R', 'RO' (random with offset), 'ROCS', 'RROCS', 'RS' (random steps), \n          'RF' (frequency response), 'file' (from CSV), 'constantInput' (from Excel)\n        - Parameters: 'R' (random uniform)\n\n    Args:\n        cfg: Data generation configuration containing:\n\n            - cfg.pModel.RawData.initial_states_include: whether to sample initial states\n            - cfg.pModel.RawData.initial_states_sampling_strategy: 'R' for random uniform\n            - cfg.pModel.RawData.controls_include: whether to sample controls\n            - cfg.pModel.RawData.controls_sampling_strategy: strategy name (see above)\n            - cfg.pModel.RawData.parameters_include: whether to sample parameters\n            - cfg.pModel.RawData.parameters_sampling_strategy: 'R' for random uniform\n            - cfg.pModel.RawData.parameters: dict with parameter bounds and defaults\n            - cfg.pModel.RawData.n_samples: number of samples to generate\n\n    Returns:\n        Tuple of (initial_state_values, param_values, ctrl_values) where:\n\n            - initial_state_values: np.ndarray (n_samples, n_states) or None\n            - param_values: np.ndarray (n_samples, n_parameters) or None\n            - ctrl_values: np.ndarray (n_samples, n_controls, sequence_length) or None\n    \"\"\"\n    if cfg.pModel.RawData.initial_states_include:\n        if cfg.pModel.RawData.initial_states_sampling_strategy == 'R':\n            initial_state_values = random_sampling_initial_states(cfg)\n        logging.info('initial_state_values.shape: {}'.format(initial_state_values.shape))\n    else:\n        initial_state_values = None\n        logging.info('No initial state sampling included in raw data generation')\n\n    if cfg.pModel.RawData.controls_include:\n        if cfg.pModel.RawData.controls_sampling_strategy == 'R':\n            ctrl_values = random_sampling_controls(cfg)\n        elif cfg.pModel.RawData.controls_sampling_strategy == 'RO':\n            ctrl_values = random_sampling_controls_w_offset(cfg)\n        elif cfg.pModel.RawData.controls_sampling_strategy == 'ROCS':\n            ctrl_values = random_sampling_controls_w_offset_cubic_splines_old_clip_manual(cfg)\n        elif cfg.pModel.RawData.controls_sampling_strategy == 'RROCS':\n            ctrl_values = random_sampling_controls_w_offset_cubic_splines_clip_random(cfg)\n        elif cfg.pModel.RawData.controls_sampling_strategy == 'RS':\n            ctrl_values = random_steps_sampling_controls(cfg)\n        elif cfg.pModel.RawData.controls_sampling_strategy == 'RF':\n            ctrl_values = random_frequency_response_sampling_controls(cfg)\n        elif cfg.pModel.RawData.controls_sampling_strategy == 'file':\n            ctrl_values = load_controls_from_file(cfg)\n        elif cfg.pModel.RawData.controls_sampling_strategy == 'constantInput':\n            ctrl_values = constant_input_simulation_from_excel(cfg)\n        logging.info('ctrl_values.shape: {}'.format(ctrl_values.shape))\n    else:\n        ctrl_values = None\n        logging.info('No control sampling included in raw data generation')\n\n    if cfg.pModel.RawData.parameters_include:\n        if cfg.pModel.RawData.parameters_sampling_strategy == 'R':\n            param_values = random_sampling_parameters(cfg)\n    else:\n        # save default parameter values\n        if cfg.pModel.RawData.parameters is not None:\n            _param_default = [cfg.pModel.RawData.parameters[key][2] for key in cfg.pModel.RawData.parameters.keys()]\n            param_values = [_param_default for _ in range(cfg.pModel.RawData.n_samples)]\n            param_values = np.array(param_values)\n        else:\n            param_values = None\n        logging.info('No parameter sampling included in raw data generation')\n    if param_values is not None:\n        logging.info('param_values.shape: {}'.format(param_values.shape))\n\n    return initial_state_values, param_values, ctrl_values\n</code></pre>"},{"location":"bnode_core/data_generation/raw_data_generation/#bnode_core.data_generation.raw_data_generation.run_data_generation","title":"<code>run_data_generation(cfg: data_gen_config) -&gt; None</code>","text":"<p>Main orchestration function for raw data generation pipeline.</p> <p>Complete raw data generation workflow:</p> <ol> <li>Convert and validate configuration</li> <li>Set reproducibility seed (np.random.seed(42))</li> <li>Create raw data HDF5 file with pre-allocated datasets</li> <li>Sample all input values (initial states, parameters, controls)</li> <li>Write sampled inputs and metadata to HDF5 file</li> <li>Execute parallel FMU simulations via data_generation()</li> <li>Save configuration as YAML file</li> </ol> <p>The function prompts for confirmation before overwriting existing raw data files  (unless cfg.overwrite is True). It creates the complete HDF5 structure including:</p> <ul> <li>Time vector and sampled inputs (initial_states, parameters, controls)</li> <li>Pre-allocated arrays for simulation outputs (states, states_der, outputs)</li> <li>Metadata attributes (creation_date, config YAML)</li> <li>Log datasets for tracking simulation status</li> </ul> <p>This is the Hydra-decorated entry point called by main().</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>data_gen_config</code> <p>Data generation configuration (automatically populated by Hydra from YAML + CLI args). Key settings include:</p> <ul> <li>pModel.RawData: all generation parameters (FMU path, bounds, solver, sampling strategies)</li> <li>overwrite: if True, skip confirmation prompt for existing files</li> <li>multiprocessing_processes: number of parallel workers</li> <li>memory_limit_per_worker: memory limit per Dask worker</li> </ul> required Notes <ul> <li>Sets np.random.seed(42) for reproducibility (added 2024-11-23).</li> <li>Raw data HDF5 file path determined by filepath_raw_data(cfg).</li> <li>Config YAML path determined by filepath_raw_data_config(cfg).</li> <li>The HDF5 file config attribute stores OmegaConf.to_yaml(cfg.pModel.RawData).</li> <li>Creation date is recorded both in HDF5 attrs and in the config YAML.</li> </ul> Source code in <code>src/bnode_core/data_generation/raw_data_generation.py</code> <pre><code>def run_data_generation(cfg: data_gen_config) -&gt; None:\n    \"\"\"Main orchestration function for raw data generation pipeline.\n\n    Complete raw data generation workflow:\n\n    1. Convert and validate configuration\n    2. Set reproducibility seed (np.random.seed(42))\n    3. Create raw data HDF5 file with pre-allocated datasets\n    4. Sample all input values (initial states, parameters, controls)\n    5. Write sampled inputs and metadata to HDF5 file\n    6. Execute parallel FMU simulations via data_generation()\n    7. Save configuration as YAML file\n\n    The function prompts for confirmation before overwriting existing raw data files \n    (unless cfg.overwrite is True). It creates the complete HDF5 structure including:\n\n    - Time vector and sampled inputs (initial_states, parameters, controls)\n    - Pre-allocated arrays for simulation outputs (states, states_der, outputs)\n    - Metadata attributes (creation_date, config YAML)\n    - Log datasets for tracking simulation status\n\n    This is the Hydra-decorated entry point called by main().\n\n    Args:\n        cfg: Data generation configuration (automatically populated by Hydra from YAML + CLI args).\n            Key settings include:\n\n            - pModel.RawData: all generation parameters (FMU path, bounds, solver, sampling strategies)\n            - overwrite: if True, skip confirmation prompt for existing files\n            - multiprocessing_processes: number of parallel workers\n            - memory_limit_per_worker: memory limit per Dask worker\n\n    Notes:\n        - Sets np.random.seed(42) for reproducibility (added 2024-11-23).\n        - Raw data HDF5 file path determined by filepath_raw_data(cfg).\n        - Config YAML path determined by filepath_raw_data_config(cfg).\n        - The HDF5 file config attribute stores OmegaConf.to_yaml(cfg.pModel.RawData).\n        - Creation date is recorded both in HDF5 attrs and in the config YAML.\n    \"\"\"\n    cfg = convert_cfg_to_dataclass(cfg)\n\n    # added np.seed for reproducibility on 23.11.2024 (databases generated before this date are not exactly reproducible)\n    np.random.seed(42)\n\n    # create hdf5 file for raw data\n    if os.path.exists(filepath_raw_data(cfg)) and cfg.overwrite is False:\n        response = input(f\"File {filepath_raw_data(cfg)} already exists. Overwrite? (y/n): \").strip().lower()\n        if response != 'y':\n            print(\"Aborting data generation.\")\n            sys.exit(0)\n    log_overwriting_file(filepath_raw_data(cfg))\n    raw_data = h5py.File(filepath_raw_data(cfg), 'w')\n\n    # sample initial states, parameters and controls with given sampling strategy\n    initial_state_values, param_values, ctrl_values = sample_all_values(cfg)\n\n    if initial_state_values is not None:\n        raw_data.create_dataset('initial_states', data=initial_state_values)\n\n    if param_values is not None:\n        raw_data.create_dataset('parameters', data=param_values)\n        raw_data.create_dataset('parameters_names', data=np.array(list(cfg.pModel.RawData.parameters.keys()), dtype='S'))\n\n    if ctrl_values is not None and cfg.pModel.RawData.controls_only_for_sampling_extract_actual_from_model is False:\n        raw_data.create_dataset('controls', data=ctrl_values)\n        raw_data.create_dataset('controls_names', data=np.array(list(cfg.pModel.RawData.controls.keys()), dtype='S'))\n\n    # generate time vector\n    time = np.arange(cfg.pModel.RawData.Solver.simulationStartTime, cfg.pModel.RawData.Solver.simulationEndTime + cfg.pModel.RawData.Solver.timestep, cfg.pModel.RawData.Solver.timestep)\n\n    # allocate memory in hdf5 file for raw data\n    raw_data.create_dataset('time', data=time)\n    if cfg.pModel.RawData.outputs is not None:\n        raw_data.create_dataset('outputs', (cfg.pModel.RawData.n_samples, len(cfg.pModel.RawData.outputs), len(time)))\n        raw_data.create_dataset('outputs_names', data=np.array(list(cfg.pModel.RawData.outputs), dtype='S'))\n    if cfg.pModel.RawData.controls_only_for_sampling_extract_actual_from_model is True:\n        raw_data.create_dataset('controls', (cfg.pModel.RawData.n_samples, len(cfg.pModel.RawData.controls_from_model), len(time)))\n        raw_data.create_dataset('controls_names', data=np.array(list(cfg.pModel.RawData.controls_from_model), dtype='S'))\n    raw_data.create_dataset('states', (cfg.pModel.RawData.n_samples, len(cfg.pModel.RawData.states), len(time)))\n    raw_data.create_dataset('states_names', data=np.array(list(cfg.pModel.RawData.states.keys()), dtype='S'))\n    if cfg.pModel.RawData.states_der_include:\n        raw_data.create_dataset('states_der', (cfg.pModel.RawData.n_samples, len(cfg.pModel.RawData.states), len(time)))\n        raw_data.create_dataset('states_der_names', data=np.array(list('der({})'.format(key) for key in cfg.pModel.RawData.states.keys()), dtype='S'))\n\n    # add creation date (YYYY-MM-DD HH:MM:SS)\n    creation_date = datetime.now()\n    raw_data.attrs['creation_date'] = str(creation_date)\n    cfg.pModel.RawData.creation_date = str(creation_date)\n    logging.info('added creation date: {} to hdf5-file and config.yaml'.format(creation_date))\n\n    # add config fields to hdf5 file\n    raw_data.attrs['config'] = OmegaConf.to_yaml(cfg.pModel.RawData)\n    # close hdf5 file\n    raw_data.close()\n\n    # generate raw data and save it to hdf5 file\n    data_generation(cfg, initial_state_values, param_values, ctrl_values)\n\n    # save pModel config as yaml\n    log_overwriting_file(filepath_raw_data_config(cfg))\n    OmegaConf.save(cfg.pModel.RawData, filepath_raw_data_config(cfg))\n</code></pre>"},{"location":"bnode_core/data_generation/raw_data_generation/#bnode_core.data_generation.raw_data_generation.main","title":"<code>main()</code>","text":"<p>CLI entry point for raw data generation.</p> <p>Sets up Hydra configuration management and launches run_data_generation(). </p> <p>Hydra automatically:</p> <ul> <li>Loads the data_generation.yaml config from the auto-detected config directory</li> <li>Parses command-line overrides</li> <li>Creates a working directory for outputs</li> <li>Injects the composed config into run_data_generation()</li> </ul> Usage <p>python raw_data_generation.py [overrides]</p> <p>Examples:</p> <pre><code>python raw_data_generation.py pModel.RawData.n_samples=1000\npython raw_data_generation.py pModel=SHF overwrite=true\n</code></pre> Source code in <code>src/bnode_core/data_generation/raw_data_generation.py</code> <pre><code>def main():\n    \"\"\"CLI entry point for raw data generation.\n\n    Sets up Hydra configuration management and launches run_data_generation(). \n\n    Hydra automatically:\n\n    - Loads the data_generation.yaml config from the auto-detected config directory\n    - Parses command-line overrides\n    - Creates a working directory for outputs\n    - Injects the composed config into run_data_generation()\n\n    Usage:\n        python raw_data_generation.py [overrides]\n\n    Examples:\n\n        python raw_data_generation.py pModel.RawData.n_samples=1000\n        python raw_data_generation.py pModel=SHF overwrite=true\n    \"\"\"\n    cs = get_config_store()\n    config_dir = config_dir_auto_recognize()\n    config_name = 'data_generation'\n    hydra.main(config_path=str(config_dir.absolute()), config_name=config_name, version_base=None)(run_data_generation)()\n</code></pre>"},{"location":"bnode_core/nn/nn_utils/","title":"Neural Network Utils","text":""},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.normalization","title":"<code>bnode_core.nn.nn_utils.normalization</code>","text":"<p>Normalization layers for neural network inputs with time series and 1D data support.</p> <p>This module provides PyTorch normalization layers that compute and store mean/std statistics from data, then normalize (or denormalize) inputs during forward passes. Supports both time series data (batch, channels, time) and 1D feature vectors (batch, features).</p>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.normalization.NormalizationLayerTimeSeries","title":"<code>NormalizationLayerTimeSeries</code>","text":"<p>               Bases: <code>Module</code></p> <p>Normalization layer for time series data with shape (batch, channels, time).</p> <p>Computes and stores per-channel mean and standard deviation from input data, then normalizes future inputs to zero mean and unit variance. Can also denormalize outputs back to original scale. Statistics are computed once during first forward pass or via explicit initialization.</p> <p>Expected input shape: (batch_size, n_channels, sequence_length)</p> <p>Attributes:</p> Name Type Description <code>_initialized</code> <code>bool</code> <p>Whether mean/std have been computed from data.</p> <code>std</code> <code>Tensor</code> <p>Per-channel standard deviations, shape (n_channels,).</p> <code>mu</code> <code>Tensor</code> <p>Per-channel means, shape (n_channels,).</p> Source code in <code>src/bnode_core/nn/nn_utils/normalization.py</code> <pre><code>class NormalizationLayerTimeSeries(nn.Module):\n    \"\"\"Normalization layer for time series data with shape (batch, channels, time).\n\n    Computes and stores per-channel mean and standard deviation from input data, then\n    normalizes future inputs to zero mean and unit variance. Can also denormalize outputs\n    back to original scale. Statistics are computed once during first forward pass or via\n    explicit initialization.\n\n    Expected input shape: (batch_size, n_channels, sequence_length)\n\n    Attributes:\n        _initialized (bool): Whether mean/std have been computed from data.\n        std (torch.Tensor): Per-channel standard deviations, shape (n_channels,).\n        mu (torch.Tensor): Per-channel means, shape (n_channels,).\n    \"\"\"\n    def __init__(self, n_channels):\n        \"\"\"Initialize normalization layer buffers.\n\n        Args:\n            n_channels (int): Number of channels in time series data.\n        \"\"\"\n        super().__init__()\n        self.register_buffer(\"_initialized\", torch.tensor(False))\n        self.register_buffer('std', torch.zeros(n_channels))\n        self.register_buffer('mu', torch.zeros(n_channels))\n\n    def initialize_normalization(self,x):\n        \"\"\"Compute and store mean and std from input data.\n\n        Calculates per-channel statistics across batch and time dimensions. Adds small\n        epsilon (1e-3) to variance for numerical stability. Only runs if not already\n        initialized.\n\n        Args:\n            x (torch.Tensor): Input data with shape (batch_size, n_channels, sequence_length).\n\n        Side Effects:\n            Sets self.mu and self.std buffers if not already initialized.\n        \"\"\"\n        if not self._initialized:\n            variance = torch.var(x, dim=(0,2)).detach()\n            self.std.set_(torch.sqrt(variance + torch.ones(variance.size()).to(variance.device) * 1e-3))\n            self.mu.set_(torch.mean(x, dim=(0,2)).detach())\n            self._initialized = torch.tensor(True)\n            assert self.std.requires_grad == False\n            assert self.mu.requires_grad == False\n\n    def forward(self, x: torch.Tensor, denormalize: bool = False) -&gt; torch.Tensor:\n        \"\"\"Normalize or denormalize input time series.\n\n        If not initialized and normalizing, automatically initializes from input data.\n        Normalizes via (x - mu) / std or denormalizes via x * std + mu.\n\n        Args:\n            x (torch.Tensor): Input with shape (batch_size, n_channels, sequence_length).\n            denormalize (bool, optional): If False, normalize input. If True, denormalize\n                (reverse transformation). Defaults to False.\n\n        Returns:\n            torch.Tensor: Normalized or denormalized data with same shape as input.\n        \"\"\"\n        if denormalize is False:\n            if not self._initialized:\n                self.initialize_normalization(x)\n        batch_size = x.shape[0]\n        seq_len = x.shape[2]\n        # add dimensions at position 0 (for number of batches) and at position 2 (for sequence length)\n        # expand these dimensions\n        std = self.std.unsqueeze(0).unsqueeze(2).expand(batch_size,-1,seq_len)\n        mu = self.mu.unsqueeze(0).unsqueeze(2).expand(batch_size,-1,seq_len)\n        if denormalize is False:\n            x = torch.subtract(x,mu)\n            x = torch.divide(x, std)\n        else:\n            x = torch.multiply(x, std)\n            x = torch.add(x, mu)\n        return x\n</code></pre>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.normalization.NormalizationLayerTimeSeries.__init__","title":"<code>__init__(n_channels)</code>","text":"<p>Initialize normalization layer buffers.</p> <p>Parameters:</p> Name Type Description Default <code>n_channels</code> <code>int</code> <p>Number of channels in time series data.</p> required Source code in <code>src/bnode_core/nn/nn_utils/normalization.py</code> <pre><code>def __init__(self, n_channels):\n    \"\"\"Initialize normalization layer buffers.\n\n    Args:\n        n_channels (int): Number of channels in time series data.\n    \"\"\"\n    super().__init__()\n    self.register_buffer(\"_initialized\", torch.tensor(False))\n    self.register_buffer('std', torch.zeros(n_channels))\n    self.register_buffer('mu', torch.zeros(n_channels))\n</code></pre>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.normalization.NormalizationLayerTimeSeries.initialize_normalization","title":"<code>initialize_normalization(x)</code>","text":"<p>Compute and store mean and std from input data.</p> <p>Calculates per-channel statistics across batch and time dimensions. Adds small epsilon (1e-3) to variance for numerical stability. Only runs if not already initialized.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input data with shape (batch_size, n_channels, sequence_length).</p> required Side Effects <p>Sets self.mu and self.std buffers if not already initialized.</p> Source code in <code>src/bnode_core/nn/nn_utils/normalization.py</code> <pre><code>def initialize_normalization(self,x):\n    \"\"\"Compute and store mean and std from input data.\n\n    Calculates per-channel statistics across batch and time dimensions. Adds small\n    epsilon (1e-3) to variance for numerical stability. Only runs if not already\n    initialized.\n\n    Args:\n        x (torch.Tensor): Input data with shape (batch_size, n_channels, sequence_length).\n\n    Side Effects:\n        Sets self.mu and self.std buffers if not already initialized.\n    \"\"\"\n    if not self._initialized:\n        variance = torch.var(x, dim=(0,2)).detach()\n        self.std.set_(torch.sqrt(variance + torch.ones(variance.size()).to(variance.device) * 1e-3))\n        self.mu.set_(torch.mean(x, dim=(0,2)).detach())\n        self._initialized = torch.tensor(True)\n        assert self.std.requires_grad == False\n        assert self.mu.requires_grad == False\n</code></pre>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.normalization.NormalizationLayerTimeSeries.forward","title":"<code>forward(x: torch.Tensor, denormalize: bool = False) -&gt; torch.Tensor</code>","text":"<p>Normalize or denormalize input time series.</p> <p>If not initialized and normalizing, automatically initializes from input data. Normalizes via (x - mu) / std or denormalizes via x * std + mu.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input with shape (batch_size, n_channels, sequence_length).</p> required <code>denormalize</code> <code>bool</code> <p>If False, normalize input. If True, denormalize (reverse transformation). Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Normalized or denormalized data with same shape as input.</p> Source code in <code>src/bnode_core/nn/nn_utils/normalization.py</code> <pre><code>def forward(self, x: torch.Tensor, denormalize: bool = False) -&gt; torch.Tensor:\n    \"\"\"Normalize or denormalize input time series.\n\n    If not initialized and normalizing, automatically initializes from input data.\n    Normalizes via (x - mu) / std or denormalizes via x * std + mu.\n\n    Args:\n        x (torch.Tensor): Input with shape (batch_size, n_channels, sequence_length).\n        denormalize (bool, optional): If False, normalize input. If True, denormalize\n            (reverse transformation). Defaults to False.\n\n    Returns:\n        torch.Tensor: Normalized or denormalized data with same shape as input.\n    \"\"\"\n    if denormalize is False:\n        if not self._initialized:\n            self.initialize_normalization(x)\n    batch_size = x.shape[0]\n    seq_len = x.shape[2]\n    # add dimensions at position 0 (for number of batches) and at position 2 (for sequence length)\n    # expand these dimensions\n    std = self.std.unsqueeze(0).unsqueeze(2).expand(batch_size,-1,seq_len)\n    mu = self.mu.unsqueeze(0).unsqueeze(2).expand(batch_size,-1,seq_len)\n    if denormalize is False:\n        x = torch.subtract(x,mu)\n        x = torch.divide(x, std)\n    else:\n        x = torch.multiply(x, std)\n        x = torch.add(x, mu)\n    return x\n</code></pre>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.normalization.NormalizationLayer1D","title":"<code>NormalizationLayer1D</code>","text":"<p>               Bases: <code>Module</code></p> <p>Normalization layer for 1D feature vectors with shape (batch, features).</p> <p>Computes and stores per-feature mean and standard deviation, then normalizes inputs to zero mean and unit variance. Can also denormalize outputs. Supports both 2D (batch, features) and 3D (batch, features, time) inputs. Accepts both torch.Tensor and numpy.ndarray for initialization.</p> <p>Expected input shape: (batch_size, num_features) or (batch_size, num_features, sequence_length)</p> <p>Attributes:</p> Name Type Description <code>_initialized</code> <code>bool</code> <p>Whether mean/std have been computed.</p> <code>std</code> <code>Tensor</code> <p>Per-feature standard deviations, shape (num_features,).</p> <code>mu</code> <code>Tensor</code> <p>Per-feature means, shape (num_features,).</p> Source code in <code>src/bnode_core/nn/nn_utils/normalization.py</code> <pre><code>class NormalizationLayer1D(nn.Module):\n    \"\"\"Normalization layer for 1D feature vectors with shape (batch, features).\n\n    Computes and stores per-feature mean and standard deviation, then normalizes inputs\n    to zero mean and unit variance. Can also denormalize outputs. Supports both 2D\n    (batch, features) and 3D (batch, features, time) inputs. Accepts both torch.Tensor\n    and numpy.ndarray for initialization.\n\n    Expected input shape: (batch_size, num_features) or (batch_size, num_features, sequence_length)\n\n    Attributes:\n        _initialized (bool): Whether mean/std have been computed.\n        std (torch.Tensor): Per-feature standard deviations, shape (num_features,).\n        mu (torch.Tensor): Per-feature means, shape (num_features,).\n    \"\"\"\n    def __init__(self, num_features):\n        \"\"\"Initialize normalization layer buffers.\n\n        Args:\n            num_features (int): Number of features/channels to normalize.\n        \"\"\"\n        super().__init__()\n        self.register_buffer(\"_initialized\", torch.tensor(False))\n        self.register_buffer('std', torch.zeros((num_features)))\n        self.register_buffer('mu', torch.zeros(num_features))\n\n    def initialize_normalization(self, x, eps = 1e-5, verbose = False, name = None):\n        \"\"\"Compute and store mean and std from input data.\n\n        Calculates per-feature statistics across batch dimension. Adds epsilon to variance\n        for numerical stability. Supports both torch.Tensor and numpy.ndarray inputs.\n\n        Args:\n            x (torch.Tensor or np.ndarray): Input data with shape (batch_size, num_features).\n            eps (float, optional): Small constant added to variance for stability. Defaults to 1e-5.\n            verbose (bool, optional): If True, logs initialization info. Defaults to False.\n            name (str, optional): Name for logging output. Defaults to None.\n\n        Raises:\n            ValueError: If x is neither torch.Tensor nor np.ndarray.\n            RuntimeError: If normalization layer has already been initialized.\n\n        Side Effects:\n            Sets self.mu and self.std buffers, logs initialization if verbose=True.\n        \"\"\"\n        if not self._initialized:\n            if isinstance(x, torch.Tensor):\n                variance = torch.var(x, dim=(0)).detach()\n                self.std.set_(torch.sqrt(variance + torch.ones(variance.size()).to(variance.device) * eps))\n                self.mu.set_(torch.mean(x, dim=(0)).detach())\n            elif isinstance(x, np.ndarray):\n                variance = np.var(x, axis=0)\n                self.std.set_(torch.sqrt(torch.tensor(variance + np.ones(variance.shape) * eps, dtype=torch.float32)))\n                self.mu.set_(torch.tensor(np.mean(x, axis=0), dtype=torch.float32))\n            else:\n                raise ValueError('Unknown type of input: {}'.format(type(x)))\n            self._initialized = torch.tensor(True)\n            assert self.std.requires_grad == False\n            assert self.mu.requires_grad == False\n\n            logging.info(\"Initialized normalization layer {} with mean {} and std {}\".format(name, self.mu, self.std))\n        else:\n            raise RuntimeError(\"normalization layer has already been initialized\")\n\n    def forward(self, x: torch.Tensor, denormalize: bool = False) -&gt; torch.Tensor:\n        \"\"\"Normalize or denormalize input features.\n\n        If not initialized and normalizing, automatically initializes from input. Handles\n        both 2D (batch, features) and 3D (batch, features, time) inputs by broadcasting.\n        Normalizes via (x - mu) / std or denormalizes via x * std + mu.\n\n        Args:\n            x (torch.Tensor): Input with shape (batch_size, num_features) or \n                (batch_size, num_features, sequence_length).\n            denormalize (bool, optional): If False, normalize input. If True, denormalize.\n                Defaults to False.\n\n        Returns:\n            torch.Tensor: Normalized or denormalized data with same shape as input.\n        \"\"\"\n        if not denormalize:\n            if not self._initialized:\n                self.initialize_normalization(x)\n        batch_size = x.shape[0]\n        # add dimension at position 0 and expand to batch_size\n        std = self.std.unsqueeze(0).expand(batch_size,-1)\n        mu = self.mu.unsqueeze(0).expand(batch_size,-1)\n        if len(x.shape) == 3:\n            # if x is a 3D tensor, we assume it has shape (batch_size, num_features, sequence_length)\n            seq_len = x.shape[2]\n            std = std.unsqueeze(2).expand(batch_size,-1,seq_len)\n            mu = mu.unsqueeze(2).expand(batch_size,-1,seq_len)\n        if not denormalize:\n            x = torch.subtract(x, mu)\n            x = torch.divide(x, std)\n        else:\n            x = torch.multiply(x, std)\n            x = torch.add(x, mu)\n        return x\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return string representation of the layer.\n\n        Returns:\n            str: String showing layer type and number of features.\n        \"\"\"\n        return 'NormalizationLayer1D(num_features={})'.format(self.std.shape[0])\n</code></pre>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.normalization.NormalizationLayer1D.__init__","title":"<code>__init__(num_features)</code>","text":"<p>Initialize normalization layer buffers.</p> <p>Parameters:</p> Name Type Description Default <code>num_features</code> <code>int</code> <p>Number of features/channels to normalize.</p> required Source code in <code>src/bnode_core/nn/nn_utils/normalization.py</code> <pre><code>def __init__(self, num_features):\n    \"\"\"Initialize normalization layer buffers.\n\n    Args:\n        num_features (int): Number of features/channels to normalize.\n    \"\"\"\n    super().__init__()\n    self.register_buffer(\"_initialized\", torch.tensor(False))\n    self.register_buffer('std', torch.zeros((num_features)))\n    self.register_buffer('mu', torch.zeros(num_features))\n</code></pre>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.normalization.NormalizationLayer1D.initialize_normalization","title":"<code>initialize_normalization(x, eps=1e-05, verbose=False, name=None)</code>","text":"<p>Compute and store mean and std from input data.</p> <p>Calculates per-feature statistics across batch dimension. Adds epsilon to variance for numerical stability. Supports both torch.Tensor and numpy.ndarray inputs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor or ndarray</code> <p>Input data with shape (batch_size, num_features).</p> required <code>eps</code> <code>float</code> <p>Small constant added to variance for stability. Defaults to 1e-5.</p> <code>1e-05</code> <code>verbose</code> <code>bool</code> <p>If True, logs initialization info. Defaults to False.</p> <code>False</code> <code>name</code> <code>str</code> <p>Name for logging output. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If x is neither torch.Tensor nor np.ndarray.</p> <code>RuntimeError</code> <p>If normalization layer has already been initialized.</p> Side Effects <p>Sets self.mu and self.std buffers, logs initialization if verbose=True.</p> Source code in <code>src/bnode_core/nn/nn_utils/normalization.py</code> <pre><code>def initialize_normalization(self, x, eps = 1e-5, verbose = False, name = None):\n    \"\"\"Compute and store mean and std from input data.\n\n    Calculates per-feature statistics across batch dimension. Adds epsilon to variance\n    for numerical stability. Supports both torch.Tensor and numpy.ndarray inputs.\n\n    Args:\n        x (torch.Tensor or np.ndarray): Input data with shape (batch_size, num_features).\n        eps (float, optional): Small constant added to variance for stability. Defaults to 1e-5.\n        verbose (bool, optional): If True, logs initialization info. Defaults to False.\n        name (str, optional): Name for logging output. Defaults to None.\n\n    Raises:\n        ValueError: If x is neither torch.Tensor nor np.ndarray.\n        RuntimeError: If normalization layer has already been initialized.\n\n    Side Effects:\n        Sets self.mu and self.std buffers, logs initialization if verbose=True.\n    \"\"\"\n    if not self._initialized:\n        if isinstance(x, torch.Tensor):\n            variance = torch.var(x, dim=(0)).detach()\n            self.std.set_(torch.sqrt(variance + torch.ones(variance.size()).to(variance.device) * eps))\n            self.mu.set_(torch.mean(x, dim=(0)).detach())\n        elif isinstance(x, np.ndarray):\n            variance = np.var(x, axis=0)\n            self.std.set_(torch.sqrt(torch.tensor(variance + np.ones(variance.shape) * eps, dtype=torch.float32)))\n            self.mu.set_(torch.tensor(np.mean(x, axis=0), dtype=torch.float32))\n        else:\n            raise ValueError('Unknown type of input: {}'.format(type(x)))\n        self._initialized = torch.tensor(True)\n        assert self.std.requires_grad == False\n        assert self.mu.requires_grad == False\n\n        logging.info(\"Initialized normalization layer {} with mean {} and std {}\".format(name, self.mu, self.std))\n    else:\n        raise RuntimeError(\"normalization layer has already been initialized\")\n</code></pre>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.normalization.NormalizationLayer1D.forward","title":"<code>forward(x: torch.Tensor, denormalize: bool = False) -&gt; torch.Tensor</code>","text":"<p>Normalize or denormalize input features.</p> <p>If not initialized and normalizing, automatically initializes from input. Handles both 2D (batch, features) and 3D (batch, features, time) inputs by broadcasting. Normalizes via (x - mu) / std or denormalizes via x * std + mu.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input with shape (batch_size, num_features) or  (batch_size, num_features, sequence_length).</p> required <code>denormalize</code> <code>bool</code> <p>If False, normalize input. If True, denormalize. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Normalized or denormalized data with same shape as input.</p> Source code in <code>src/bnode_core/nn/nn_utils/normalization.py</code> <pre><code>def forward(self, x: torch.Tensor, denormalize: bool = False) -&gt; torch.Tensor:\n    \"\"\"Normalize or denormalize input features.\n\n    If not initialized and normalizing, automatically initializes from input. Handles\n    both 2D (batch, features) and 3D (batch, features, time) inputs by broadcasting.\n    Normalizes via (x - mu) / std or denormalizes via x * std + mu.\n\n    Args:\n        x (torch.Tensor): Input with shape (batch_size, num_features) or \n            (batch_size, num_features, sequence_length).\n        denormalize (bool, optional): If False, normalize input. If True, denormalize.\n            Defaults to False.\n\n    Returns:\n        torch.Tensor: Normalized or denormalized data with same shape as input.\n    \"\"\"\n    if not denormalize:\n        if not self._initialized:\n            self.initialize_normalization(x)\n    batch_size = x.shape[0]\n    # add dimension at position 0 and expand to batch_size\n    std = self.std.unsqueeze(0).expand(batch_size,-1)\n    mu = self.mu.unsqueeze(0).expand(batch_size,-1)\n    if len(x.shape) == 3:\n        # if x is a 3D tensor, we assume it has shape (batch_size, num_features, sequence_length)\n        seq_len = x.shape[2]\n        std = std.unsqueeze(2).expand(batch_size,-1,seq_len)\n        mu = mu.unsqueeze(2).expand(batch_size,-1,seq_len)\n    if not denormalize:\n        x = torch.subtract(x, mu)\n        x = torch.divide(x, std)\n    else:\n        x = torch.multiply(x, std)\n        x = torch.add(x, mu)\n    return x\n</code></pre>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.normalization.NormalizationLayer1D.__repr__","title":"<code>__repr__() -&gt; str</code>","text":"<p>Return string representation of the layer.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>String showing layer type and number of features.</p> Source code in <code>src/bnode_core/nn/nn_utils/normalization.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return string representation of the layer.\n\n    Returns:\n        str: String showing layer type and number of features.\n    \"\"\"\n    return 'NormalizationLayer1D(num_features={})'.format(self.std.shape[0])\n</code></pre>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.load_data","title":"<code>bnode_core.nn.nn_utils.load_data</code>","text":"<p>Dataset loading utilities for neural network training.</p> <p>Provides functions to load HDF5 datasets and their configurations, and create PyTorch-compatible dataset objects for training.</p> Attention <p>This documentation is generated by AI. Please be aware of possible inaccurcies.</p>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.load_data.TimeSeriesDataset","title":"<code>TimeSeriesDataset</code>","text":"<p>               Bases: <code>StackDataset</code></p> <p>Dataset for time series with sliding window sampling of variable-length subsequences.</p> <p>Extends StackDataset to enable extracting subsequences from longer time series via a sliding window approach. The full sequences are stored internally, but getitem returns only a subsequence of specified length. This enables training on different sequence lengths without reloading data, and increases effective dataset size by treating each sliding window position as a separate sample.</p> <p>The dataset expects dict-style data with a 'time' key, where all time series have shape (n_samples, n_channels, n_timesteps). Non-time-series data (2D) is replicated across all windows from the same sample.</p> <p>Attributes:</p> Name Type Description <code>seq_len</code> <code>int</code> <p>Length of subsequences returned by getitem.</p> <code>mapping</code> <code>list</code> <p>List of [sample_idx, start_pos, end_pos] tuples defining each sliding window position across all samples.</p> <code>_length</code> <code>int</code> <p>Total number of sliding windows (dataset length).</p> <code>_length_old</code> <code>int</code> <p>Original number of samples before windowing.</p> Source code in <code>src/bnode_core/nn/nn_utils/load_data.py</code> <pre><code>class TimeSeriesDataset(torch.utils.data.StackDataset):\n    \"\"\"Dataset for time series with sliding window sampling of variable-length subsequences.\n\n    Extends StackDataset to enable extracting subsequences from longer time series via a\n    sliding window approach. The full sequences are stored internally, but __getitem__\n    returns only a subsequence of specified length. This enables training on different\n    sequence lengths without reloading data, and increases effective dataset size by\n    treating each sliding window position as a separate sample.\n\n    The dataset expects dict-style data with a 'time' key, where all time series have shape\n    (n_samples, n_channels, n_timesteps). Non-time-series data (2D) is replicated across\n    all windows from the same sample.\n\n    Attributes:\n        seq_len (int): Length of subsequences returned by __getitem__.\n        mapping (list): List of [sample_idx, start_pos, end_pos] tuples defining each\n            sliding window position across all samples.\n        _length (int): Total number of sliding windows (dataset length).\n        _length_old (int): Original number of samples before windowing.\n    \"\"\"\n    def __init__(self, seq_len: int, *args, **kwargs):\n        \"\"\"Initialize TimeSeriesDataset with sliding window parameters.\n\n        Args:\n            seq_len (int): Length of subsequences to extract. If larger than available\n                time series length, will be clamped to maximum available length.\n            *args: Positional arguments passed to parent StackDataset.\n            **kwargs: Keyword arguments passed to parent StackDataset. Must result in\n                a dict-style dataset with a 'time' key.\n\n        Raises:\n            AssertionError: If datasets is not a dict or lacks 'time' key.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        assert isinstance(self.datasets, dict), \"can only handle dict style stacked datasets with one key-value pair time\"\n        assert 'time' in self.datasets.keys(), \"need one dataset with key time to define the map\" \n        self._length_old = self._length\n        if seq_len &gt; self.datasets['time'].shape[2]:\n            Warning(\"seq_len is {}, setting to len of timeseries\".format(seq_len))\n            seq_len = self.datasets['time'].shape[2]\n        self.seq_len = seq_len\n        self.initialize_map(seq_len)\n\n    def set_seq_len(self, seq_len: int):\n        \"\"\"Change the subsequence length and rebuild the sliding window mapping.\n\n        Args:\n            seq_len (int): New subsequence length. If None, 0, or larger than available\n                time series length, will be clamped to maximum available length.\n        \"\"\"\n        if seq_len == None or seq_len == 0 or seq_len &gt; self.datasets['time'].shape[2]:\n            Warning(\"seq_len is {}, setting to len of timeseries\".format(seq_len))\n            seq_len = self.datasets['time'].shape[2]\n        self.seq_len = seq_len\n        self.initialize_map(seq_len)\n\n    def initialize_map(self, seq_len: int):\n        \"\"\"Create the sliding window index mapping for all samples.\n\n        Builds a mapping list where each entry [sample_idx, start_pos, end_pos] defines\n        a sliding window position. Windows slide by 1 timestep across each sample, then\n        continue to the next sample. This treats each window position as an independent\n        dataset item.\n\n        Args:\n            seq_len (int): Length of sliding windows. Must be at least 1.\n\n        Raises:\n            AssertionError: If seq_len &lt; 1.\n\n        Side Effects:\n            - Sets self.mapping to list of [sample, start, end] tuples\n            - Updates self._length to total number of windows across all samples\n        \"\"\"\n        assert seq_len &gt; 0, \"seq_len must be at least 1\"\n        # define map\n        n_batches_per_sample = (self.datasets['time'].shape[2] - (seq_len - 1))\n        n_batches_total = n_batches_per_sample * self._length_old\n        self.mapping = [[] for i in range(n_batches_total)]\n        self._length = n_batches_total\n        # fill out map. mapping shall contain [n_sample, start_position, stop_position]\n        k_stop=seq_len # stop position in sequence\n        j=0 # sample position in datasets\n        for i in range(n_batches_total):\n            self.mapping[i] = [j, k_stop-seq_len, k_stop]\n            if k_stop + 2 &gt; n_batches_per_sample + seq_len: # if the over next sequence would be out of bounds, go to next sample (+1 more because of &gt; and not &gt;=)\n                k_stop = seq_len\n                j += 1\n            else:\n                k_stop += 1\n        #self.mapping = torch.tensor(np.array(self.mapping), dtype=torch.int64)\n\n\n    def __getitem__(self, index: int) -&gt; dict:\n        \"\"\"Get a single sliding window sample.\n\n        Args:\n            index (int): Index of the sliding window to retrieve.\n\n        Returns:\n            dict: Dictionary with same keys as self.datasets. For 3D+ arrays (time series),\n                returns subsequence [start:end] from appropriate sample. For 2D arrays,\n                returns full array for the sample.\n        \"\"\"\n        i, k_start, k_stop = self.mapping[index]\n        ret_val = {}\n        for key, value in self.datasets.items():\n            if value.ndim == 2:\n                ret_val[key] = value[i, :]\n            else:\n                ret_val[key] = value[i, :, k_start:k_stop]\n        return ret_val\n\n    def __getitems__(self, indices: list) -&gt; list:\n        \"\"\"Get multiple sliding window samples (batch retrieval).\n\n        Args:\n            indices (list): List of window indices to retrieve.\n\n        Returns:\n            list: List of dictionaries, one per index, each containing the requested\n                sliding window data.\n\n        Note:\n            This method requires PyTorch &gt;= 2.2 for optimal batched data loading.\n        \"\"\"\n        samples = [None] * len(indices)\n        for i, index in enumerate(indices):\n            samples[i] = self.__getitem__(index)\n        return samples\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the total number of sliding windows in the dataset.\n\n        Returns:\n            int: Total number of windows across all samples.\n        \"\"\"\n        return self._length\n</code></pre>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.load_data.TimeSeriesDataset.__init__","title":"<code>__init__(seq_len: int, *args, **kwargs)</code>","text":"<p>Initialize TimeSeriesDataset with sliding window parameters.</p> <p>Parameters:</p> Name Type Description Default <code>seq_len</code> <code>int</code> <p>Length of subsequences to extract. If larger than available time series length, will be clamped to maximum available length.</p> required <code>*args</code> <p>Positional arguments passed to parent StackDataset.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments passed to parent StackDataset. Must result in a dict-style dataset with a 'time' key.</p> <code>{}</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If datasets is not a dict or lacks 'time' key.</p> Source code in <code>src/bnode_core/nn/nn_utils/load_data.py</code> <pre><code>def __init__(self, seq_len: int, *args, **kwargs):\n    \"\"\"Initialize TimeSeriesDataset with sliding window parameters.\n\n    Args:\n        seq_len (int): Length of subsequences to extract. If larger than available\n            time series length, will be clamped to maximum available length.\n        *args: Positional arguments passed to parent StackDataset.\n        **kwargs: Keyword arguments passed to parent StackDataset. Must result in\n            a dict-style dataset with a 'time' key.\n\n    Raises:\n        AssertionError: If datasets is not a dict or lacks 'time' key.\n    \"\"\"\n    super().__init__(*args, **kwargs)\n    assert isinstance(self.datasets, dict), \"can only handle dict style stacked datasets with one key-value pair time\"\n    assert 'time' in self.datasets.keys(), \"need one dataset with key time to define the map\" \n    self._length_old = self._length\n    if seq_len &gt; self.datasets['time'].shape[2]:\n        Warning(\"seq_len is {}, setting to len of timeseries\".format(seq_len))\n        seq_len = self.datasets['time'].shape[2]\n    self.seq_len = seq_len\n    self.initialize_map(seq_len)\n</code></pre>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.load_data.TimeSeriesDataset.set_seq_len","title":"<code>set_seq_len(seq_len: int)</code>","text":"<p>Change the subsequence length and rebuild the sliding window mapping.</p> <p>Parameters:</p> Name Type Description Default <code>seq_len</code> <code>int</code> <p>New subsequence length. If None, 0, or larger than available time series length, will be clamped to maximum available length.</p> required Source code in <code>src/bnode_core/nn/nn_utils/load_data.py</code> <pre><code>def set_seq_len(self, seq_len: int):\n    \"\"\"Change the subsequence length and rebuild the sliding window mapping.\n\n    Args:\n        seq_len (int): New subsequence length. If None, 0, or larger than available\n            time series length, will be clamped to maximum available length.\n    \"\"\"\n    if seq_len == None or seq_len == 0 or seq_len &gt; self.datasets['time'].shape[2]:\n        Warning(\"seq_len is {}, setting to len of timeseries\".format(seq_len))\n        seq_len = self.datasets['time'].shape[2]\n    self.seq_len = seq_len\n    self.initialize_map(seq_len)\n</code></pre>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.load_data.TimeSeriesDataset.initialize_map","title":"<code>initialize_map(seq_len: int)</code>","text":"<p>Create the sliding window index mapping for all samples.</p> <p>Builds a mapping list where each entry [sample_idx, start_pos, end_pos] defines a sliding window position. Windows slide by 1 timestep across each sample, then continue to the next sample. This treats each window position as an independent dataset item.</p> <p>Parameters:</p> Name Type Description Default <code>seq_len</code> <code>int</code> <p>Length of sliding windows. Must be at least 1.</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>If seq_len &lt; 1.</p> Side Effects <ul> <li>Sets self.mapping to list of [sample, start, end] tuples</li> <li>Updates self._length to total number of windows across all samples</li> </ul> Source code in <code>src/bnode_core/nn/nn_utils/load_data.py</code> <pre><code>def initialize_map(self, seq_len: int):\n    \"\"\"Create the sliding window index mapping for all samples.\n\n    Builds a mapping list where each entry [sample_idx, start_pos, end_pos] defines\n    a sliding window position. Windows slide by 1 timestep across each sample, then\n    continue to the next sample. This treats each window position as an independent\n    dataset item.\n\n    Args:\n        seq_len (int): Length of sliding windows. Must be at least 1.\n\n    Raises:\n        AssertionError: If seq_len &lt; 1.\n\n    Side Effects:\n        - Sets self.mapping to list of [sample, start, end] tuples\n        - Updates self._length to total number of windows across all samples\n    \"\"\"\n    assert seq_len &gt; 0, \"seq_len must be at least 1\"\n    # define map\n    n_batches_per_sample = (self.datasets['time'].shape[2] - (seq_len - 1))\n    n_batches_total = n_batches_per_sample * self._length_old\n    self.mapping = [[] for i in range(n_batches_total)]\n    self._length = n_batches_total\n    # fill out map. mapping shall contain [n_sample, start_position, stop_position]\n    k_stop=seq_len # stop position in sequence\n    j=0 # sample position in datasets\n    for i in range(n_batches_total):\n        self.mapping[i] = [j, k_stop-seq_len, k_stop]\n        if k_stop + 2 &gt; n_batches_per_sample + seq_len: # if the over next sequence would be out of bounds, go to next sample (+1 more because of &gt; and not &gt;=)\n            k_stop = seq_len\n            j += 1\n        else:\n            k_stop += 1\n</code></pre>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.load_data.TimeSeriesDataset.__getitem__","title":"<code>__getitem__(index: int) -&gt; dict</code>","text":"<p>Get a single sliding window sample.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of the sliding window to retrieve.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary with same keys as self.datasets. For 3D+ arrays (time series), returns subsequence [start:end] from appropriate sample. For 2D arrays, returns full array for the sample.</p> Source code in <code>src/bnode_core/nn/nn_utils/load_data.py</code> <pre><code>def __getitem__(self, index: int) -&gt; dict:\n    \"\"\"Get a single sliding window sample.\n\n    Args:\n        index (int): Index of the sliding window to retrieve.\n\n    Returns:\n        dict: Dictionary with same keys as self.datasets. For 3D+ arrays (time series),\n            returns subsequence [start:end] from appropriate sample. For 2D arrays,\n            returns full array for the sample.\n    \"\"\"\n    i, k_start, k_stop = self.mapping[index]\n    ret_val = {}\n    for key, value in self.datasets.items():\n        if value.ndim == 2:\n            ret_val[key] = value[i, :]\n        else:\n            ret_val[key] = value[i, :, k_start:k_stop]\n    return ret_val\n</code></pre>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.load_data.TimeSeriesDataset.__getitems__","title":"<code>__getitems__(indices: list) -&gt; list</code>","text":"<p>Get multiple sliding window samples (batch retrieval).</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>list</code> <p>List of window indices to retrieve.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>List of dictionaries, one per index, each containing the requested sliding window data.</p> Note <p>This method requires PyTorch &gt;= 2.2 for optimal batched data loading.</p> Source code in <code>src/bnode_core/nn/nn_utils/load_data.py</code> <pre><code>def __getitems__(self, indices: list) -&gt; list:\n    \"\"\"Get multiple sliding window samples (batch retrieval).\n\n    Args:\n        indices (list): List of window indices to retrieve.\n\n    Returns:\n        list: List of dictionaries, one per index, each containing the requested\n            sliding window data.\n\n    Note:\n        This method requires PyTorch &gt;= 2.2 for optimal batched data loading.\n    \"\"\"\n    samples = [None] * len(indices)\n    for i, index in enumerate(indices):\n        samples[i] = self.__getitem__(index)\n    return samples\n</code></pre>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.load_data.TimeSeriesDataset.__len__","title":"<code>__len__() -&gt; int</code>","text":"<p>Return the total number of sliding windows in the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Total number of windows across all samples.</p> Source code in <code>src/bnode_core/nn/nn_utils/load_data.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the total number of sliding windows in the dataset.\n\n    Returns:\n        int: Total number of windows across all samples.\n    \"\"\"\n    return self._length\n</code></pre>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.load_data.load_validate_dataset_config","title":"<code>load_validate_dataset_config(path: Path) -&gt; base_pModelClass</code>","text":"<p>Load and validate dataset configuration from YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the dataset configuration YAML file.</p> required <p>Returns:</p> Type Description <code>base_pModelClass</code> <p>Validated dataset configuration as base_pModelClass instance.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If configuration file doesn't exist.</p> Note <p>Uses OmegaConf to load YAML and validates against base_pModelClass schema.</p> Source code in <code>src/bnode_core/nn/nn_utils/load_data.py</code> <pre><code>def load_validate_dataset_config(path: Path) -&gt; base_pModelClass:\n    \"\"\"Load and validate dataset configuration from YAML file.\n\n    Args:\n        path: Path to the dataset configuration YAML file.\n\n    Returns:\n        Validated dataset configuration as base_pModelClass instance.\n\n    Raises:\n        FileNotFoundError: If configuration file doesn't exist.\n\n    Note:\n        Uses OmegaConf to load YAML and validates against base_pModelClass schema.\n    \"\"\"\n\n    if not path.exists():\n        raise FileNotFoundError('Dataset config file not found: {}'.format(path))\n\n    logging.info('Loading dataset config file: {}'.format(path))\n    _dataset_config_dict = OmegaConf.load(path)\n    _dataset_config_dict = OmegaConf.to_object(_dataset_config_dict) # make dict\n    dataset_config = base_pModelClass(**_dataset_config_dict) # validate\n    logging.info('Validated dataset config file: {}'.format(path))\n    return dataset_config\n</code></pre>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.load_data.load_dataset_and_config","title":"<code>load_dataset_and_config(dataset_name: str, dataset_path: str) -&gt; Tuple[h5py.File, Optional[base_pModelClass]]</code>","text":"<p>Load HDF5 dataset and its configuration.</p> <p>Loads the HDF5 dataset file and attempts to load its configuration. If configuration file doesn't exist, returns None for config.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>Name/identifier of the dataset.</p> required <code>dataset_path</code> <code>str</code> <p>Explicit path to dataset file, or empty string to use default location.</p> required <p>Returns:</p> Type Description <code>Tuple[File, Optional[base_pModelClass]]</code> <p>Tuple of (dataset, dataset_config) where:</p> <ul> <li>dataset: Open h5py.File handle to HDF5 dataset.</li> <li>dataset_config: Validated configuration (base_pModelClass) or None if not found.</li> </ul> Note <p>The returned h5py.File should be closed when done (dataset.close()). Uses filepath_dataset_from_config to resolve actual file path.</p> Source code in <code>src/bnode_core/nn/nn_utils/load_data.py</code> <pre><code>def load_dataset_and_config(dataset_name: str, dataset_path: str) -&gt; Tuple[h5py.File, Optional[base_pModelClass]]:\n    \"\"\"Load HDF5 dataset and its configuration.\n\n    Loads the HDF5 dataset file and attempts to load its configuration.\n    If configuration file doesn't exist, returns None for config.\n\n    Args:\n        dataset_name: Name/identifier of the dataset.\n        dataset_path: Explicit path to dataset file, or empty string to use default location.\n\n    Returns:\n        Tuple of (dataset, dataset_config) where:\n\n            - dataset: Open h5py.File handle to HDF5 dataset.\n            - dataset_config: Validated configuration (base_pModelClass) or None if not found.\n\n    Note:\n        The returned h5py.File should be closed when done (dataset.close()).\n        Uses filepath_dataset_from_config to resolve actual file path.\n    \"\"\"\n    _path = filepath_dataset_from_config(dataset_name, dataset_path)\n\n    dataset = h5py.File(_path, 'r')\n    logging.info('Loaded dataset from file: {}'.format(_path))\n\n    _path = filepath_dataset_config_from_name(dataset_name)\n    if not _path.exists():\n        logging.info('No dataset config file found, using information from dataset file')\n        dataset_config = None\n    else:\n        dataset_config = load_validate_dataset_config(_path)\n    return dataset, dataset_config\n</code></pre>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.load_data.make_stacked_dataset","title":"<code>make_stacked_dataset(dataset: h5py.File, context: str, seq_len_from_file: Optional[int] = None, seq_len_batches: Optional[int] = None) -&gt; Union[torch.utils.data.StackDataset, TimeSeriesDataset]</code>","text":"<p>Create a PyTorch dataset from HDF5 data with optional time series batching.</p> <p>Loads time series data (states, derivatives, parameters, controls, outputs) from an HDF5 file and wraps it in a PyTorch StackDataset. If seq_len_batches is specified, returns a TimeSeriesDataset that enables sliding window sampling for variable-length sequences.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>File</code> <p>Open HDF5 file containing time series data with groups for different contexts (train/test/validation).</p> required <code>context</code> <code>str</code> <p>Dataset context to load. Must be one of: 'train', 'test', 'validation', 'common_test', or 'common_validation'.</p> required <code>seq_len_from_file</code> <code>int</code> <p>If provided, truncates loaded sequences to this length from the original file data. Defaults to None (use full sequence length).</p> <code>None</code> <code>seq_len_batches</code> <code>int</code> <p>If provided, returns a TimeSeriesDataset that extracts subsequences of this length via sliding window. If None, returns standard StackDataset with full sequences. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[StackDataset, TimeSeriesDataset]</code> <p>torch.utils.data.StackDataset or TimeSeriesDataset: A dataset that yields dictionaries containing tensors for 'time', 'states', and optionally 'states_der', 'parameters', 'controls', and 'outputs'. Each tensor has shape (batch, channels, time_steps).</p> Note <ul> <li>All None-valued arrays are automatically excluded from the returned dataset</li> <li>Time tensor is replicated across batch dimension from single time vector</li> <li>When seq_len_batches is used, the dataset length increases to accommodate all   possible sliding windows across the original sequences</li> </ul> Source code in <code>src/bnode_core/nn/nn_utils/load_data.py</code> <pre><code>def make_stacked_dataset(\n    dataset: h5py.File, \n    context: str, \n    seq_len_from_file: Optional[int] = None, \n    seq_len_batches: Optional[int] = None\n) -&gt; Union[torch.utils.data.StackDataset, 'TimeSeriesDataset']:\n    \"\"\"Create a PyTorch dataset from HDF5 data with optional time series batching.\n\n    Loads time series data (states, derivatives, parameters, controls, outputs) from an HDF5\n    file and wraps it in a PyTorch StackDataset. If seq_len_batches is specified, returns a\n    TimeSeriesDataset that enables sliding window sampling for variable-length sequences.\n\n    Args:\n        dataset (h5py.File): Open HDF5 file containing time series data with groups for\n            different contexts (train/test/validation).\n        context (str): Dataset context to load. Must be one of: 'train', 'test', 'validation',\n            'common_test', or 'common_validation'.\n        seq_len_from_file (int, optional): If provided, truncates loaded sequences to this length\n            from the original file data. Defaults to None (use full sequence length).\n        seq_len_batches (int, optional): If provided, returns a TimeSeriesDataset that extracts\n            subsequences of this length via sliding window. If None, returns standard StackDataset\n            with full sequences. Defaults to None.\n\n    Returns:\n        torch.utils.data.StackDataset or TimeSeriesDataset: A dataset that yields dictionaries\n            containing tensors for 'time', 'states', and optionally 'states_der', 'parameters',\n            'controls', and 'outputs'. Each tensor has shape (batch, channels, time_steps).\n\n    Note:\n        - All None-valued arrays are automatically excluded from the returned dataset\n        - Time tensor is replicated across batch dimension from single time vector\n        - When seq_len_batches is used, the dataset length increases to accommodate all\n          possible sliding windows across the original sequences\n    \"\"\"\n    assert context in ['train', 'test', 'validation', 'common_test', 'common_validation'], 'context must be one of train, test, validation, common_test, common_validation'\n\n    # get tensors of dataset\n    time = dataset['time'][:]\n    states = dataset[context]['states'][:]\n    states_der = dataset[context]['states_der'][:] if 'states_der' in dataset[context].keys() else None\n    parameters = dataset[context]['parameters'][:] if 'parameters' in dataset[context].keys() else None\n    controls = dataset[context]['controls'][:] if 'controls' in dataset[context].keys() else None\n    outputs = dataset[context]['outputs'][:] if 'outputs' in dataset[context].keys() else None\n\n    # cut data from file to seq_len\n    if seq_len_from_file is not None:\n        time = time[:seq_len_from_file]\n        states = states[:,:,:seq_len_from_file]\n        states_der = states_der[:,:,:seq_len_from_file] if states_der is not None else None # TODO: add finite difference calculation for states_der and cfg.dataset_prep entries to say if derivatives are included\n        parameters = parameters[:] if parameters is not None else None\n        controls = controls[:,:,:seq_len_from_file] if controls is not None else None\n        outputs = outputs[:,:,:seq_len_from_file] if outputs is not None else None\n\n    # define wrapper to delete nones from kwargs dict\n    def _delete_nones(**kwargs):\n        kwargs = dict((k,v) for k,v in kwargs.items() if v is not None)\n        return kwargs\n\n    # make torch dataset with dict as output\n    dataset_type = torch.utils.data.StackDataset if seq_len_batches is None else lambda *args, **kwargs: TimeSeriesDataset(seq_len_batches, *args, **kwargs)\n    torch_dataset = dataset_type(\n        **_delete_nones(\n            time = torch.tensor(time, dtype=torch.float32).unsqueeze(0).expand(states.shape[0], -1).unsqueeze(1),\n            states = torch.tensor(states, dtype=torch.float32),\n            states_der = torch.tensor(states_der, dtype=torch.float32) if states_der is not None else None,\n            parameters = torch.tensor(parameters, dtype=torch.float32) if parameters is not None else None,\n            controls = torch.tensor(controls, dtype=torch.float32) if controls is not None else None,\n            outputs = torch.tensor(outputs, dtype=torch.float32) if outputs is not None else None,\n        )\n    )\n    logging.info('Created {} with {} and sequence length {}'.format(type(torch_dataset),torch_dataset.datasets.keys(), torch_dataset.datasets['time'].shape[2]))\n    return torch_dataset\n</code></pre>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.kullback_leibler","title":"<code>bnode_core.nn.nn_utils.kullback_leibler</code>","text":"<p>Kullback-Leibler divergence computation for VAE training.</p> <p>Provides functions to compute KL divergence between learned latent distributions and standard normal prior, with support for timeseries data and dimension analysis.</p> Attention <p>This documentation is generated by AI. Please be aware of possible inaccurcies.</p>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.kullback_leibler.kullback_leibler","title":"<code>kullback_leibler(mu: torch.Tensor, logvar: torch.Tensor, per_dimension: bool = False, reduce: bool = True, time_series_aggregation_mode: Optional[str] = 'mean') -&gt; torch.Tensor</code>","text":"<p>Compute KL divergence KL(N(mu, exp(logvar)) || N(0, I)).</p> <p>Calculates the Kullback-Leibler divergence between a learned normal distribution N(mu, sigma^2) and the standard normal prior N(0, 1). Uses the analytical formula: KL = -0.5 * sum(1 + logvar - mu^2 - exp(logvar))</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>Tensor</code> <p>Mean of learned distribution, shape (batch, latent_dim) or  (batch, latent_dim, seq_len) for timeseries.</p> required <code>logvar</code> <code>Tensor</code> <p>Log-variance of learned distribution, same shape as mu.</p> required <code>per_dimension</code> <code>bool</code> <p>If True, return KL divergence per latent dimension instead of summing across dimensions. Default: False.</p> <code>False</code> <code>reduce</code> <code>bool</code> <p>If True, return mean over batch. If False, return per-sample values. Default: True.</p> <code>True</code> <code>time_series_aggregation_mode</code> <code>Optional[str]</code> <p>How to aggregate over time dimension if input is timeseries (3D). Options: 'mean', 'max', 'sum', or None (keep time dim). Default: 'mean'.</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>KL divergence tensor. Shape depends on parameters: - per_dimension=False, reduce=True: scalar - per_dimension=False, reduce=False: (batch,) - per_dimension=True, reduce=True: (latent_dim,) - per_dimension=True, reduce=False: (batch, latent_dim)</p> Note <p>The KL divergence is always non-negative and equals zero only when the learned distribution matches the prior exactly.</p> Source code in <code>src/bnode_core/nn/nn_utils/kullback_leibler.py</code> <pre><code>def kullback_leibler(\n    mu: torch.Tensor, \n    logvar: torch.Tensor, \n    per_dimension: bool = False, \n    reduce: bool = True, \n    time_series_aggregation_mode: Optional[str] = 'mean'\n) -&gt; torch.Tensor:\n    \"\"\"Compute KL divergence KL(N(mu, exp(logvar)) || N(0, I)).\n\n    Calculates the Kullback-Leibler divergence between a learned normal distribution\n    N(mu, sigma^2) and the standard normal prior N(0, 1). Uses the analytical formula:\n    KL = -0.5 * sum(1 + logvar - mu^2 - exp(logvar))\n\n    Args:\n        mu: Mean of learned distribution, shape (batch, latent_dim) or \n            (batch, latent_dim, seq_len) for timeseries.\n        logvar: Log-variance of learned distribution, same shape as mu.\n        per_dimension: If True, return KL divergence per latent dimension instead of\n            summing across dimensions. Default: False.\n        reduce: If True, return mean over batch. If False, return per-sample values.\n            Default: True.\n        time_series_aggregation_mode: How to aggregate over time dimension if input is\n            timeseries (3D). Options: 'mean', 'max', 'sum', or None (keep time dim).\n            Default: 'mean'.\n\n    Returns:\n        KL divergence tensor. Shape depends on parameters:\n            - per_dimension=False, reduce=True: scalar\n            - per_dimension=False, reduce=False: (batch,)\n            - per_dimension=True, reduce=True: (latent_dim,)\n            - per_dimension=True, reduce=False: (batch, latent_dim)\n\n    Note:\n        The KL divergence is always non-negative and equals zero only when the learned\n        distribution matches the prior exactly.\n    \"\"\"\n    is_timeseries = len(mu.shape) == 3\n    kl = -0.5 *(1 + logvar - mu.pow(2) - logvar.exp())\n\n    if is_timeseries:\n        if time_series_aggregation_mode == 'mean':\n            kl = torch.mean(kl, dim=2)\n        elif time_series_aggregation_mode == 'max':\n            kl = torch.max(kl, dim=2)\n        elif time_series_aggregation_mode == 'sum':\n            kl = torch.sum(kl, dim=2)\n        elif time_series_aggregation_mode == None:\n            pass\n\n    if per_dimension is False:\n        kl = torch.sum(kl, dim=1)  \n\n    if reduce:\n        kl = torch.mean(kl, dim=0)\n\n    return kl\n</code></pre>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.kullback_leibler.count_populated_dimensions","title":"<code>count_populated_dimensions(mu: torch.Tensor, logvar: torch.Tensor, threshold: float = 0.05, kl_timeseries_aggregation_mode: str = 'mean', return_idx: bool = False) -&gt; torch.Tensor</code>","text":"<p>Count number of latent dimensions actively used by the model.</p> <p>A dimension is considered \"populated\" or \"active\" if its KL divergence exceeds a threshold. This helps diagnose posterior collapse (when KL \u2192 0 for all dimensions) and track how many dimensions the model actually uses.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>Tensor</code> <p>Mean of learned distribution, shape (batch, latent_dim) or (batch, latent_dim, seq_len).</p> required <code>logvar</code> <code>Tensor</code> <p>Log-variance of learned distribution, same shape as mu.</p> required <code>threshold</code> <code>float</code> <p>Minimum KL divergence for a dimension to be considered active. Default: 0.05.</p> <code>0.05</code> <code>kl_timeseries_aggregation_mode</code> <code>str</code> <p>Aggregation mode for timeseries data ('mean', 'max', 'sum'). Default: 'mean'.</p> <code>'mean'</code> <code>return_idx</code> <code>bool</code> <p>If True, also return boolean mask of active dimensions. Default: False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tuple of (n_dim_populated, idx): - n_dim_populated: Number of dimensions with KL &gt; threshold (scalar tensor). - idx: Boolean mask of active dimensions (tensor or None if return_idx=False).</p> Note <p>Computes KL per dimension averaged over batch (per_dimension=True, reduce=True). Uses torch.no_grad() for efficiency since this is a diagnostic metric.</p> Source code in <code>src/bnode_core/nn/nn_utils/kullback_leibler.py</code> <pre><code>def count_populated_dimensions(\n    mu: torch.Tensor, \n    logvar: torch.Tensor, \n    threshold: float = 0.05, \n    kl_timeseries_aggregation_mode: str = 'mean', \n    return_idx: bool = False\n) -&gt; torch.Tensor:\n    \"\"\"Count number of latent dimensions actively used by the model.\n\n    A dimension is considered \"populated\" or \"active\" if its KL divergence exceeds\n    a threshold. This helps diagnose posterior collapse (when KL \u2192 0 for all dimensions)\n    and track how many dimensions the model actually uses.\n\n    Args:\n        mu: Mean of learned distribution, shape (batch, latent_dim) or (batch, latent_dim, seq_len).\n        logvar: Log-variance of learned distribution, same shape as mu.\n        threshold: Minimum KL divergence for a dimension to be considered active. Default: 0.05.\n        kl_timeseries_aggregation_mode: Aggregation mode for timeseries data ('mean', 'max', 'sum').\n            Default: 'mean'.\n        return_idx: If True, also return boolean mask of active dimensions. Default: False.\n\n    Returns:\n        Tuple of (n_dim_populated, idx):\n            - n_dim_populated: Number of dimensions with KL &gt; threshold (scalar tensor).\n            - idx: Boolean mask of active dimensions (tensor or None if return_idx=False).\n\n    Note:\n        Computes KL per dimension averaged over batch (per_dimension=True, reduce=True).\n        Uses torch.no_grad() for efficiency since this is a diagnostic metric.\n    \"\"\"\n    with torch.no_grad():\n        kl = kullback_leibler(mu, logvar, per_dimension = True, reduce = True, time_series_aggregation_mode = kl_timeseries_aggregation_mode)\n        # print histogram of kl divergence to terminal\n        # print('kl divergence histogram: 0.0 - 2.0')\n        # print(torch.histc(kl, bins=21, min=0, max=2.0))\n        idx = kl &gt; threshold\n        n_dim_populated = torch.sum(idx)\n    if return_idx:\n        return n_dim_populated.detach(), idx.detach()\n    else:\n        return n_dim_populated.detach(), None\n</code></pre>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.early_stopping","title":"<code>bnode_core.nn.nn_utils.early_stopping</code>","text":"<p>Early stopping utility for PyTorch model training.</p> <p>Monitors validation loss and stops training when no improvement is observed for a specified number of epochs (patience). Saves best model checkpoint.</p> Attention <p>This documentation is generated by AI. Please be aware of possible inaccurcies.</p>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.early_stopping.EarlyStopping","title":"<code>EarlyStopping</code>","text":"<p>Stop training early if validation loss doesn't improve after given patience.</p> <p>Tracks validation loss and saves model checkpoints when improvements occur. Triggers early stopping flag when loss plateaus for 'patience' epochs.</p> <p>Attributes:</p> Name Type Description <code>patience</code> <p>Number of epochs to wait before stopping after loss plateau.</p> <code>verbose</code> <p>If True, print messages for each loss improvement.</p> <code>counter</code> <p>Number of epochs since last loss improvement.</p> <code>best_score</code> <p>Best validation loss seen so far.</p> <code>corresponding_score</code> <p>Training loss corresponding to best validation loss.</p> <code>early_stop</code> <p>Flag indicating whether to stop training.</p> <code>score_last_save</code> <p>Validation loss at last checkpoint save.</p> <code>threshold</code> <p>Minimum loss improvement to qualify as improvement.</p> <code>threshold_mode</code> <p>Either 'abs' (absolute) or 'rel' (relative) threshold.</p> <code>path</code> <p>File path for saving model checkpoint.</p> <code>optimizer_path</code> <p>File path for saving optimizer state.</p> Source code in <code>src/bnode_core/nn/nn_utils/early_stopping.py</code> <pre><code>class EarlyStopping:\n    \"\"\"Stop training early if validation loss doesn't improve after given patience.\n\n    Tracks validation loss and saves model checkpoints when improvements occur.\n    Triggers early stopping flag when loss plateaus for 'patience' epochs.\n\n    Attributes:\n        patience: Number of epochs to wait before stopping after loss plateau.\n        verbose: If True, print messages for each loss improvement.\n        counter: Number of epochs since last loss improvement.\n        best_score: Best validation loss seen so far.\n        corresponding_score: Training loss corresponding to best validation loss.\n        early_stop: Flag indicating whether to stop training.\n        score_last_save: Validation loss at last checkpoint save.\n        threshold: Minimum loss improvement to qualify as improvement.\n        threshold_mode: Either 'abs' (absolute) or 'rel' (relative) threshold.\n        path: File path for saving model checkpoint.\n        optimizer_path: File path for saving optimizer state.\n    \"\"\"\n\n    def __init__(\n        self, \n        patience: int = 7, \n        verbose: bool = False, \n        threshold: float = 0, \n        threshold_mode: str = 'abs', \n        path: str = 'checkpoint.pt', \n        optimizer_path: str = 'optimizer.pt', \n        trace_func: Callable = print\n    ):\n        \"\"\"Initialize early stopping monitor.\n\n        Args:\n            patience: Number of epochs to wait after last validation loss improvement\n                before triggering early stop. Default: 7.\n            verbose: If True, prints message for each validation loss improvement. Default: False.\n            threshold: Minimum change in monitored loss to qualify as improvement. Default: 0.\n            threshold_mode: Either 'abs' (absolute: loss &lt; best - threshold) or \n                'rel' (relative: loss &lt; best * (1 - threshold)). Default: 'abs'.\n            path: Path to save best model checkpoint. Default: 'checkpoint.pt'.\n            optimizer_path: Path to save optimizer state. Default: 'optimizer.pt'.\n            trace_func: Logging function for status messages. Default: print.\n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.corresponding_score = None\n        self.early_stop = False\n        self.score_last_save = np.inf\n        self.threshold = threshold\n        self.threshold_mode = threshold_mode\n        self.path = path\n        self.optimizer_path = optimizer_path\n        self.trace_func = trace_func\n        self.trace_func('EarlyStopping initialized with patience = {}, threshold = {}, threshold_mode = {}'.format(\n            self.patience, self.threshold, self.threshold_mode))\n\n    def reset(self):\n        \"\"\"Reset all early stopping state to initial values.\n\n        Useful for starting a new training phase with fresh early stopping.\n        \"\"\"\n        self.reset_counter()\n        self.best_score = None\n        self.corresponding_score = None\n        self.early_stop = False\n        self.score_last_save = np.inf\n\n    def reset_counter(self):\n        \"\"\"Reset only the patience counter and early_stop flag.\n\n        Keeps best_score intact. Useful after manual interventions.\n        \"\"\"\n        self.counter = 0\n        self.early_stop = False\n\n    def __call__(\n        self, \n        loss: float, \n        model: nn.Module, \n        epoch: Optional[int] = None, \n        optimizer: Optional[optim.Optimizer] = None, \n        corresponding_loss: Optional[float] = None\n    ):\n        \"\"\"Update early stopping state based on current validation loss.\n\n        Checks if loss has improved according to threshold criteria. Saves checkpoint\n        if improvement occurred, otherwise increments patience counter. Sets early_stop\n        flag when counter reaches patience.\n\n        Args:\n            loss: Current validation loss.\n            model: PyTorch model with save() method.\n            epoch: Current epoch number (for logging). Optional.\n            optimizer: PyTorch optimizer to save state. Optional.\n            corresponding_loss: Training loss from same epoch (for tracking). Optional.\n\n        Side Effects:\n            - Updates counter, best_score, and corresponding_score\n            - Saves model checkpoint when loss improves\n            - Sets early_stop flag when patience exceeded\n            - Handles NaN loss by setting to infinity\n        \"\"\"\n        # if loss is not a number\n        if np.isnan(loss):\n            loss = np.inf\n            logging.warning('EarlyStopping: loss is NaN. Setting to Inf for early stopping update.')\n        score = loss\n\n        # initial case\n        if self.best_score is None:\n            self.best_score = score\n            self.corresponding_score = corresponding_loss\n            self.save_checkpoint(loss, model, optimizer, epoch)\n\n        _update_flag = False\n\n        if self.threshold_mode == 'abs':\n            if score &lt; self.best_score - self.threshold:\n                _update_flag = True\n        elif self.threshold_mode == 'rel':\n            if score &lt; self.best_score * (1 - self.threshold):\n                _update_flag = True\n        else:\n            raise ValueError('Invalid threshold mode selected.')\n\n        if _update_flag:\n            self.best_score = score\n            self.counter = 0\n            self.corresponding_score = corresponding_loss\n            self.save_checkpoint(loss, model, optimizer, epoch)\n        else:\n            self.counter += 1\n        if self.counter &gt;= self.patience:\n                self.early_stop = True\n\n    def save_checkpoint(\n        self, \n        loss: float, \n        model: nn.Module, \n        optimizer: Optional[optim.Optimizer], \n        epoch: Optional[int]\n    ):\n        \"\"\"Save model checkpoint when validation loss improves.\n\n        Args:\n            loss: Current validation loss (for logging).\n            model: PyTorch model with save() method.\n            optimizer: PyTorch optimizer (state saved if not None).\n            epoch: Current epoch number (for logging).\n\n        Side Effects:\n            - Calls model.save(path) to persist model state\n            - Saves optimizer state_dict if optimizer provided\n            - Updates score_last_save\n            - Logs checkpoint save if verbose=True\n        \"\"\"\n        if self.verbose:\n            self.trace_func('----------------------&gt; Epoch {} Validation loss decreased ({:.6f} --&gt; {:.6f}).  Saving model to {}'.format(epoch, self.score_last_save, loss, self.path))\n        model.save(self.path)\n        self.score_last_save = loss\n        if optimizer is not None:\n                torch.save(optimizer.state_dict(), self.optimizer_path)\n</code></pre>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.early_stopping.EarlyStopping.__init__","title":"<code>__init__(patience: int = 7, verbose: bool = False, threshold: float = 0, threshold_mode: str = 'abs', path: str = 'checkpoint.pt', optimizer_path: str = 'optimizer.pt', trace_func: Callable = print)</code>","text":"<p>Initialize early stopping monitor.</p> <p>Parameters:</p> Name Type Description Default <code>patience</code> <code>int</code> <p>Number of epochs to wait after last validation loss improvement before triggering early stop. Default: 7.</p> <code>7</code> <code>verbose</code> <code>bool</code> <p>If True, prints message for each validation loss improvement. Default: False.</p> <code>False</code> <code>threshold</code> <code>float</code> <p>Minimum change in monitored loss to qualify as improvement. Default: 0.</p> <code>0</code> <code>threshold_mode</code> <code>str</code> <p>Either 'abs' (absolute: loss &lt; best - threshold) or  'rel' (relative: loss &lt; best * (1 - threshold)). Default: 'abs'.</p> <code>'abs'</code> <code>path</code> <code>str</code> <p>Path to save best model checkpoint. Default: 'checkpoint.pt'.</p> <code>'checkpoint.pt'</code> <code>optimizer_path</code> <code>str</code> <p>Path to save optimizer state. Default: 'optimizer.pt'.</p> <code>'optimizer.pt'</code> <code>trace_func</code> <code>Callable</code> <p>Logging function for status messages. Default: print.</p> <code>print</code> Source code in <code>src/bnode_core/nn/nn_utils/early_stopping.py</code> <pre><code>def __init__(\n    self, \n    patience: int = 7, \n    verbose: bool = False, \n    threshold: float = 0, \n    threshold_mode: str = 'abs', \n    path: str = 'checkpoint.pt', \n    optimizer_path: str = 'optimizer.pt', \n    trace_func: Callable = print\n):\n    \"\"\"Initialize early stopping monitor.\n\n    Args:\n        patience: Number of epochs to wait after last validation loss improvement\n            before triggering early stop. Default: 7.\n        verbose: If True, prints message for each validation loss improvement. Default: False.\n        threshold: Minimum change in monitored loss to qualify as improvement. Default: 0.\n        threshold_mode: Either 'abs' (absolute: loss &lt; best - threshold) or \n            'rel' (relative: loss &lt; best * (1 - threshold)). Default: 'abs'.\n        path: Path to save best model checkpoint. Default: 'checkpoint.pt'.\n        optimizer_path: Path to save optimizer state. Default: 'optimizer.pt'.\n        trace_func: Logging function for status messages. Default: print.\n    \"\"\"\n    self.patience = patience\n    self.verbose = verbose\n    self.counter = 0\n    self.best_score = None\n    self.corresponding_score = None\n    self.early_stop = False\n    self.score_last_save = np.inf\n    self.threshold = threshold\n    self.threshold_mode = threshold_mode\n    self.path = path\n    self.optimizer_path = optimizer_path\n    self.trace_func = trace_func\n    self.trace_func('EarlyStopping initialized with patience = {}, threshold = {}, threshold_mode = {}'.format(\n        self.patience, self.threshold, self.threshold_mode))\n</code></pre>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.early_stopping.EarlyStopping.reset","title":"<code>reset()</code>","text":"<p>Reset all early stopping state to initial values.</p> <p>Useful for starting a new training phase with fresh early stopping.</p> Source code in <code>src/bnode_core/nn/nn_utils/early_stopping.py</code> <pre><code>def reset(self):\n    \"\"\"Reset all early stopping state to initial values.\n\n    Useful for starting a new training phase with fresh early stopping.\n    \"\"\"\n    self.reset_counter()\n    self.best_score = None\n    self.corresponding_score = None\n    self.early_stop = False\n    self.score_last_save = np.inf\n</code></pre>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.early_stopping.EarlyStopping.reset_counter","title":"<code>reset_counter()</code>","text":"<p>Reset only the patience counter and early_stop flag.</p> <p>Keeps best_score intact. Useful after manual interventions.</p> Source code in <code>src/bnode_core/nn/nn_utils/early_stopping.py</code> <pre><code>def reset_counter(self):\n    \"\"\"Reset only the patience counter and early_stop flag.\n\n    Keeps best_score intact. Useful after manual interventions.\n    \"\"\"\n    self.counter = 0\n    self.early_stop = False\n</code></pre>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.early_stopping.EarlyStopping.__call__","title":"<code>__call__(loss: float, model: nn.Module, epoch: Optional[int] = None, optimizer: Optional[optim.Optimizer] = None, corresponding_loss: Optional[float] = None)</code>","text":"<p>Update early stopping state based on current validation loss.</p> <p>Checks if loss has improved according to threshold criteria. Saves checkpoint if improvement occurred, otherwise increments patience counter. Sets early_stop flag when counter reaches patience.</p> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>float</code> <p>Current validation loss.</p> required <code>model</code> <code>Module</code> <p>PyTorch model with save() method.</p> required <code>epoch</code> <code>Optional[int]</code> <p>Current epoch number (for logging). Optional.</p> <code>None</code> <code>optimizer</code> <code>Optional[Optimizer]</code> <p>PyTorch optimizer to save state. Optional.</p> <code>None</code> <code>corresponding_loss</code> <code>Optional[float]</code> <p>Training loss from same epoch (for tracking). Optional.</p> <code>None</code> Side Effects <ul> <li>Updates counter, best_score, and corresponding_score</li> <li>Saves model checkpoint when loss improves</li> <li>Sets early_stop flag when patience exceeded</li> <li>Handles NaN loss by setting to infinity</li> </ul> Source code in <code>src/bnode_core/nn/nn_utils/early_stopping.py</code> <pre><code>def __call__(\n    self, \n    loss: float, \n    model: nn.Module, \n    epoch: Optional[int] = None, \n    optimizer: Optional[optim.Optimizer] = None, \n    corresponding_loss: Optional[float] = None\n):\n    \"\"\"Update early stopping state based on current validation loss.\n\n    Checks if loss has improved according to threshold criteria. Saves checkpoint\n    if improvement occurred, otherwise increments patience counter. Sets early_stop\n    flag when counter reaches patience.\n\n    Args:\n        loss: Current validation loss.\n        model: PyTorch model with save() method.\n        epoch: Current epoch number (for logging). Optional.\n        optimizer: PyTorch optimizer to save state. Optional.\n        corresponding_loss: Training loss from same epoch (for tracking). Optional.\n\n    Side Effects:\n        - Updates counter, best_score, and corresponding_score\n        - Saves model checkpoint when loss improves\n        - Sets early_stop flag when patience exceeded\n        - Handles NaN loss by setting to infinity\n    \"\"\"\n    # if loss is not a number\n    if np.isnan(loss):\n        loss = np.inf\n        logging.warning('EarlyStopping: loss is NaN. Setting to Inf for early stopping update.')\n    score = loss\n\n    # initial case\n    if self.best_score is None:\n        self.best_score = score\n        self.corresponding_score = corresponding_loss\n        self.save_checkpoint(loss, model, optimizer, epoch)\n\n    _update_flag = False\n\n    if self.threshold_mode == 'abs':\n        if score &lt; self.best_score - self.threshold:\n            _update_flag = True\n    elif self.threshold_mode == 'rel':\n        if score &lt; self.best_score * (1 - self.threshold):\n            _update_flag = True\n    else:\n        raise ValueError('Invalid threshold mode selected.')\n\n    if _update_flag:\n        self.best_score = score\n        self.counter = 0\n        self.corresponding_score = corresponding_loss\n        self.save_checkpoint(loss, model, optimizer, epoch)\n    else:\n        self.counter += 1\n    if self.counter &gt;= self.patience:\n            self.early_stop = True\n</code></pre>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.early_stopping.EarlyStopping.save_checkpoint","title":"<code>save_checkpoint(loss: float, model: nn.Module, optimizer: Optional[optim.Optimizer], epoch: Optional[int])</code>","text":"<p>Save model checkpoint when validation loss improves.</p> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>float</code> <p>Current validation loss (for logging).</p> required <code>model</code> <code>Module</code> <p>PyTorch model with save() method.</p> required <code>optimizer</code> <code>Optional[Optimizer]</code> <p>PyTorch optimizer (state saved if not None).</p> required <code>epoch</code> <code>Optional[int]</code> <p>Current epoch number (for logging).</p> required Side Effects <ul> <li>Calls model.save(path) to persist model state</li> <li>Saves optimizer state_dict if optimizer provided</li> <li>Updates score_last_save</li> <li>Logs checkpoint save if verbose=True</li> </ul> Source code in <code>src/bnode_core/nn/nn_utils/early_stopping.py</code> <pre><code>def save_checkpoint(\n    self, \n    loss: float, \n    model: nn.Module, \n    optimizer: Optional[optim.Optimizer], \n    epoch: Optional[int]\n):\n    \"\"\"Save model checkpoint when validation loss improves.\n\n    Args:\n        loss: Current validation loss (for logging).\n        model: PyTorch model with save() method.\n        optimizer: PyTorch optimizer (state saved if not None).\n        epoch: Current epoch number (for logging).\n\n    Side Effects:\n        - Calls model.save(path) to persist model state\n        - Saves optimizer state_dict if optimizer provided\n        - Updates score_last_save\n        - Logs checkpoint save if verbose=True\n    \"\"\"\n    if self.verbose:\n        self.trace_func('----------------------&gt; Epoch {} Validation loss decreased ({:.6f} --&gt; {:.6f}).  Saving model to {}'.format(epoch, self.score_last_save, loss, self.path))\n    model.save(self.path)\n    self.score_last_save = loss\n    if optimizer is not None:\n            torch.save(optimizer.state_dict(), self.optimizer_path)\n</code></pre>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.count_parameters","title":"<code>bnode_core.nn.nn_utils.count_parameters</code>","text":"<p>Utility for counting trainable parameters in PyTorch models.</p>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.count_parameters.count_parameters","title":"<code>count_parameters(model: nn.Module) -&gt; int</code>","text":"<p>Count total number of trainable parameters in a model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>PyTorch model (nn.Module).</p> required <p>Returns:</p> Type Description <code>int</code> <p>Total number of trainable parameters (int).</p> Example <p>model = VAE(...) num_params = count_parameters(model) print(f\"Model has {num_params:,} trainable parameters\")</p> Source code in <code>src/bnode_core/nn/nn_utils/count_parameters.py</code> <pre><code>def count_parameters(model: nn.Module) -&gt; int:\n    \"\"\"Count total number of trainable parameters in a model.\n\n    Args:\n        model: PyTorch model (nn.Module).\n\n    Returns:\n        Total number of trainable parameters (int).\n\n    Example:\n        &gt;&gt;&gt; model = VAE(...)\n        &gt;&gt;&gt; num_params = count_parameters(model)\n        &gt;&gt;&gt; print(f\"Model has {num_params:,} trainable parameters\")\n    \"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n</code></pre>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.capacity_scheduler","title":"<code>bnode_core.nn.nn_utils.capacity_scheduler</code>","text":"<p>Capacity scheduler for VAE training with controlled KL divergence growth.</p> <p>This module implements a scheduler that gradually increases the KL divergence capacity during VAE training to prevent posterior collapse while maintaining good reconstruction quality.</p> Attention <p>This documentation is generated by AI. Please be aware of possible inaccurcies.</p>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.capacity_scheduler.capacity_scheduler","title":"<code>capacity_scheduler</code>","text":"<p>Schedule the KL divergence capacity for VAE bottleneck layer.</p> <p>Gradually increases the target KL divergence (capacity) to allow the model to learn meaningful latent representations without posterior collapse. The capacity increases when validation loss plateaus, encouraging the model to use more of the latent space.</p> <p>Attributes:</p> Name Type Description <code>patience</code> <p>Number of epochs to wait before increasing capacity after loss plateau.</p> <code>capacity</code> <p>Current KL divergence capacity target.</p> <code>capacity_max</code> <p>Maximum capacity value (stops increasing after reaching this).</p> <code>capacity_increment</code> <p>Amount to increase capacity by (absolute or relative).</p> <code>capacity_increment_mode</code> <p>Either 'abs' (add increment) or 'rel' (multiply by increment).</p> <code>counter</code> <p>Number of epochs since last loss improvement.</p> <code>best_score</code> <p>Best validation loss seen so far.</p> <code>threshold</code> <p>Minimum loss improvement to reset counter.</p> <code>threshold_mode</code> <p>Either 'abs' (absolute threshold) or 'rel' (relative threshold).</p> <code>enabled</code> <p>If False, scheduler is disabled and returns None for capacity.</p> <code>reached_max_capacity</code> <p>True if capacity has reached capacity_max.</p> Source code in <code>src/bnode_core/nn/nn_utils/capacity_scheduler.py</code> <pre><code>class capacity_scheduler:\n    \"\"\"Schedule the KL divergence capacity for VAE bottleneck layer.\n\n    Gradually increases the target KL divergence (capacity) to allow the model to\n    learn meaningful latent representations without posterior collapse. The capacity\n    increases when validation loss plateaus, encouraging the model to use more of\n    the latent space.\n\n    Attributes:\n        patience: Number of epochs to wait before increasing capacity after loss plateau.\n        capacity: Current KL divergence capacity target.\n        capacity_max: Maximum capacity value (stops increasing after reaching this).\n        capacity_increment: Amount to increase capacity by (absolute or relative).\n        capacity_increment_mode: Either 'abs' (add increment) or 'rel' (multiply by increment).\n        counter: Number of epochs since last loss improvement.\n        best_score: Best validation loss seen so far.\n        threshold: Minimum loss improvement to reset counter.\n        threshold_mode: Either 'abs' (absolute threshold) or 'rel' (relative threshold).\n        enabled: If False, scheduler is disabled and returns None for capacity.\n        reached_max_capacity: True if capacity has reached capacity_max.\n    \"\"\"\n\n    def __init__(\n        self, \n        patience: int, \n        capacity_start: float, \n        capacity_max: float, \n        capacity_increment: float, \n        capacity_increment_mode: str, \n        threshold: float, \n        threshold_mode: str, \n        trace_func: Callable = logging.info, \n        enabled: bool = True\n    ):\n        \"\"\"Initialize the capacity scheduler.\n\n        Args:\n            patience: Number of epochs to wait after last validation loss improvement\n                before increasing capacity.\n            capacity_start: Initial KL divergence capacity target.\n            capacity_max: Maximum capacity value (caps further increases).\n            capacity_increment: Amount to change capacity when triggered.\n            capacity_increment_mode: Either 'abs' (additive: capacity += increment) or \n                'rel' (multiplicative: capacity *= increment).\n            threshold: Minimum change in validation loss to qualify as improvement.\n            threshold_mode: Either 'abs' (absolute: loss &lt; best - threshold) or \n                'rel' (relative: loss &lt; best * (1 - threshold)).\n            trace_func: Logging function for status messages (default: logging.info).\n            enabled: If False, scheduler is disabled and get_capacity() returns None.\n\n        Raises:\n            AssertionError: If capacity_increment_mode or threshold_mode are invalid.\n        \"\"\"\n        self.patience = patience\n        self.capacity = capacity_start\n        self.capacity_max = capacity_max\n        self.capacity_increment = capacity_increment\n        assert capacity_increment_mode in ['abs', 'rel'], 'Invalid capacity increment mode selected.'\n        self.capacity_increment_mode = capacity_increment_mode\n        self.counter = 0\n        self.best_score = np.inf\n        self.corresponding_score = None\n        self.early_stop = False\n        self.threshold = threshold\n        self.threshold_mode = threshold_mode\n        assert threshold_mode in ['abs', 'rel'], 'Invalid threshold mode selected.'\n        self.trace_func = trace_func\n        self.enabled = enabled\n        self.reached_max_capacity = False\n        self.trace_func('CapacityScheduler initialized with patience = {}, capacity = {}, capacity_increment = {}, capacity_increment_mode = {}, threshold = {}, threshold_mode = {}'.format(\n            self.patience, self.capacity, self.capacity_increment, self.capacity_increment_mode, self.threshold, self.threshold_mode))\n        if not self.enabled:\n            self.trace_func('CapacityScheduler disabled.')\n\n    def update(self, score: float):\n        \"\"\"Update scheduler state based on current validation loss.\n\n        Tracks validation loss improvements and increases capacity when loss\n        plateaus for 'patience' epochs. Capacity increases until reaching\n        capacity_max.\n\n        Args:\n            score: Current validation loss (typically MSE loss).\n\n        Side Effects:\n            - Updates counter and best_score\n            - Increases capacity if patience threshold reached\n            - Sets reached_max_capacity flag when maximum is hit\n            - Logs capacity changes via trace_func\n        \"\"\"\n        if self.enabled and not self.reached_max_capacity:\n            _update_flag = False\n\n            if self.threshold_mode == 'abs':\n                if score &lt; self.best_score - self.threshold:\n                    _update_flag = True\n            elif self.threshold_mode == 'rel':\n                if score &lt; self.best_score * (1 - self.threshold):\n                    _update_flag = True\n\n            if _update_flag:\n                self.best_score = score\n                self.counter = 0\n            else:\n                self.counter += 1\n\n            if self.counter &gt;= self.patience:\n                if self.reached_max_capacity is False:\n                    # update capacity\n                    if self.capacity_increment_mode == 'abs':\n                        new_capacity = self.capacity + self.capacity_increment\n                    elif self.capacity_increment_mode == 'rel':\n                        new_capacity = self.capacity * self.capacity_increment\n                    if new_capacity &gt; self.capacity_max:\n                        new_capacity = self.capacity_max\n                        self.reached_max_capacity = True\n                        self.trace_func('\\tCapacityScheduler reached maximum capacity of {}.'.format(self.capacity_max))\n                    self.capacity = new_capacity\n\n                    self.trace_func('\\tCapacityScheduler updated capacity to {} after {} epochs.'.format(self.capacity, self.counter))\n                    self.counter = 0\n\n    def get_capacity(self) -&gt; Optional[float]:\n        \"\"\"Get current capacity target for KL divergence loss.\n\n        Returns:\n            Current capacity value (float) if enabled, None if disabled.\n        \"\"\"\n        return self.capacity if self.enabled else None\n</code></pre>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.capacity_scheduler.capacity_scheduler.__init__","title":"<code>__init__(patience: int, capacity_start: float, capacity_max: float, capacity_increment: float, capacity_increment_mode: str, threshold: float, threshold_mode: str, trace_func: Callable = logging.info, enabled: bool = True)</code>","text":"<p>Initialize the capacity scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>patience</code> <code>int</code> <p>Number of epochs to wait after last validation loss improvement before increasing capacity.</p> required <code>capacity_start</code> <code>float</code> <p>Initial KL divergence capacity target.</p> required <code>capacity_max</code> <code>float</code> <p>Maximum capacity value (caps further increases).</p> required <code>capacity_increment</code> <code>float</code> <p>Amount to change capacity when triggered.</p> required <code>capacity_increment_mode</code> <code>str</code> <p>Either 'abs' (additive: capacity += increment) or  'rel' (multiplicative: capacity *= increment).</p> required <code>threshold</code> <code>float</code> <p>Minimum change in validation loss to qualify as improvement.</p> required <code>threshold_mode</code> <code>str</code> <p>Either 'abs' (absolute: loss &lt; best - threshold) or  'rel' (relative: loss &lt; best * (1 - threshold)).</p> required <code>trace_func</code> <code>Callable</code> <p>Logging function for status messages (default: logging.info).</p> <code>info</code> <code>enabled</code> <code>bool</code> <p>If False, scheduler is disabled and get_capacity() returns None.</p> <code>True</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If capacity_increment_mode or threshold_mode are invalid.</p> Source code in <code>src/bnode_core/nn/nn_utils/capacity_scheduler.py</code> <pre><code>def __init__(\n    self, \n    patience: int, \n    capacity_start: float, \n    capacity_max: float, \n    capacity_increment: float, \n    capacity_increment_mode: str, \n    threshold: float, \n    threshold_mode: str, \n    trace_func: Callable = logging.info, \n    enabled: bool = True\n):\n    \"\"\"Initialize the capacity scheduler.\n\n    Args:\n        patience: Number of epochs to wait after last validation loss improvement\n            before increasing capacity.\n        capacity_start: Initial KL divergence capacity target.\n        capacity_max: Maximum capacity value (caps further increases).\n        capacity_increment: Amount to change capacity when triggered.\n        capacity_increment_mode: Either 'abs' (additive: capacity += increment) or \n            'rel' (multiplicative: capacity *= increment).\n        threshold: Minimum change in validation loss to qualify as improvement.\n        threshold_mode: Either 'abs' (absolute: loss &lt; best - threshold) or \n            'rel' (relative: loss &lt; best * (1 - threshold)).\n        trace_func: Logging function for status messages (default: logging.info).\n        enabled: If False, scheduler is disabled and get_capacity() returns None.\n\n    Raises:\n        AssertionError: If capacity_increment_mode or threshold_mode are invalid.\n    \"\"\"\n    self.patience = patience\n    self.capacity = capacity_start\n    self.capacity_max = capacity_max\n    self.capacity_increment = capacity_increment\n    assert capacity_increment_mode in ['abs', 'rel'], 'Invalid capacity increment mode selected.'\n    self.capacity_increment_mode = capacity_increment_mode\n    self.counter = 0\n    self.best_score = np.inf\n    self.corresponding_score = None\n    self.early_stop = False\n    self.threshold = threshold\n    self.threshold_mode = threshold_mode\n    assert threshold_mode in ['abs', 'rel'], 'Invalid threshold mode selected.'\n    self.trace_func = trace_func\n    self.enabled = enabled\n    self.reached_max_capacity = False\n    self.trace_func('CapacityScheduler initialized with patience = {}, capacity = {}, capacity_increment = {}, capacity_increment_mode = {}, threshold = {}, threshold_mode = {}'.format(\n        self.patience, self.capacity, self.capacity_increment, self.capacity_increment_mode, self.threshold, self.threshold_mode))\n    if not self.enabled:\n        self.trace_func('CapacityScheduler disabled.')\n</code></pre>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.capacity_scheduler.capacity_scheduler.update","title":"<code>update(score: float)</code>","text":"<p>Update scheduler state based on current validation loss.</p> <p>Tracks validation loss improvements and increases capacity when loss plateaus for 'patience' epochs. Capacity increases until reaching capacity_max.</p> <p>Parameters:</p> Name Type Description Default <code>score</code> <code>float</code> <p>Current validation loss (typically MSE loss).</p> required Side Effects <ul> <li>Updates counter and best_score</li> <li>Increases capacity if patience threshold reached</li> <li>Sets reached_max_capacity flag when maximum is hit</li> <li>Logs capacity changes via trace_func</li> </ul> Source code in <code>src/bnode_core/nn/nn_utils/capacity_scheduler.py</code> <pre><code>def update(self, score: float):\n    \"\"\"Update scheduler state based on current validation loss.\n\n    Tracks validation loss improvements and increases capacity when loss\n    plateaus for 'patience' epochs. Capacity increases until reaching\n    capacity_max.\n\n    Args:\n        score: Current validation loss (typically MSE loss).\n\n    Side Effects:\n        - Updates counter and best_score\n        - Increases capacity if patience threshold reached\n        - Sets reached_max_capacity flag when maximum is hit\n        - Logs capacity changes via trace_func\n    \"\"\"\n    if self.enabled and not self.reached_max_capacity:\n        _update_flag = False\n\n        if self.threshold_mode == 'abs':\n            if score &lt; self.best_score - self.threshold:\n                _update_flag = True\n        elif self.threshold_mode == 'rel':\n            if score &lt; self.best_score * (1 - self.threshold):\n                _update_flag = True\n\n        if _update_flag:\n            self.best_score = score\n            self.counter = 0\n        else:\n            self.counter += 1\n\n        if self.counter &gt;= self.patience:\n            if self.reached_max_capacity is False:\n                # update capacity\n                if self.capacity_increment_mode == 'abs':\n                    new_capacity = self.capacity + self.capacity_increment\n                elif self.capacity_increment_mode == 'rel':\n                    new_capacity = self.capacity * self.capacity_increment\n                if new_capacity &gt; self.capacity_max:\n                    new_capacity = self.capacity_max\n                    self.reached_max_capacity = True\n                    self.trace_func('\\tCapacityScheduler reached maximum capacity of {}.'.format(self.capacity_max))\n                self.capacity = new_capacity\n\n                self.trace_func('\\tCapacityScheduler updated capacity to {} after {} epochs.'.format(self.capacity, self.counter))\n                self.counter = 0\n</code></pre>"},{"location":"bnode_core/nn/nn_utils/#bnode_core.nn.nn_utils.capacity_scheduler.capacity_scheduler.get_capacity","title":"<code>get_capacity() -&gt; Optional[float]</code>","text":"<p>Get current capacity target for KL divergence loss.</p> <p>Returns:</p> Type Description <code>Optional[float]</code> <p>Current capacity value (float) if enabled, None if disabled.</p> Source code in <code>src/bnode_core/nn/nn_utils/capacity_scheduler.py</code> <pre><code>def get_capacity(self) -&gt; Optional[float]:\n    \"\"\"Get current capacity target for KL divergence loss.\n\n    Returns:\n        Current capacity value (float) if enabled, None if disabled.\n    \"\"\"\n    return self.capacity if self.enabled else None\n</code></pre>"},{"location":"bnode_core/nn/vae/","title":"VAE (Variational Autoencoder)","text":""},{"location":"bnode_core/nn/vae/#bnode_core.nn.vae.vae_architecture","title":"<code>bnode_core.nn.vae.vae_architecture</code>","text":"<p>Variational Autoencoder (VAE) architecture for timeseries reconstruction.</p> <p>This module implements a Variational Autoencoder with parameter conditioning for  timeseries data (states and outputs). The architecture supports multiple modes:</p> <ul> <li>Standard VAE: Encoder-Decoder with latent space</li> <li>PELS-VAE: Parameter-conditioned VAE with Regressor for mu/logvar prediction</li> <li>Feed-forward NN: Direct mapping from parameters to timeseries (bypasses latent space)</li> </ul> <p>The model can reconstruct timeseries from either the encoder (during training) or from the regressor (during testing/prediction), enabling parameter-conditioned generation.</p> <p>It is intedend to be used for task that model <code>physical parameters --&gt; complete timeseries</code>, e.g. the transient response of a RC circuit with fixed initial condition on different parameter values <code>R,L,C</code>.</p> Attention <p>This documentation is AI generated. Be aware of possible inaccuricies.</p> <p>Key components:</p> <pre><code>- Encoder: Maps timeseries (states + outputs) to latent distribution (mu, logvar)\n- Decoder: Maps latent samples (and optionally parameters) to reconstructed timeseries\n- Regressor: Maps parameters to latent distribution for parameter-conditioned generation\n- Normalization: Time-series and parameter normalization layers\n</code></pre> <p>Loss function:</p> <pre><code>loss = mse_loss + beta * kl_loss + regressor_loss\nor with capacity scheduling:\nloss = mse_loss + gamma * |kl_loss - capacity| + regressor_loss\n</code></pre>"},{"location":"bnode_core/nn/vae/#bnode_core.nn.vae.vae_architecture.Encoder","title":"<code>Encoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Encoder network mapping timeseries to latent distribution parameters.</p> <p>Maps concatenated states and outputs to mean (mu) and log-variance (logvar)  of a multivariate Gaussian distribution in latent space. Uses a multi-layer  perceptron (MLP) with configurable depth and hidden dimensions.</p> <p>Architecture:</p> <pre><code>Flatten -&gt; Linear(n_channels*seq_len, hidden_dim) -&gt; Activation\n-&gt; [Linear(hidden_dim, hidden_dim) -&gt; Activation] x (n_layers-2)\n-&gt; Linear(hidden_dim, 2*bottleneck_dim) -&gt; Reshape to [mu, logvar]\n</code></pre> <p>Attributes:</p> Name Type Description <code>bottleneck_dim</code> <p>Dimensionality of the latent space.</p> <code>flatten</code> <p>Flattens input timeseries to 1D.</p> <code>linear</code> <p>Sequential MLP mapping flattened input to 2*bottleneck_dim outputs.</p> Source code in <code>src/bnode_core/nn/vae/vae_architecture.py</code> <pre><code>class Encoder(nn.Module):\n    \"\"\"Encoder network mapping timeseries to latent distribution parameters.\n\n    Maps concatenated states and outputs to mean (mu) and log-variance (logvar) \n    of a multivariate Gaussian distribution in latent space. Uses a multi-layer \n    perceptron (MLP) with configurable depth and hidden dimensions.\n\n    Architecture:\n\n        Flatten -&gt; Linear(n_channels*seq_len, hidden_dim) -&gt; Activation\n        -&gt; [Linear(hidden_dim, hidden_dim) -&gt; Activation] x (n_layers-2)\n        -&gt; Linear(hidden_dim, 2*bottleneck_dim) -&gt; Reshape to [mu, logvar]\n\n    Attributes:\n        bottleneck_dim: Dimensionality of the latent space.\n        flatten: Flattens input timeseries to 1D.\n        linear: Sequential MLP mapping flattened input to 2*bottleneck_dim outputs.\n    \"\"\"\n\n    def __init__(self, n_channels: int, seq_len: int, hidden_dim: int, bottleneck_dim: int,\n                 activation: nn.Module = nn.ReLU, n_layers: int = 3):\n        \"\"\"Initialize the Encoder network.\n\n        Args:\n            n_channels: Number of input channels (states + outputs concatenated).\n            seq_len: Length of the timeseries sequence.\n            hidden_dim: Number of hidden units in intermediate layers.\n            bottleneck_dim: Dimensionality of latent space (output is 2*bottleneck_dim for mu and logvar).\n            activation: Activation function class (default: nn.ReLU).\n            n_layers: Total number of linear layers (minimum 2, includes input and output layers).\n        \"\"\"\n        super().__init__()\n        # save dimensions of output\n        self.bottleneck_dim = bottleneck_dim\n\n        self.flatten = nn.Flatten()\n\n        # construct MLP\n        modules = [\n            nn.Linear(n_channels*seq_len, hidden_dim),\n            activation(),\n        ]\n        if n_layers &lt; 2:\n            logging.warning('n_layers must be at least 2, setting n_layers to 2')\n        for i in range(n_layers-2):\n            modules.append(nn.Linear(hidden_dim, hidden_dim))\n            modules.append(activation())\n        modules.append(nn.Linear(hidden_dim, 2*bottleneck_dim))\n        self.linear = nn.Sequential(*modules)  \n\n    def forward(self, x: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Encode timeseries to latent distribution parameters.\n\n        Args:\n            x: Input timeseries tensor of shape (batch, n_channels, seq_len).\n\n        Returns:\n            Tuple of (mu, logvar) where:\n\n                - mu: Mean of latent distribution, shape (batch, bottleneck_dim)\n                - logvar: Log-variance of latent distribution, shape (batch, bottleneck_dim)\n        \"\"\"\n        x = self.flatten(x)\n        latent = self.linear(x)\n        latent = torch.reshape(latent, (-1, 2, self.bottleneck_dim))\n        mu, logvar = latent[:,0], latent[:,1]\n        return mu, logvar\n</code></pre>"},{"location":"bnode_core/nn/vae/#bnode_core.nn.vae.vae_architecture.Encoder.__init__","title":"<code>__init__(n_channels: int, seq_len: int, hidden_dim: int, bottleneck_dim: int, activation: nn.Module = nn.ReLU, n_layers: int = 3)</code>","text":"<p>Initialize the Encoder network.</p> <p>Parameters:</p> Name Type Description Default <code>n_channels</code> <code>int</code> <p>Number of input channels (states + outputs concatenated).</p> required <code>seq_len</code> <code>int</code> <p>Length of the timeseries sequence.</p> required <code>hidden_dim</code> <code>int</code> <p>Number of hidden units in intermediate layers.</p> required <code>bottleneck_dim</code> <code>int</code> <p>Dimensionality of latent space (output is 2*bottleneck_dim for mu and logvar).</p> required <code>activation</code> <code>Module</code> <p>Activation function class (default: nn.ReLU).</p> <code>ReLU</code> <code>n_layers</code> <code>int</code> <p>Total number of linear layers (minimum 2, includes input and output layers).</p> <code>3</code> Source code in <code>src/bnode_core/nn/vae/vae_architecture.py</code> <pre><code>def __init__(self, n_channels: int, seq_len: int, hidden_dim: int, bottleneck_dim: int,\n             activation: nn.Module = nn.ReLU, n_layers: int = 3):\n    \"\"\"Initialize the Encoder network.\n\n    Args:\n        n_channels: Number of input channels (states + outputs concatenated).\n        seq_len: Length of the timeseries sequence.\n        hidden_dim: Number of hidden units in intermediate layers.\n        bottleneck_dim: Dimensionality of latent space (output is 2*bottleneck_dim for mu and logvar).\n        activation: Activation function class (default: nn.ReLU).\n        n_layers: Total number of linear layers (minimum 2, includes input and output layers).\n    \"\"\"\n    super().__init__()\n    # save dimensions of output\n    self.bottleneck_dim = bottleneck_dim\n\n    self.flatten = nn.Flatten()\n\n    # construct MLP\n    modules = [\n        nn.Linear(n_channels*seq_len, hidden_dim),\n        activation(),\n    ]\n    if n_layers &lt; 2:\n        logging.warning('n_layers must be at least 2, setting n_layers to 2')\n    for i in range(n_layers-2):\n        modules.append(nn.Linear(hidden_dim, hidden_dim))\n        modules.append(activation())\n    modules.append(nn.Linear(hidden_dim, 2*bottleneck_dim))\n    self.linear = nn.Sequential(*modules)  \n</code></pre>"},{"location":"bnode_core/nn/vae/#bnode_core.nn.vae.vae_architecture.Encoder.forward","title":"<code>forward(x: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]</code>","text":"<p>Encode timeseries to latent distribution parameters.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input timeseries tensor of shape (batch, n_channels, seq_len).</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Tuple of (mu, logvar) where:</p> <ul> <li>mu: Mean of latent distribution, shape (batch, bottleneck_dim)</li> <li>logvar: Log-variance of latent distribution, shape (batch, bottleneck_dim)</li> </ul> Source code in <code>src/bnode_core/nn/vae/vae_architecture.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Encode timeseries to latent distribution parameters.\n\n    Args:\n        x: Input timeseries tensor of shape (batch, n_channels, seq_len).\n\n    Returns:\n        Tuple of (mu, logvar) where:\n\n            - mu: Mean of latent distribution, shape (batch, bottleneck_dim)\n            - logvar: Log-variance of latent distribution, shape (batch, bottleneck_dim)\n    \"\"\"\n    x = self.flatten(x)\n    latent = self.linear(x)\n    latent = torch.reshape(latent, (-1, 2, self.bottleneck_dim))\n    mu, logvar = latent[:,0], latent[:,1]\n    return mu, logvar\n</code></pre>"},{"location":"bnode_core/nn/vae/#bnode_core.nn.vae.vae_architecture.Decoder","title":"<code>Decoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Decoder network for VAE, generating timeseries from latent vectors.</p> <p>The decoder maps latent vectors (and optionally system parameters) back to timeseries data. It supports three modes:</p> <ul> <li>Standard VAE: z_latent \u2192 timeseries</li> <li>PELS-VAE: (z_latent, parameters) \u2192 timeseries (params_to_decoder=True)</li> <li>Feed-forward: parameters \u2192 timeseries (bottleneck_dim=0, params_to_decoder=True)</li> </ul> <p>Architecture: Linear (latent+params \u2192 hidden) \u2192 MLP \u2192 Linear (hidden \u2192 n_channels*seq_len) \u2192 Reshape</p> <p>Attributes:</p> Name Type Description <code>channels</code> <p>Number of output channels in reconstructed timeseries.</p> <code>seq_len</code> <p>Length of output timeseries sequence.</p> <code>params_to_decoder</code> <p>If True, concatenate normalized parameters to latent vector as decoder input.</p> <code>param_normalization</code> <p>Normalization layer for parameters (if params_to_decoder=True).</p> <code>feed_forward_nn</code> <p>If True, decoder operates in feed-forward mode (no latent vector).</p> <code>linear</code> <p>Sequential MLP mapping latent (+ params) to flattened timeseries.</p> Source code in <code>src/bnode_core/nn/vae/vae_architecture.py</code> <pre><code>class Decoder(nn.Module):\n    \"\"\"Decoder network for VAE, generating timeseries from latent vectors.\n\n    The decoder maps latent vectors (and optionally system parameters) back to timeseries\n    data. It supports three modes:\n\n    - Standard VAE: z_latent \u2192 timeseries\n    - PELS-VAE: (z_latent, parameters) \u2192 timeseries (params_to_decoder=True)\n    - Feed-forward: parameters \u2192 timeseries (bottleneck_dim=0, params_to_decoder=True)\n\n    Architecture: Linear (latent+params \u2192 hidden) \u2192 MLP \u2192 Linear (hidden \u2192 n_channels*seq_len) \u2192 Reshape\n\n    Attributes:\n        channels: Number of output channels in reconstructed timeseries.\n        seq_len: Length of output timeseries sequence.\n        params_to_decoder: If True, concatenate normalized parameters to latent vector as decoder input.\n        param_normalization: Normalization layer for parameters (if params_to_decoder=True).\n        feed_forward_nn: If True, decoder operates in feed-forward mode (no latent vector).\n        linear: Sequential MLP mapping latent (+ params) to flattened timeseries.\n    \"\"\"\n\n    def __init__(self, n_channels: int, seq_len: int, hidden_dim: int, bottleneck_dim: int,\n                 activation: nn.Module = nn.ReLU, n_layers: int = 3, params_to_decoder: bool = False, \n                 param_dim: Optional[int] = None):\n        \"\"\"Initialize the Decoder network.\n\n        Args:\n            n_channels: Number of output channels in reconstructed timeseries.\n            seq_len: Length of output timeseries sequence.\n            hidden_dim: Number of hidden units in intermediate layers.\n            bottleneck_dim: Dimensionality of latent space input (0 for feed-forward mode).\n            activation: Activation function class (default: nn.ReLU).\n            n_layers: Total number of linear layers (minimum 2).\n            params_to_decoder: If True, concatenate system parameters to latent input (PELS-VAE mode).\n            param_dim: Dimensionality of parameter vector (required if params_to_decoder=True).\n        \"\"\"\n        super().__init__()\n\n        # save dimensions of output\n        self.channels = n_channels\n        self.seq_len = seq_len\n        self.params_to_decoder = params_to_decoder\n        if params_to_decoder:\n            assert param_dim is not None, 'param_dim must be specified if params_to_decoder is True'\n            self.param_normalization = NormalizationLayer1D(num_features=param_dim)\n\n        self.feed_forward_nn = True if bottleneck_dim == 0 else False\n        if self.feed_forward_nn:\n            assert params_to_decoder is True, 'params_to_decoder must be True if bottleneck_dim is 0'\n        # construct MLP\n        modules = [\n            nn.Linear(bottleneck_dim if params_to_decoder is False else bottleneck_dim + param_dim, hidden_dim),\n            activation(),\n        ]\n        if n_layers &lt; 2:\n            logging.warning('n_layers must be at least 2, setting n_layers to 2')\n        for i in range(n_layers-2):\n            modules.append(nn.Linear(hidden_dim, hidden_dim))\n            modules.append(activation())\n        modules.append(nn.Linear(hidden_dim, n_channels*seq_len))\n        self.linear = nn.Sequential(*modules)  \n\n    def forward(self, z_latent: torch.Tensor, param: Optional[torch.Tensor] = None) -&gt; torch.Tensor:\n        \"\"\"Decode latent vector (and optionally parameters) to timeseries.\n\n        Args:\n            z_latent: Latent vector of shape (batch, bottleneck_dim).\n            param: System parameters of shape (batch, param_dim) (required if params_to_decoder=True).\n\n        Returns:\n            Reconstructed timeseries tensor of shape (batch, n_channels, seq_len).\n        \"\"\"\n        if self.params_to_decoder:\n            param = self.param_normalization(param)\n            x = self.linear(torch.cat((z_latent, param), dim=1)) if not self.feed_forward_nn else self.linear(param)\n        else:\n            x = self.linear(z_latent)\n        x = torch.reshape(x,(-1, self.channels, self.seq_len))\n        return x\n</code></pre>"},{"location":"bnode_core/nn/vae/#bnode_core.nn.vae.vae_architecture.Decoder.__init__","title":"<code>__init__(n_channels: int, seq_len: int, hidden_dim: int, bottleneck_dim: int, activation: nn.Module = nn.ReLU, n_layers: int = 3, params_to_decoder: bool = False, param_dim: Optional[int] = None)</code>","text":"<p>Initialize the Decoder network.</p> <p>Parameters:</p> Name Type Description Default <code>n_channels</code> <code>int</code> <p>Number of output channels in reconstructed timeseries.</p> required <code>seq_len</code> <code>int</code> <p>Length of output timeseries sequence.</p> required <code>hidden_dim</code> <code>int</code> <p>Number of hidden units in intermediate layers.</p> required <code>bottleneck_dim</code> <code>int</code> <p>Dimensionality of latent space input (0 for feed-forward mode).</p> required <code>activation</code> <code>Module</code> <p>Activation function class (default: nn.ReLU).</p> <code>ReLU</code> <code>n_layers</code> <code>int</code> <p>Total number of linear layers (minimum 2).</p> <code>3</code> <code>params_to_decoder</code> <code>bool</code> <p>If True, concatenate system parameters to latent input (PELS-VAE mode).</p> <code>False</code> <code>param_dim</code> <code>Optional[int]</code> <p>Dimensionality of parameter vector (required if params_to_decoder=True).</p> <code>None</code> Source code in <code>src/bnode_core/nn/vae/vae_architecture.py</code> <pre><code>def __init__(self, n_channels: int, seq_len: int, hidden_dim: int, bottleneck_dim: int,\n             activation: nn.Module = nn.ReLU, n_layers: int = 3, params_to_decoder: bool = False, \n             param_dim: Optional[int] = None):\n    \"\"\"Initialize the Decoder network.\n\n    Args:\n        n_channels: Number of output channels in reconstructed timeseries.\n        seq_len: Length of output timeseries sequence.\n        hidden_dim: Number of hidden units in intermediate layers.\n        bottleneck_dim: Dimensionality of latent space input (0 for feed-forward mode).\n        activation: Activation function class (default: nn.ReLU).\n        n_layers: Total number of linear layers (minimum 2).\n        params_to_decoder: If True, concatenate system parameters to latent input (PELS-VAE mode).\n        param_dim: Dimensionality of parameter vector (required if params_to_decoder=True).\n    \"\"\"\n    super().__init__()\n\n    # save dimensions of output\n    self.channels = n_channels\n    self.seq_len = seq_len\n    self.params_to_decoder = params_to_decoder\n    if params_to_decoder:\n        assert param_dim is not None, 'param_dim must be specified if params_to_decoder is True'\n        self.param_normalization = NormalizationLayer1D(num_features=param_dim)\n\n    self.feed_forward_nn = True if bottleneck_dim == 0 else False\n    if self.feed_forward_nn:\n        assert params_to_decoder is True, 'params_to_decoder must be True if bottleneck_dim is 0'\n    # construct MLP\n    modules = [\n        nn.Linear(bottleneck_dim if params_to_decoder is False else bottleneck_dim + param_dim, hidden_dim),\n        activation(),\n    ]\n    if n_layers &lt; 2:\n        logging.warning('n_layers must be at least 2, setting n_layers to 2')\n    for i in range(n_layers-2):\n        modules.append(nn.Linear(hidden_dim, hidden_dim))\n        modules.append(activation())\n    modules.append(nn.Linear(hidden_dim, n_channels*seq_len))\n    self.linear = nn.Sequential(*modules)  \n</code></pre>"},{"location":"bnode_core/nn/vae/#bnode_core.nn.vae.vae_architecture.Decoder.forward","title":"<code>forward(z_latent: torch.Tensor, param: Optional[torch.Tensor] = None) -&gt; torch.Tensor</code>","text":"<p>Decode latent vector (and optionally parameters) to timeseries.</p> <p>Parameters:</p> Name Type Description Default <code>z_latent</code> <code>Tensor</code> <p>Latent vector of shape (batch, bottleneck_dim).</p> required <code>param</code> <code>Optional[Tensor]</code> <p>System parameters of shape (batch, param_dim) (required if params_to_decoder=True).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Reconstructed timeseries tensor of shape (batch, n_channels, seq_len).</p> Source code in <code>src/bnode_core/nn/vae/vae_architecture.py</code> <pre><code>def forward(self, z_latent: torch.Tensor, param: Optional[torch.Tensor] = None) -&gt; torch.Tensor:\n    \"\"\"Decode latent vector (and optionally parameters) to timeseries.\n\n    Args:\n        z_latent: Latent vector of shape (batch, bottleneck_dim).\n        param: System parameters of shape (batch, param_dim) (required if params_to_decoder=True).\n\n    Returns:\n        Reconstructed timeseries tensor of shape (batch, n_channels, seq_len).\n    \"\"\"\n    if self.params_to_decoder:\n        param = self.param_normalization(param)\n        x = self.linear(torch.cat((z_latent, param), dim=1)) if not self.feed_forward_nn else self.linear(param)\n    else:\n        x = self.linear(z_latent)\n    x = torch.reshape(x,(-1, self.channels, self.seq_len))\n    return x\n</code></pre>"},{"location":"bnode_core/nn/vae/#bnode_core.nn.vae.vae_architecture.Regressor","title":"<code>Regressor</code>","text":"<p>               Bases: <code>Module</code></p> <p>Regressor network mapping system parameters to latent distribution.</p> <p>Used in PELS-VAE mode to predict latent distribution parameters (mu, logvar)  directly from system parameters, without requiring timeseries input. This allows the VAE to learn relationships between system parameters and latent representations.</p> <p>Architecture: </p> <pre><code>Normalize params \u2192 Linear (params \u2192 hidden) \u2192 MLP \u2192 Linear (hidden \u2192 2*bottleneck_dim) \u2192 Reshape to (mu, logvar)\n</code></pre> <p>Attributes:</p> Name Type Description <code>bottleneck_dim</code> <p>Dimensionality of the latent space.</p> <code>normalization</code> <p>Normalization layer for input parameters.</p> <code>linear</code> <p>Sequential MLP mapping parameters to 2*bottleneck_dim outputs.</p> Source code in <code>src/bnode_core/nn/vae/vae_architecture.py</code> <pre><code>class Regressor(nn.Module):\n    \"\"\"Regressor network mapping system parameters to latent distribution.\n\n    Used in PELS-VAE mode to predict latent distribution parameters (mu, logvar) \n    directly from system parameters, without requiring timeseries input. This allows\n    the VAE to learn relationships between system parameters and latent representations.\n\n    Architecture: \n\n        Normalize params \u2192 Linear (params \u2192 hidden) \u2192 MLP \u2192 Linear (hidden \u2192 2*bottleneck_dim) \u2192 Reshape to (mu, logvar)\n\n    Attributes:\n        bottleneck_dim: Dimensionality of the latent space.\n        normalization: Normalization layer for input parameters.\n        linear: Sequential MLP mapping parameters to 2*bottleneck_dim outputs.\n    \"\"\"\n\n    def __init__(self, parameter_dim: int, hidden_dim: int, \n                 bottleneck_dim: int, activation: nn.Module = nn.ReLU, \n                 n_layers: int = 3):\n        \"\"\"Initialize the Regressor network.\n\n        Args:\n            parameter_dim: Dimensionality of input parameter vector.\n            hidden_dim: Number of hidden units in intermediate layers.\n            bottleneck_dim: Dimensionality of latent space (output is 2*bottleneck_dim for mu and logvar).\n            activation: Activation function class (default: nn.ReLU).\n            n_layers: Total number of linear layers (minimum 2).\n        \"\"\"\n        super().__init__()\n        # save dimensions of output\n        self.bottleneck_dim = bottleneck_dim\n\n        self.normalization = NormalizationLayer1D(num_features=parameter_dim)\n\n        # construct MLP\n        modules = [\n            nn.Linear(parameter_dim, hidden_dim),\n            activation(),\n        ]\n        if n_layers &lt; 2:\n            logging.warning('n_layers must be at least 2, setting n_layers to 2')\n        for i in range(n_layers-2):\n            modules.append(nn.Linear(hidden_dim, hidden_dim))\n            modules.append(activation())\n        modules.append(nn.Linear(hidden_dim, 2*bottleneck_dim))\n        self.linear = nn.Sequential(*modules)\n\n    def forward(self, param: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Predict latent distribution parameters from system parameters.\n\n        Args:\n            param: System parameters of shape (batch, parameter_dim).\n\n        Returns:\n            Tuple of (mu, logvar) where:\n                - mu: Predicted mean of latent distribution, shape (batch, bottleneck_dim)\n                - logvar: Predicted log-variance of latent distribution, shape (batch, bottleneck_dim)\n        \"\"\"\n        param = self.normalization(param)\n        latent = self.linear(param)\n        latent = torch.reshape(latent,(-1, 2, self.bottleneck_dim))\n        mu, logvar = latent[:,0], latent[:,1]\n        return mu, logvar\n</code></pre>"},{"location":"bnode_core/nn/vae/#bnode_core.nn.vae.vae_architecture.Regressor.__init__","title":"<code>__init__(parameter_dim: int, hidden_dim: int, bottleneck_dim: int, activation: nn.Module = nn.ReLU, n_layers: int = 3)</code>","text":"<p>Initialize the Regressor network.</p> <p>Parameters:</p> Name Type Description Default <code>parameter_dim</code> <code>int</code> <p>Dimensionality of input parameter vector.</p> required <code>hidden_dim</code> <code>int</code> <p>Number of hidden units in intermediate layers.</p> required <code>bottleneck_dim</code> <code>int</code> <p>Dimensionality of latent space (output is 2*bottleneck_dim for mu and logvar).</p> required <code>activation</code> <code>Module</code> <p>Activation function class (default: nn.ReLU).</p> <code>ReLU</code> <code>n_layers</code> <code>int</code> <p>Total number of linear layers (minimum 2).</p> <code>3</code> Source code in <code>src/bnode_core/nn/vae/vae_architecture.py</code> <pre><code>def __init__(self, parameter_dim: int, hidden_dim: int, \n             bottleneck_dim: int, activation: nn.Module = nn.ReLU, \n             n_layers: int = 3):\n    \"\"\"Initialize the Regressor network.\n\n    Args:\n        parameter_dim: Dimensionality of input parameter vector.\n        hidden_dim: Number of hidden units in intermediate layers.\n        bottleneck_dim: Dimensionality of latent space (output is 2*bottleneck_dim for mu and logvar).\n        activation: Activation function class (default: nn.ReLU).\n        n_layers: Total number of linear layers (minimum 2).\n    \"\"\"\n    super().__init__()\n    # save dimensions of output\n    self.bottleneck_dim = bottleneck_dim\n\n    self.normalization = NormalizationLayer1D(num_features=parameter_dim)\n\n    # construct MLP\n    modules = [\n        nn.Linear(parameter_dim, hidden_dim),\n        activation(),\n    ]\n    if n_layers &lt; 2:\n        logging.warning('n_layers must be at least 2, setting n_layers to 2')\n    for i in range(n_layers-2):\n        modules.append(nn.Linear(hidden_dim, hidden_dim))\n        modules.append(activation())\n    modules.append(nn.Linear(hidden_dim, 2*bottleneck_dim))\n    self.linear = nn.Sequential(*modules)\n</code></pre>"},{"location":"bnode_core/nn/vae/#bnode_core.nn.vae.vae_architecture.Regressor.forward","title":"<code>forward(param: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]</code>","text":"<p>Predict latent distribution parameters from system parameters.</p> <p>Parameters:</p> Name Type Description Default <code>param</code> <code>Tensor</code> <p>System parameters of shape (batch, parameter_dim).</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Tuple of (mu, logvar) where: - mu: Predicted mean of latent distribution, shape (batch, bottleneck_dim) - logvar: Predicted log-variance of latent distribution, shape (batch, bottleneck_dim)</p> Source code in <code>src/bnode_core/nn/vae/vae_architecture.py</code> <pre><code>def forward(self, param: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Predict latent distribution parameters from system parameters.\n\n    Args:\n        param: System parameters of shape (batch, parameter_dim).\n\n    Returns:\n        Tuple of (mu, logvar) where:\n            - mu: Predicted mean of latent distribution, shape (batch, bottleneck_dim)\n            - logvar: Predicted log-variance of latent distribution, shape (batch, bottleneck_dim)\n    \"\"\"\n    param = self.normalization(param)\n    latent = self.linear(param)\n    latent = torch.reshape(latent,(-1, 2, self.bottleneck_dim))\n    mu, logvar = latent[:,0], latent[:,1]\n    return mu, logvar\n</code></pre>"},{"location":"bnode_core/nn/vae/#bnode_core.nn.vae.vae_architecture.VAE","title":"<code>VAE</code>","text":"<p>               Bases: <code>Module</code></p> <p>Variational Autoencoder for timeseries modeling with parameter conditioning.</p> <p>This class implements three operational modes:</p> <ol> <li>Standard VAE: Encodes timeseries to latent space, decodes back to timeseries.    Uses both Encoder and Regressor to predict latent distributions.</li> <li>PELS-VAE (params_to_decoder=True): Decoder receives both latent vector and     system parameters, allowing parameter-conditioned reconstruction.</li> <li>Feed-forward NN (feed_forward_nn=True): Bypasses latent space entirely,    directly mapping parameters to timeseries outputs.</li> </ol> <p>The model jointly trains:</p> <ul> <li>Encoder: timeseries \u2192 (mu_encoder, logvar_encoder)</li> <li>Regressor: parameters \u2192 (mu_regressor, logvar_regressor)</li> <li>Decoder: latent vector (+ params) \u2192 timeseries</li> </ul> <p>During training, reconstruction uses Encoder's latent distribution. During prediction, reconstruction uses Regressor's latent distribution.</p> <p>Attributes:</p> Name Type Description <code>n_channels</code> <p>Total number of channels (n_states + n_outputs).</p> <code>n_states</code> <p>Number of state channels.</p> <code>n_outputs</code> <p>Number of output channels.</p> <code>timeseries_normalization</code> <p>Normalization layer for timeseries data.</p> <code>feed_forward_nn</code> <p>If True, operates in feed-forward mode (no latent space).</p> <code>Regressor</code> <p>Parameter-to-latent network (if not feed_forward_nn).</p> <code>Encoder</code> <p>Timeseries-to-latent network (if not feed_forward_nn).</p> <code>Decoder</code> <p>Latent-to-timeseries network.</p> Source code in <code>src/bnode_core/nn/vae/vae_architecture.py</code> <pre><code>class VAE(nn.Module):\n    \"\"\"Variational Autoencoder for timeseries modeling with parameter conditioning.\n\n    This class implements three operational modes:\n\n    1. **Standard VAE**: Encodes timeseries to latent space, decodes back to timeseries.\n       Uses both Encoder and Regressor to predict latent distributions.\n    2. **PELS-VAE** (params_to_decoder=True): Decoder receives both latent vector and \n       system parameters, allowing parameter-conditioned reconstruction.\n    3. **Feed-forward NN** (feed_forward_nn=True): Bypasses latent space entirely,\n       directly mapping parameters to timeseries outputs.\n\n    The model jointly trains:\n\n    - Encoder: timeseries \u2192 (mu_encoder, logvar_encoder)\n    - Regressor: parameters \u2192 (mu_regressor, logvar_regressor)\n    - Decoder: latent vector (+ params) \u2192 timeseries\n\n    During training, reconstruction uses Encoder's latent distribution.\n    During prediction, reconstruction uses Regressor's latent distribution.\n\n    Attributes:\n        n_channels: Total number of channels (n_states + n_outputs).\n        n_states: Number of state channels.\n        n_outputs: Number of output channels.\n        timeseries_normalization: Normalization layer for timeseries data.\n        feed_forward_nn: If True, operates in feed-forward mode (no latent space).\n        Regressor: Parameter-to-latent network (if not feed_forward_nn).\n        Encoder: Timeseries-to-latent network (if not feed_forward_nn).\n        Decoder: Latent-to-timeseries network.\n    \"\"\"\n\n    def __init__(self, n_states: int, n_outputs: int, seq_len: int, parameter_dim: int, \n                 hidden_dim: int, bottleneck_dim: int, activation: nn.Module = nn.ReLU, \n                 n_layers: int = 3, params_to_decoder: bool = False, feed_forward_nn: bool = False):\n        \"\"\"Initialize the VAE model.\n\n        Args:\n            n_states: Number of state channels in timeseries.\n            n_outputs: Number of output channels in timeseries.\n            seq_len: Length of timeseries sequence.\n            parameter_dim: Dimensionality of system parameters.\n            hidden_dim: Number of hidden units in all sub-networks.\n            bottleneck_dim: Dimensionality of latent space.\n            activation: Activation function class (default: nn.ReLU).\n            n_layers: Number of layers in all sub-networks (minimum 2).\n            params_to_decoder: If True, decoder receives parameters as additional input (PELS-VAE mode).\n            feed_forward_nn: If True, operate in feed-forward mode without latent space.\n        \"\"\"\n        if feed_forward_nn is True:\n            if params_to_decoder is False:\n                Warning('params_to_decoder is set to False, but feed_forward_nn is set to True. Setting params_to_decoder to True')\n        super().__init__()\n        self.n_channels = n_states + n_outputs\n        self.n_states = n_states\n        self.n_outputs = n_outputs\n        self.timeseries_normalization = NormalizationLayerTimeSeries(n_channels=self.n_channels)\n        self.feed_forward_nn = feed_forward_nn\n\n        if feed_forward_nn is False:\n            self.Regressor = Regressor(parameter_dim, hidden_dim, bottleneck_dim, activation, n_layers)\n            self.Encoder = Encoder(self.n_channels, seq_len, hidden_dim,\n                                bottleneck_dim, activation, n_layers)\n            self.Decoder = Decoder(self.n_channels, seq_len, hidden_dim,\n                                bottleneck_dim, activation, n_layers,\n                                params_to_decoder, parameter_dim)\n        else:\n            _bottleneck_dim = 0\n            _params_to_decoder = True\n            self.Decoder = Decoder(self.n_channels, seq_len, hidden_dim,\n                                   _bottleneck_dim, activation, n_layers,\n                                      _params_to_decoder, parameter_dim)\n        logging.info('VAE with n_channels = {}, seq_len = {}, parameter_dim = {}, \\\n                     hidden_dim = {}, bottleneck_dim = {}, activation = {}, n_layers = {}, params to decoder: {}'.format(\n                         self.n_channels, seq_len, parameter_dim, hidden_dim, bottleneck_dim, activation, n_layers, self.Decoder.params_to_decoder))\n        logging.info('VAE initialized with {} parameters'.format(count_parameters(self)))\n\n    def reparametrize(self, mu: torch.Tensor, logvar: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Apply reparametrization trick to sample from latent distribution.\n\n        Samples z ~ N(mu, exp(0.5 * logvar)) using z = mu + eps * std, where eps ~ N(0, I).\n        This allows backpropagation through the sampling operation.\n\n        Args:\n            mu: Mean of latent distribution, shape (batch, bottleneck_dim).\n            logvar: Log-variance of latent distribution, shape (batch, bottleneck_dim).\n\n\n        Returns:\n            Sampled latent vector z of shape (batch, bottleneck_dim).\n        \"\"\"\n        # if device.type == 'cuda':\n        #     eps = torch.autograd.Variable(torch.cuda.FloatTensor(mu.shape).normal_())\n        # else: \n        #     eps = torch.autograd.Variable(torch.FloatTensor(mu.shape).normal_())\n        eps = torch.randn_like(mu, device=mu.device)\n        std = logvar.mul(0.5).exp()\n        z_latent = eps.mul(std).add_(mu)\n        return z_latent\n\n    def forward(\n        self, \n        states: torch.Tensor, \n        outputs: torch.Tensor, \n        params: torch.Tensor, \n        train: bool = True, \n        predict: bool = False, \n        n_passes: int = 1, \n        test_with_zero_eps: bool = False, \n        device: Optional[torch.device] = None\n    ) -&gt; Tuple:\n        \"\"\"Perform forward pass through the VAE network.\n\n        Three operational modes based on flags:\n\n        1. Training (train=True, predict=False): Encode timeseries, reconstruct using Encoder's latent distribution\n        2. Testing (train=False, predict=False): Encode timeseries, reconstruct using Regressor's latent distribution\n        3. Prediction (predict=True, train=False): Skip Encoder, reconstruct using Regressor's latent distribution only\n\n        Args:\n            states: State timeseries of shape (batch, n_states, seq_len).\n            outputs: Output timeseries of shape (batch, n_outputs, seq_len).\n            params: System parameters of shape (batch, parameter_dim).\n            train: If True, use Encoder's latent distribution for reconstruction.\n            predict: If True, bypass Encoder and reconstruct from parameters only.\n            n_passes: Number of decoder passes to average (for stochastic predictions).\n            test_with_zero_eps: If True during testing, use mu directly (zero variance sampling).\n            device: Device for tensor operations.\n\n        Returns:\n            Tuple of (x, x_hat, states_hat, outputs_hat, mu_encoder, logvar_encoder, \n                     mu_regressor, logvar_regressor, retvals_norm) where:\n\n                - x: Concatenated input timeseries (states + outputs)\n                - x_hat: Reconstructed timeseries\n                - states_hat: Reconstructed states\n                - outputs_hat: Reconstructed outputs\n                - mu_encoder: Encoder's predicted latent mean\n                - logvar_encoder: Encoder's predicted latent log-variance\n                - mu_regressor: Regressor's predicted latent mean\n                - logvar_regressor: Regressor's predicted latent log-variance\n                - retvals_norm: Dictionary of normalized versions of above tensors\n        \"\"\"\n        if self.feed_forward_nn is False:\n            if predict:\n                assert not train, 'predict and train cannot be true at the same time'\n            else:\n                # concatenate states and outputs\n                x = torch.cat((states, outputs), dim=1)\n                x_norm = self.timeseries_normalization(x)\n                states_norm = x_norm[:,:self.n_states]\n                outputs_norm = x_norm[:,self.n_states:]\n                mu_encoder, logvar_encoder = self.Encoder(x_norm)\n            mu_regressor, logvar_regressor = self.Regressor(params)\n            # assign mu, logvar based on if train or not\n            if train:\n                mu = mu_encoder\n                logvar = logvar_encoder\n            else:\n                mu = mu_regressor\n                logvar = logvar_regressor\n            # if predict, we need some dummy values for mu_encoder and logvar_encoder\n            if predict:\n                mu_encoder = torch.ones_like(mu_encoder, device=device) * np.inf\n                logvar_encoder = torch.ones_like(logvar_encoder, device=device) * np.inf\n            # perform multiple passes through decoder\n            x_pass = []\n            x_pass_norm = []\n            for _ in range(n_passes):\n                if train or not test_with_zero_eps:\n                    z_latent = self.reparametrize(mu, logvar)\n                else:\n                    z_latent = mu\n                if self.Decoder.params_to_decoder:\n                    x_i_hat_norm = self.Decoder(z_latent, params)\n                else:\n                    x_i_hat_norm = self.Decoder(z_latent)\n                x_i_hat = self.timeseries_normalization(x_i_hat_norm, denormalize = True)\n                x_pass.append(x_i_hat)\n                x_pass_norm.append(x_i_hat_norm)\n            # stack along new dimension 1 and take mean along that dimension\n            x_hat = torch.stack(x_pass, dim=0).mean(dim=0)\n            x_hat_norm = torch.stack(x_pass_norm, dim=0).mean(dim=0)\n            # unpack x\n            states_hat, outputs_hat = torch.split(x_hat, [self.n_states, self.n_outputs], dim=1)\n            # unpack x_norm\n            states_hat_norm, outputs_hat_norm = torch.split(x_hat_norm, [self.n_states, self.n_outputs], dim=1)\n            retvals_norm = {\n                'x': x_norm,\n                'x_hat': x_hat_norm,\n                'states': states_norm,\n                'outputs': outputs_norm,\n                'states_hat': states_hat_norm,\n                'outputs_hat': outputs_hat_norm,\n            }\n        else:\n            x = torch.cat((states, outputs), dim=1)\n            x_norm = self.timeseries_normalization(x)\n            states_norm = x_norm[:,:self.n_states]\n            outputs_norm = x_norm[:,self.n_states:]\n            x_hat_norm = self.Decoder(None, params)\n            x_hat = self.timeseries_normalization(x_hat_norm, denormalize = True)\n             # unpack x\n            states_hat, outputs_hat = torch.split(x_hat, [self.n_states, self.n_outputs], dim=1)\n            # unpack x_norm\n            states_hat_norm, outputs_hat_norm = torch.split(x_hat_norm, [self.n_states, self.n_outputs], dim=1)\n            retvals_norm = {\n                'x': x_norm,\n                'x_hat': x_hat_norm,\n                'states': states_norm,\n                'outputs': outputs_norm,\n                'states_hat': states_hat_norm,\n                'outputs_hat': outputs_hat_norm,\n            }\n            mu_encoder = torch.ones_like(states_hat_norm, device=device) * np.inf\n            logvar_encoder = torch.ones_like(states_hat_norm, device=device) * np.inf\n            mu_regressor = torch.ones_like(states_hat_norm, device=device) * np.inf\n            logvar_regressor = torch.ones_like(states_hat_norm, device=device) * np.inf\n\n        return x, x_hat, states_hat, outputs_hat, mu_encoder, logvar_encoder, mu_regressor, logvar_regressor, retvals_norm\n\n    def predict(self, param: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Generate timeseries predictions from system parameters only.\n\n        Convenience method for inference mode. Bypasses Encoder and generates\n        predictions using only Regressor and Decoder.\n\n        Args:\n            param: System parameters of shape (batch, parameter_dim).\n\n        Returns:\n            Same as forward() method with predict=True.\n        \"\"\"\n        return self.forward(states=None, outputs=None, params=param, train=False, predict=True)\n\n    def save(self, path: Path):\n        \"\"\"Save model state dictionary to disk.\n\n        Args:\n            path: Path to save the model weights. Parent directories are created if needed.\n        \"\"\"\n        if not path.parent.exists():\n            path.parent.mkdir(parents=True)\n        torch.save(self.state_dict(), path)\n        logging.info('\\t \\t \\tSaved model to {}'.format(path))\n\n    def load(self, path: Path, device: Optional[torch.device] = None):\n        \"\"\"Load model state dictionary from disk.\n\n        Args:\n            path: Path to the saved model weights.\n            device: Device to map the loaded weights to (e.g., 'cpu', 'cuda').\n        \"\"\"\n        self.load_state_dict(torch.load(path, map_location=device))\n        logging.info('\\tLoaded model from {}'.format(path))\n</code></pre>"},{"location":"bnode_core/nn/vae/#bnode_core.nn.vae.vae_architecture.VAE.__init__","title":"<code>__init__(n_states: int, n_outputs: int, seq_len: int, parameter_dim: int, hidden_dim: int, bottleneck_dim: int, activation: nn.Module = nn.ReLU, n_layers: int = 3, params_to_decoder: bool = False, feed_forward_nn: bool = False)</code>","text":"<p>Initialize the VAE model.</p> <p>Parameters:</p> Name Type Description Default <code>n_states</code> <code>int</code> <p>Number of state channels in timeseries.</p> required <code>n_outputs</code> <code>int</code> <p>Number of output channels in timeseries.</p> required <code>seq_len</code> <code>int</code> <p>Length of timeseries sequence.</p> required <code>parameter_dim</code> <code>int</code> <p>Dimensionality of system parameters.</p> required <code>hidden_dim</code> <code>int</code> <p>Number of hidden units in all sub-networks.</p> required <code>bottleneck_dim</code> <code>int</code> <p>Dimensionality of latent space.</p> required <code>activation</code> <code>Module</code> <p>Activation function class (default: nn.ReLU).</p> <code>ReLU</code> <code>n_layers</code> <code>int</code> <p>Number of layers in all sub-networks (minimum 2).</p> <code>3</code> <code>params_to_decoder</code> <code>bool</code> <p>If True, decoder receives parameters as additional input (PELS-VAE mode).</p> <code>False</code> <code>feed_forward_nn</code> <code>bool</code> <p>If True, operate in feed-forward mode without latent space.</p> <code>False</code> Source code in <code>src/bnode_core/nn/vae/vae_architecture.py</code> <pre><code>def __init__(self, n_states: int, n_outputs: int, seq_len: int, parameter_dim: int, \n             hidden_dim: int, bottleneck_dim: int, activation: nn.Module = nn.ReLU, \n             n_layers: int = 3, params_to_decoder: bool = False, feed_forward_nn: bool = False):\n    \"\"\"Initialize the VAE model.\n\n    Args:\n        n_states: Number of state channels in timeseries.\n        n_outputs: Number of output channels in timeseries.\n        seq_len: Length of timeseries sequence.\n        parameter_dim: Dimensionality of system parameters.\n        hidden_dim: Number of hidden units in all sub-networks.\n        bottleneck_dim: Dimensionality of latent space.\n        activation: Activation function class (default: nn.ReLU).\n        n_layers: Number of layers in all sub-networks (minimum 2).\n        params_to_decoder: If True, decoder receives parameters as additional input (PELS-VAE mode).\n        feed_forward_nn: If True, operate in feed-forward mode without latent space.\n    \"\"\"\n    if feed_forward_nn is True:\n        if params_to_decoder is False:\n            Warning('params_to_decoder is set to False, but feed_forward_nn is set to True. Setting params_to_decoder to True')\n    super().__init__()\n    self.n_channels = n_states + n_outputs\n    self.n_states = n_states\n    self.n_outputs = n_outputs\n    self.timeseries_normalization = NormalizationLayerTimeSeries(n_channels=self.n_channels)\n    self.feed_forward_nn = feed_forward_nn\n\n    if feed_forward_nn is False:\n        self.Regressor = Regressor(parameter_dim, hidden_dim, bottleneck_dim, activation, n_layers)\n        self.Encoder = Encoder(self.n_channels, seq_len, hidden_dim,\n                            bottleneck_dim, activation, n_layers)\n        self.Decoder = Decoder(self.n_channels, seq_len, hidden_dim,\n                            bottleneck_dim, activation, n_layers,\n                            params_to_decoder, parameter_dim)\n    else:\n        _bottleneck_dim = 0\n        _params_to_decoder = True\n        self.Decoder = Decoder(self.n_channels, seq_len, hidden_dim,\n                               _bottleneck_dim, activation, n_layers,\n                                  _params_to_decoder, parameter_dim)\n    logging.info('VAE with n_channels = {}, seq_len = {}, parameter_dim = {}, \\\n                 hidden_dim = {}, bottleneck_dim = {}, activation = {}, n_layers = {}, params to decoder: {}'.format(\n                     self.n_channels, seq_len, parameter_dim, hidden_dim, bottleneck_dim, activation, n_layers, self.Decoder.params_to_decoder))\n    logging.info('VAE initialized with {} parameters'.format(count_parameters(self)))\n</code></pre>"},{"location":"bnode_core/nn/vae/#bnode_core.nn.vae.vae_architecture.VAE.reparametrize","title":"<code>reparametrize(mu: torch.Tensor, logvar: torch.Tensor) -&gt; torch.Tensor</code>","text":"<p>Apply reparametrization trick to sample from latent distribution.</p> <p>Samples z ~ N(mu, exp(0.5 * logvar)) using z = mu + eps * std, where eps ~ N(0, I). This allows backpropagation through the sampling operation.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>Tensor</code> <p>Mean of latent distribution, shape (batch, bottleneck_dim).</p> required <code>logvar</code> <code>Tensor</code> <p>Log-variance of latent distribution, shape (batch, bottleneck_dim).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Sampled latent vector z of shape (batch, bottleneck_dim).</p> Source code in <code>src/bnode_core/nn/vae/vae_architecture.py</code> <pre><code>def reparametrize(self, mu: torch.Tensor, logvar: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Apply reparametrization trick to sample from latent distribution.\n\n    Samples z ~ N(mu, exp(0.5 * logvar)) using z = mu + eps * std, where eps ~ N(0, I).\n    This allows backpropagation through the sampling operation.\n\n    Args:\n        mu: Mean of latent distribution, shape (batch, bottleneck_dim).\n        logvar: Log-variance of latent distribution, shape (batch, bottleneck_dim).\n\n\n    Returns:\n        Sampled latent vector z of shape (batch, bottleneck_dim).\n    \"\"\"\n    # if device.type == 'cuda':\n    #     eps = torch.autograd.Variable(torch.cuda.FloatTensor(mu.shape).normal_())\n    # else: \n    #     eps = torch.autograd.Variable(torch.FloatTensor(mu.shape).normal_())\n    eps = torch.randn_like(mu, device=mu.device)\n    std = logvar.mul(0.5).exp()\n    z_latent = eps.mul(std).add_(mu)\n    return z_latent\n</code></pre>"},{"location":"bnode_core/nn/vae/#bnode_core.nn.vae.vae_architecture.VAE.forward","title":"<code>forward(states: torch.Tensor, outputs: torch.Tensor, params: torch.Tensor, train: bool = True, predict: bool = False, n_passes: int = 1, test_with_zero_eps: bool = False, device: Optional[torch.device] = None) -&gt; Tuple</code>","text":"<p>Perform forward pass through the VAE network.</p> <p>Three operational modes based on flags:</p> <ol> <li>Training (train=True, predict=False): Encode timeseries, reconstruct using Encoder's latent distribution</li> <li>Testing (train=False, predict=False): Encode timeseries, reconstruct using Regressor's latent distribution</li> <li>Prediction (predict=True, train=False): Skip Encoder, reconstruct using Regressor's latent distribution only</li> </ol> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>Tensor</code> <p>State timeseries of shape (batch, n_states, seq_len).</p> required <code>outputs</code> <code>Tensor</code> <p>Output timeseries of shape (batch, n_outputs, seq_len).</p> required <code>params</code> <code>Tensor</code> <p>System parameters of shape (batch, parameter_dim).</p> required <code>train</code> <code>bool</code> <p>If True, use Encoder's latent distribution for reconstruction.</p> <code>True</code> <code>predict</code> <code>bool</code> <p>If True, bypass Encoder and reconstruct from parameters only.</p> <code>False</code> <code>n_passes</code> <code>int</code> <p>Number of decoder passes to average (for stochastic predictions).</p> <code>1</code> <code>test_with_zero_eps</code> <code>bool</code> <p>If True during testing, use mu directly (zero variance sampling).</p> <code>False</code> <code>device</code> <code>Optional[device]</code> <p>Device for tensor operations.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple</code> <p>Tuple of (x, x_hat, states_hat, outputs_hat, mu_encoder, logvar_encoder,       mu_regressor, logvar_regressor, retvals_norm) where:</p> <ul> <li>x: Concatenated input timeseries (states + outputs)</li> <li>x_hat: Reconstructed timeseries</li> <li>states_hat: Reconstructed states</li> <li>outputs_hat: Reconstructed outputs</li> <li>mu_encoder: Encoder's predicted latent mean</li> <li>logvar_encoder: Encoder's predicted latent log-variance</li> <li>mu_regressor: Regressor's predicted latent mean</li> <li>logvar_regressor: Regressor's predicted latent log-variance</li> <li>retvals_norm: Dictionary of normalized versions of above tensors</li> </ul> Source code in <code>src/bnode_core/nn/vae/vae_architecture.py</code> <pre><code>def forward(\n    self, \n    states: torch.Tensor, \n    outputs: torch.Tensor, \n    params: torch.Tensor, \n    train: bool = True, \n    predict: bool = False, \n    n_passes: int = 1, \n    test_with_zero_eps: bool = False, \n    device: Optional[torch.device] = None\n) -&gt; Tuple:\n    \"\"\"Perform forward pass through the VAE network.\n\n    Three operational modes based on flags:\n\n    1. Training (train=True, predict=False): Encode timeseries, reconstruct using Encoder's latent distribution\n    2. Testing (train=False, predict=False): Encode timeseries, reconstruct using Regressor's latent distribution\n    3. Prediction (predict=True, train=False): Skip Encoder, reconstruct using Regressor's latent distribution only\n\n    Args:\n        states: State timeseries of shape (batch, n_states, seq_len).\n        outputs: Output timeseries of shape (batch, n_outputs, seq_len).\n        params: System parameters of shape (batch, parameter_dim).\n        train: If True, use Encoder's latent distribution for reconstruction.\n        predict: If True, bypass Encoder and reconstruct from parameters only.\n        n_passes: Number of decoder passes to average (for stochastic predictions).\n        test_with_zero_eps: If True during testing, use mu directly (zero variance sampling).\n        device: Device for tensor operations.\n\n    Returns:\n        Tuple of (x, x_hat, states_hat, outputs_hat, mu_encoder, logvar_encoder, \n                 mu_regressor, logvar_regressor, retvals_norm) where:\n\n            - x: Concatenated input timeseries (states + outputs)\n            - x_hat: Reconstructed timeseries\n            - states_hat: Reconstructed states\n            - outputs_hat: Reconstructed outputs\n            - mu_encoder: Encoder's predicted latent mean\n            - logvar_encoder: Encoder's predicted latent log-variance\n            - mu_regressor: Regressor's predicted latent mean\n            - logvar_regressor: Regressor's predicted latent log-variance\n            - retvals_norm: Dictionary of normalized versions of above tensors\n    \"\"\"\n    if self.feed_forward_nn is False:\n        if predict:\n            assert not train, 'predict and train cannot be true at the same time'\n        else:\n            # concatenate states and outputs\n            x = torch.cat((states, outputs), dim=1)\n            x_norm = self.timeseries_normalization(x)\n            states_norm = x_norm[:,:self.n_states]\n            outputs_norm = x_norm[:,self.n_states:]\n            mu_encoder, logvar_encoder = self.Encoder(x_norm)\n        mu_regressor, logvar_regressor = self.Regressor(params)\n        # assign mu, logvar based on if train or not\n        if train:\n            mu = mu_encoder\n            logvar = logvar_encoder\n        else:\n            mu = mu_regressor\n            logvar = logvar_regressor\n        # if predict, we need some dummy values for mu_encoder and logvar_encoder\n        if predict:\n            mu_encoder = torch.ones_like(mu_encoder, device=device) * np.inf\n            logvar_encoder = torch.ones_like(logvar_encoder, device=device) * np.inf\n        # perform multiple passes through decoder\n        x_pass = []\n        x_pass_norm = []\n        for _ in range(n_passes):\n            if train or not test_with_zero_eps:\n                z_latent = self.reparametrize(mu, logvar)\n            else:\n                z_latent = mu\n            if self.Decoder.params_to_decoder:\n                x_i_hat_norm = self.Decoder(z_latent, params)\n            else:\n                x_i_hat_norm = self.Decoder(z_latent)\n            x_i_hat = self.timeseries_normalization(x_i_hat_norm, denormalize = True)\n            x_pass.append(x_i_hat)\n            x_pass_norm.append(x_i_hat_norm)\n        # stack along new dimension 1 and take mean along that dimension\n        x_hat = torch.stack(x_pass, dim=0).mean(dim=0)\n        x_hat_norm = torch.stack(x_pass_norm, dim=0).mean(dim=0)\n        # unpack x\n        states_hat, outputs_hat = torch.split(x_hat, [self.n_states, self.n_outputs], dim=1)\n        # unpack x_norm\n        states_hat_norm, outputs_hat_norm = torch.split(x_hat_norm, [self.n_states, self.n_outputs], dim=1)\n        retvals_norm = {\n            'x': x_norm,\n            'x_hat': x_hat_norm,\n            'states': states_norm,\n            'outputs': outputs_norm,\n            'states_hat': states_hat_norm,\n            'outputs_hat': outputs_hat_norm,\n        }\n    else:\n        x = torch.cat((states, outputs), dim=1)\n        x_norm = self.timeseries_normalization(x)\n        states_norm = x_norm[:,:self.n_states]\n        outputs_norm = x_norm[:,self.n_states:]\n        x_hat_norm = self.Decoder(None, params)\n        x_hat = self.timeseries_normalization(x_hat_norm, denormalize = True)\n         # unpack x\n        states_hat, outputs_hat = torch.split(x_hat, [self.n_states, self.n_outputs], dim=1)\n        # unpack x_norm\n        states_hat_norm, outputs_hat_norm = torch.split(x_hat_norm, [self.n_states, self.n_outputs], dim=1)\n        retvals_norm = {\n            'x': x_norm,\n            'x_hat': x_hat_norm,\n            'states': states_norm,\n            'outputs': outputs_norm,\n            'states_hat': states_hat_norm,\n            'outputs_hat': outputs_hat_norm,\n        }\n        mu_encoder = torch.ones_like(states_hat_norm, device=device) * np.inf\n        logvar_encoder = torch.ones_like(states_hat_norm, device=device) * np.inf\n        mu_regressor = torch.ones_like(states_hat_norm, device=device) * np.inf\n        logvar_regressor = torch.ones_like(states_hat_norm, device=device) * np.inf\n\n    return x, x_hat, states_hat, outputs_hat, mu_encoder, logvar_encoder, mu_regressor, logvar_regressor, retvals_norm\n</code></pre>"},{"location":"bnode_core/nn/vae/#bnode_core.nn.vae.vae_architecture.VAE.predict","title":"<code>predict(param: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]</code>","text":"<p>Generate timeseries predictions from system parameters only.</p> <p>Convenience method for inference mode. Bypasses Encoder and generates predictions using only Regressor and Decoder.</p> <p>Parameters:</p> Name Type Description Default <code>param</code> <code>Tensor</code> <p>System parameters of shape (batch, parameter_dim).</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Same as forward() method with predict=True.</p> Source code in <code>src/bnode_core/nn/vae/vae_architecture.py</code> <pre><code>def predict(self, param: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Generate timeseries predictions from system parameters only.\n\n    Convenience method for inference mode. Bypasses Encoder and generates\n    predictions using only Regressor and Decoder.\n\n    Args:\n        param: System parameters of shape (batch, parameter_dim).\n\n    Returns:\n        Same as forward() method with predict=True.\n    \"\"\"\n    return self.forward(states=None, outputs=None, params=param, train=False, predict=True)\n</code></pre>"},{"location":"bnode_core/nn/vae/#bnode_core.nn.vae.vae_architecture.VAE.save","title":"<code>save(path: Path)</code>","text":"<p>Save model state dictionary to disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to save the model weights. Parent directories are created if needed.</p> required Source code in <code>src/bnode_core/nn/vae/vae_architecture.py</code> <pre><code>def save(self, path: Path):\n    \"\"\"Save model state dictionary to disk.\n\n    Args:\n        path: Path to save the model weights. Parent directories are created if needed.\n    \"\"\"\n    if not path.parent.exists():\n        path.parent.mkdir(parents=True)\n    torch.save(self.state_dict(), path)\n    logging.info('\\t \\t \\tSaved model to {}'.format(path))\n</code></pre>"},{"location":"bnode_core/nn/vae/#bnode_core.nn.vae.vae_architecture.VAE.load","title":"<code>load(path: Path, device: Optional[torch.device] = None)</code>","text":"<p>Load model state dictionary from disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the saved model weights.</p> required <code>device</code> <code>Optional[device]</code> <p>Device to map the loaded weights to (e.g., 'cpu', 'cuda').</p> <code>None</code> Source code in <code>src/bnode_core/nn/vae/vae_architecture.py</code> <pre><code>def load(self, path: Path, device: Optional[torch.device] = None):\n    \"\"\"Load model state dictionary from disk.\n\n    Args:\n        path: Path to the saved model weights.\n        device: Device to map the loaded weights to (e.g., 'cpu', 'cuda').\n    \"\"\"\n    self.load_state_dict(torch.load(path, map_location=device))\n    logging.info('\\tLoaded model from {}'.format(path))\n</code></pre>"},{"location":"bnode_core/nn/vae/#bnode_core.nn.vae.vae_architecture.loss_function","title":"<code>loss_function(x: torch.Tensor, x_hat: torch.Tensor, mu: torch.Tensor, mu_hat: torch.Tensor, logvar: torch.Tensor, logvar_hat: torch.Tensor, beta: float = 1.0, gamma: float = 1000.0, capacity: Optional[float] = None, reduce: bool = True, device: Optional[torch.device] = None) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]</code>","text":"<p>Compute composite loss function for VAE training.</p> <p>Implements the PELS-VAE loss combining reconstruction, KL divergence, and regressor losses. Supports two modes:</p> <ol> <li>Standard \u03b2-VAE: loss = mse_loss + \u03b2 * kl_loss + regressor_loss</li> <li>Capacity-constrained: loss = mse_loss + \u03b3 * |kl_loss - capacity| + regressor_loss</li> </ol> <p>The regressor loss ensures that the Regressor's predicted latent distribution matches the Encoder's latent distribution, enabling parameter-to-latent predictions.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Original timeseries (normalized), shape (batch, n_channels, seq_len).</p> required <code>x_hat</code> <code>Tensor</code> <p>Reconstructed timeseries (normalized), shape (batch, n_channels, seq_len).</p> required <code>mu</code> <code>Tensor</code> <p>Encoder's latent mean, shape (batch, bottleneck_dim).</p> required <code>mu_hat</code> <code>Tensor</code> <p>Regressor's latent mean, shape (batch, bottleneck_dim).</p> required <code>logvar</code> <code>Tensor</code> <p>Encoder's latent log-variance, shape (batch, bottleneck_dim).</p> required <code>logvar_hat</code> <code>Tensor</code> <p>Regressor's latent log-variance, shape (batch, bottleneck_dim).</p> required <code>beta</code> <code>float</code> <p>Weight for KL divergence term (ignored if capacity is not None).</p> <code>1.0</code> <code>gamma</code> <code>float</code> <p>Weight for capacity constraint term (used only if capacity is not None).</p> <code>1000.0</code> <code>capacity</code> <code>Optional[float]</code> <p>Target KL divergence capacity. If None, uses standard \u03b2-VAE loss.</p> <code>None</code> <code>reduce</code> <code>bool</code> <p>If True, return scalar losses. If False, return per-sample losses.</p> <code>True</code> <code>device</code> <code>Optional[device]</code> <p>Device for tensor operations.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor, Tensor]</code> <p>Tuple of (loss, mse_loss, kl_loss, regressor_loss) where:</p> <ul> <li>loss: Total loss (inf if reduce=False)</li> <li>mse_loss: Mean squared error between x and x_hat</li> <li>kl_loss: KL divergence KL(N(mu, exp(logvar)) || N(0, I))</li> <li>regressor_loss: MSE between (mu, logvar) and (mu_hat, logvar_hat)</li> </ul> Notes <p>The capacity constraint encourages the model to use exactly 'capacity' nats of information in the latent space, preventing posterior collapse or over-regularization.</p> Source code in <code>src/bnode_core/nn/vae/vae_architecture.py</code> <pre><code>def loss_function(\n    x: torch.Tensor, \n    x_hat: torch.Tensor, \n    mu: torch.Tensor, \n    mu_hat: torch.Tensor, \n    logvar: torch.Tensor, \n    logvar_hat: torch.Tensor,\n    beta: float = 1.0, \n    gamma: float = 1000.0, \n    capacity: Optional[float] = None,\n    reduce: bool = True,\n    device: Optional[torch.device] = None\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Compute composite loss function for VAE training.\n\n    Implements the PELS-VAE loss combining reconstruction, KL divergence, and regressor losses.\n    Supports two modes:\n\n    1. Standard \u03b2-VAE: loss = mse_loss + \u03b2 * kl_loss + regressor_loss\n    2. Capacity-constrained: loss = mse_loss + \u03b3 * |kl_loss - capacity| + regressor_loss\n\n    The regressor loss ensures that the Regressor's predicted latent distribution\n    matches the Encoder's latent distribution, enabling parameter-to-latent predictions.\n\n    Args:\n        x: Original timeseries (normalized), shape (batch, n_channels, seq_len).\n        x_hat: Reconstructed timeseries (normalized), shape (batch, n_channels, seq_len).\n        mu: Encoder's latent mean, shape (batch, bottleneck_dim).\n        mu_hat: Regressor's latent mean, shape (batch, bottleneck_dim).\n        logvar: Encoder's latent log-variance, shape (batch, bottleneck_dim).\n        logvar_hat: Regressor's latent log-variance, shape (batch, bottleneck_dim).\n        beta: Weight for KL divergence term (ignored if capacity is not None).\n        gamma: Weight for capacity constraint term (used only if capacity is not None).\n        capacity: Target KL divergence capacity. If None, uses standard \u03b2-VAE loss.\n        reduce: If True, return scalar losses. If False, return per-sample losses.\n        device: Device for tensor operations.\n\n    Returns:\n        Tuple of (loss, mse_loss, kl_loss, regressor_loss) where:\n\n            - loss: Total loss (inf if reduce=False)\n            - mse_loss: Mean squared error between x and x_hat\n            - kl_loss: KL divergence KL(N(mu, exp(logvar)) || N(0, I))\n            - regressor_loss: MSE between (mu, logvar) and (mu_hat, logvar_hat)\n\n    Notes:\n        The capacity constraint encourages the model to use exactly 'capacity' nats\n        of information in the latent space, preventing posterior collapse or over-regularization.\n    \"\"\"\n    mse = nn.MSELoss(reduction='mean' if reduce else 'none')\n    mse_loss = mse(x_hat, x)\n    kl_loss = kullback_leibler(mu, logvar, per_dimension=not reduce, reduce=reduce)\n    regressor_loss = mse(mu_hat, mu) + mse(logvar_hat, logvar)\n    if reduce:\n        if capacity is None:\n            loss = mse_loss + beta * kl_loss + regressor_loss\n        else:\n            if capacity &lt; 0:\n                raise ValueError('capacity must be positive')\n            # kl_loss is always positive, so subtracting capacity and \n            # taking the absolute value sets a capacity\n            loss = mse_loss + gamma * (kl_loss - capacity).abs() + regressor_loss\n    else:\n        loss = torch.tensor(np.inf, device=device)\n    return loss, mse_loss, kl_loss, regressor_loss\n</code></pre>"},{"location":"bnode_core/nn/vae/#bnode_core.nn.vae.vae_train_test","title":"<code>bnode_core.nn.vae.vae_train_test</code>","text":"<p>VAE training and testing pipeline for timeseries modeling.</p> <p>This module implements the complete training pipeline for Variational Autoencoders (VAE) with parameter conditioning, supporting standard VAE, PELS-VAE, and feed-forward modes.</p> Attention <p>This documentation is AI generated. Be aware of possible inaccuricies.</p>"},{"location":"bnode_core/nn/vae/#bnode_core.nn.vae.vae_train_test--command-line-usage","title":"Command-line Usage","text":"<p>The module uses Hydra for configuration management and MLflow for experiment tracking. Training is launched via the command line:</p> <pre><code>uv run python -m bnode_core.nn.vae.vae_train_test\n\nuv run &lt;path to vae_train_test.py&gt;\n</code></pre> <p>Configuration files are loaded from <code>conf/train_test_vae.yaml</code> (or specified config path).</p>"},{"location":"bnode_core/nn/vae/#bnode_core.nn.vae.vae_train_test--configuration","title":"Configuration","text":"<p>Key configuration parameters (via Hydra config):</p> <ul> <li><code>dataset_name</code>: Name of HDF5 dataset to load</li> <li><code>use_cuda</code>: Enable CUDA acceleration</li> <li><code>use_amp</code>: Enable automatic mixed precision training</li> <li><code>nn_model.network.*</code>: Model architecture parameters (hidden_dim, n_latent, activation, etc.)</li> <li><code>nn_model.training.*</code>: Training hyperparameters (batch_size, lr, max_epochs, etc.)</li> </ul>"},{"location":"bnode_core/nn/vae/#bnode_core.nn.vae.vae_train_test--training-workflow","title":"Training Workflow","text":"<ol> <li>Load HDF5 dataset and create train/validation/test/common_test dataloaders</li> <li>Initialize VAE model with specified architecture</li> <li>Initialize normalization layers on full training dataset</li> <li> <p>Train with:</p> </li> <li> <p>Adam optimizer with learning rate scheduling (ReduceLROnPlateau)</p> </li> <li>Early stopping monitoring validation loss</li> <li>Capacity scheduling for controlled KL divergence growth</li> <li>Automatic mixed precision (AMP) support</li> <li>Gradient clipping for stability</li> <li>Save best model checkpoint based on validation loss</li> <li>Evaluate on all dataset splits and save predictions to HDF5 file</li> <li>Log all metrics and artifacts to MLflow</li> </ol>"},{"location":"bnode_core/nn/vae/#bnode_core.nn.vae.vae_train_test--output-files","title":"Output Files","text":"<ul> <li><code>model.pth</code>: Best model checkpoint (state_dict)</li> <li><code>dataset_with_predictions.h5</code>: Copy of input dataset with added model predictions</li> <li><code>vae_train_test.py</code>, <code>vae_architecture.py</code>: Copies of source files for reproducibility</li> </ul>"},{"location":"bnode_core/nn/vae/#bnode_core.nn.vae.vae_train_test--key-features","title":"Key Features","text":"<ul> <li>Multi-pass prediction: Average multiple stochastic forward passes for robust predictions</li> <li>Capacity scheduling: Gradually increase KL divergence capacity to prevent posterior collapse</li> <li>Early stopping: Monitor validation loss with configurable patience and threshold</li> <li>MLflow integration: Automatic logging of metrics, parameters, and artifacts via decorator</li> <li>Reproducibility: Saves source code and full configuration to output directory</li> </ul>"},{"location":"bnode_core/nn/vae/#bnode_core.nn.vae.vae_train_test.train","title":"<code>train(cfg: train_test_config_class) -&gt; float</code>","text":"<p>Train VAE model on timeseries dataset with MLflow tracking.</p> <p>Complete training pipeline including:</p> <ul> <li>Dataset loading and preprocessing</li> <li>Model initialization and normalization layer setup</li> <li>Training loop with early stopping and capacity scheduling</li> <li>Evaluation on all dataset splits</li> <li>Model checkpoint saving and artifact logging</li> </ul> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>train_test_config_class</code> <p>Hydra configuration object containing all training parameters. Key sections: dataset_name, use_cuda, use_amp, nn_model.network, nn_model.training</p> required <p>Returns:</p> Type Description <code>float</code> <p>Final MSE loss on test set (float).</p> Notes <ul> <li>Uses @log_hydra_to_mlflow decorator for automatic MLflow experiment tracking</li> <li>Saves best model based on validation loss</li> <li>Copies dataset to output directory with added model predictions</li> <li>Logs metrics at each epoch: loss, mse_loss, kl_loss, regressor_loss, populated_dims</li> </ul> Source code in <code>src/bnode_core/nn/vae/vae_train_test.py</code> <pre><code>@log_hydra_to_mlflow\ndef train(cfg: train_test_config_class) -&gt; float:\n    \"\"\"Train VAE model on timeseries dataset with MLflow tracking.\n\n    Complete training pipeline including:\n\n    - Dataset loading and preprocessing\n    - Model initialization and normalization layer setup\n    - Training loop with early stopping and capacity scheduling\n    - Evaluation on all dataset splits\n    - Model checkpoint saving and artifact logging\n\n    Args:\n        cfg: Hydra configuration object containing all training parameters.\n            Key sections: dataset_name, use_cuda, use_amp, nn_model.network, nn_model.training\n\n    Returns:\n        Final MSE loss on test set (float).\n\n    Notes:\n        - Uses @log_hydra_to_mlflow decorator for automatic MLflow experiment tracking\n        - Saves best model based on validation loss\n        - Copies dataset to output directory with added model predictions\n        - Logs metrics at each epoch: loss, mse_loss, kl_loss, regressor_loss, populated_dims\n    \"\"\"\n    device = torch.device('cuda' if torch.cuda.is_available() and cfg.use_cuda else 'cpu')\n\n    # load dataset and config\n    dataset, dataset_config = load_dataset_and_config(cfg.dataset_name, cfg.dataset_path)\n\n    # make train and test torch tensor datasets\n    train_dataset = make_stacked_dataset(dataset, 'train')\n    test_dataset = make_stacked_dataset(dataset, 'test')\n    validation_dataset = make_stacked_dataset(dataset, 'validation')\n    common_test_dataset = make_stacked_dataset(dataset, 'common_test')\n\n    # initialize data loaders\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=cfg.nn_model.training.batch_size, shuffle=True)\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=cfg.nn_model.training.batch_size, shuffle=True)\n    validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=cfg.nn_model.training.batch_size, shuffle=True)\n    common_test_loader = torch.utils.data.DataLoader(common_test_dataset, batch_size=cfg.nn_model.training.batch_size, shuffle=True)\n\n    # initialize model\n    model = VAE(\n        n_states=dataset['train']['states'].shape[1],\n        n_outputs=dataset['train']['outputs'].shape[1],\n        seq_len=dataset['train']['states'].shape[2],\n        parameter_dim=dataset['train']['parameters'].shape[1],\n        hidden_dim=cfg.nn_model.network.linear_hidden_dim,\n        bottleneck_dim=cfg.nn_model.network.n_latent,\n        activation=eval(cfg.nn_model.network.activation),\n        n_layers=cfg.nn_model.network.n_linear_layers,\n        params_to_decoder=cfg.nn_model.network.params_to_decoder,\n        feed_forward_nn=cfg.nn_model.network.feed_forward_nn,\n    )\n    model.to(device)\n\n    # initialize timeseries_normalization layer on whole dataset\n    _states = train_dataset.datasets['states'].to(device)\n    _outputs = train_dataset.datasets['outputs'].to(device)\n    _parameters = train_dataset.datasets['parameters'].to(device)\n    _x = x = torch.cat((_states, _outputs), dim=1)\n    model.timeseries_normalization.initialize_normalization(_x)\n    model.Regressor.normalization(_parameters) if model.feed_forward_nn is False else None\n    del _states, _outputs, _parameters, _x\n    logging.info('Initialized timeseries_normalization layer on whole dataset')\n    logging.info('Initialized Regressor normalization layer on whole dataset')\n\n    # initialize optimizer\n    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.nn_model.training.lr_start)\n    #optimizer = torch.optim.SGD(model.parameters(), lr=cfg.nn_model.training.lr_start)\n\n    # initialize lr scheduler\n    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n                                                            mode='min',\n                                                            factor=cfg.nn_model.training.lr_scheduler_plateau_gamma,\n                                                            patience=cfg.nn_model.training.lr_scheduler_plateau_patience,\n                                                            threshold=cfg.nn_model.training.lr_scheduler_threshold,\n                                                            threshold_mode=cfg.nn_model.training.lr_scheduler_threshold_mode,\n                                                            min_lr=cfg.nn_model.training.lr_min,\n                                                            )\n    # initialize early stopping\n    early_stopping = EarlyStopping(patience=cfg.nn_model.training.early_stopping_patience,\n                                      verbose=True,\n                                      threshold=cfg.nn_model.training.early_stopping_threshold,\n                                      threshold_mode=cfg.nn_model.training.early_stopping_threshold_mode,\n                                      path=filepaths.filepath_model_current_hydra_output(),\n                                      trace_func=logging.info)\n\n    # initialize capacity scheduler\n    capacity_scheduler = CapacityScheduler(\n        patience = cfg.nn_model.training.capacity_patience,\n        capacity_start = cfg.nn_model.training.capacity_start,\n        capacity_max=cfg.nn_model.training.capacity_max,\n        capacity_increment = cfg.nn_model.training.capacity_increment,\n        capacity_increment_mode = cfg.nn_model.training.capacity_increment_mode,\n        threshold = cfg.nn_model.training.capacity_threshold,\n        threshold_mode = cfg.nn_model.training.capacity_threshold_mode,\n        trace_func = logging.info,\n        enabled=cfg.nn_model.training.use_capacity\n    )\n    # initialize gradient scaler\n    scaler = torch.cuda.amp.GradScaler(enabled=cfg.use_amp)\n    logging.info('Training with automatic mixed precision: {}'.format(cfg.use_amp))\n\n    # define one model and loss evaluation\n    def model_and_loss_evaluation(model, states, outputs, parameters, train=True, n_passes: int = 1, return_model_outputs: bool = False, test_from_regressor: bool = True):\n        \"\"\"Evaluate model forward pass and compute all loss components.\n\n        Args:\n            model: VAE model instance.\n            states: State timeseries tensor, shape (batch, n_states, seq_len).\n            outputs: Output timeseries tensor, shape (batch, n_outputs, seq_len).\n            parameters: System parameters tensor, shape (batch, parameter_dim).\n            train: If True, use Encoder's latent distribution. If False, controlled by test_from_regressor.\n            n_passes: Number of stochastic forward passes to average.\n            return_model_outputs: If True, return model outputs (predictions, latent variables, raw losses).\n            test_from_regressor: If True during testing, use Regressor's latent distribution instead of Encoder's.\n\n        Returns:\n            Dictionary with keys: loss, mse_loss, kl_loss, regressor_loss, populated_dims.\n            If return_model_outputs=True, returns tuple (ret_val, model_outputs) where model_outputs\n            contains: mse_loss_raw, kl_loss_raw, regressor_loss_raw, states_hat, outputs_hat, \n            mu, logvar, mu_hat, logvar_hat.\n        \"\"\"\n        _train = train if train is True else test_from_regressor # if not training, do the test with either mu, logvar from regressor or from encoder\n        x, x_hat, states_hat, outputs_hat, mu, logvar, mu_hat, logvar_hat, normed_values = model(states, outputs, parameters, train=_train, \n                                                                                                 predict = False, n_passes=n_passes, \n                                                                                                 test_with_zero_eps=cfg.nn_model.training.test_with_zero_eps,\n                                                                                                 device=device)\n        loss, mse_loss, kl_loss, regressor_loss = loss_function(\n                    normed_values['x'], normed_values['x_hat'], mu, mu_hat, \n                    logvar, logvar_hat, \n                    beta=cfg.nn_model.training.beta_start, \n                    gamma=cfg.nn_model.training.gamma,\n                    capacity= None if cfg.nn_model.training.use_capacity is False\n                        else capacity_scheduler.get_capacity(),\n                    device=device,\n        )\n        _populated_dimensions, _ = count_populated_dimensions(mu, logvar, cfg.nn_model.training.count_populated_dimensions_threshold)\n        ret_val = {\n            'loss': loss,\n            'mse_loss': mse_loss,\n            'kl_loss': kl_loss,\n            'regressor_loss': regressor_loss,\n            'populated_dims': _populated_dimensions,\n        }\n        if return_model_outputs:\n            # losses per dim\n            _, mse_loss_raw, kl_loss_raw, regressor_loss_raw = loss_function(\n                    x, x_hat, mu, mu_hat, \n                    logvar, logvar_hat, \n                    beta=cfg.nn_model.training.beta_start, \n                    gamma=cfg.nn_model.training.gamma,\n                    capacity= None,\n                    reduce=False\n                    )   \n            model_outputs = {\n                'mse_loss_raw': mse_loss_raw,\n                'kl_loss_raw': kl_loss_raw,\n                'regressor_loss_raw': regressor_loss_raw,\n                'states_hat': states_hat,\n                'outputs_hat': outputs_hat,\n                'mu': mu,\n                'logvar': logvar,\n                'mu_hat': mu_hat,\n                'logvar_hat': logvar_hat,\n            }\n        if not train:\n            # call value.item() for each value in return_value\n            ret_val = dict({key: value.item() for key, value in ret_val.items()})\n            if return_model_outputs:\n                model_outputs = dict({key: value.cpu().detach().numpy() for key, value in model_outputs.items()})\n        return ret_val if not return_model_outputs else (ret_val, model_outputs)\n\n    def get_model_inputs(data_loader: torch.utils.data.DataLoader, data: dict = None):\n        \"\"\"Extract model inputs from data loader or data dictionary.\n\n        Args:\n            data_loader: PyTorch DataLoader (if provided, fetches next batch).\n            data: Dictionary with keys 'states', 'outputs', 'parameters' (alternative to data_loader).\n\n        Returns:\n            Tuple of (states, outputs, parameters) tensors moved to device.\n        \"\"\"\n        if data_loader is None:\n            assert data is not None, 'Either data_loader or data must be not None'\n        else:\n            data = next(iter(data_loader))\n        # get data from data loader\n        states = data['states'].to(device)\n        outputs = data['outputs'].to(device)\n        parameters = data['parameters'].to(device)\n        return states, outputs, parameters\n\n\n    # define train loop for one epoch\n    def train_one_epoch(model, train_loader, optimizer, scaler, epoch):\n        \"\"\"Execute one training epoch with gradient updates.\n\n        Iterates through all batches in train_loader, computes losses, performs\n        backpropagation with gradient clipping and AMP scaling.\n\n        Args:\n            model: VAE model instance.\n            train_loader: PyTorch DataLoader for training data.\n            optimizer: PyTorch optimizer.\n            scaler: CUDA AMP gradient scaler.\n            epoch: Current epoch number (for logging).\n\n        Returns:\n            Dictionary with training metrics: loss, mse_loss, kl_loss, regressor_loss, populated_dims.\n        \"\"\"\n        # get data from train loader\n        model.train()\n        for batch_idx, data in enumerate(train_loader):\n            # get data from data loader\n            states, outputs, parameters = get_model_inputs(train_loader)\n            # zero the parameter gradients\n            optimizer.zero_grad()\n            # forward + backward + optimize\n            with torch.cuda.amp.autocast(enabled=cfg.use_amp):\n                ret_vals_train = model_and_loss_evaluation(model, states, outputs, parameters, n_passes=cfg.nn_model.training.n_passes_train, test_from_regressor=cfg.nn_model.training.test_from_regressor)\n            loss = ret_vals_train['loss'] if cfg.nn_model.network.feed_forward_nn is False else ret_vals_train['mse_loss']\n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.nn_model.training.clip_grad_norm)\n            scaler.step(optimizer)\n            scaler.update()\n\n        # call value.item() for each value in return_value\n        ret_vals_train = dict({key: value.item() for key, value in ret_vals_train.items()})\n\n        return ret_vals_train\n\n    def test_or_validate_one_epoch(model, _data_loader, n_passes: int = 1, all_batches: bool = False,\n                                   return_model_outputs: bool = False):\n        \"\"\"Evaluate model on validation or test set without gradient computation.\n\n        Args:\n            model: VAE model instance.\n            _data_loader: PyTorch DataLoader for evaluation data.\n            n_passes: Number of stochastic forward passes to average.\n            all_batches: If True, evaluate on all batches and average metrics. If False, evaluate only first batch.\n            return_model_outputs: If True, return model predictions and latent variables.\n\n        Returns:\n            Dictionary with evaluation metrics: loss, mse_loss, kl_loss, regressor_loss, populated_dims.\n            If return_model_outputs=True, returns tuple (ret_vals, model_outputs) where model_outputs\n            contains predictions and latent variables for all evaluated batches.\n        \"\"\"\n        model.eval()\n        # make sure that the data loader is not shuffled by initializing a new data loader\n        if all_batches:\n            data_loader = torch.utils.data.DataLoader(_data_loader.dataset, batch_size=_data_loader.batch_size, shuffle=False)\n        else:\n            data_loader = _data_loader\n        _ret_vals = []\n        _model_outputs = []\n        for step, data in enumerate(data_loader):\n            states, outputs, parameters = get_model_inputs(data_loader=None, data=data)\n            # forward\n            with torch.no_grad():\n                ret_vals, model_outputs = model_and_loss_evaluation(model, states, outputs, parameters, train=False, n_passes=n_passes, return_model_outputs=True, test_from_regressor = cfg.nn_model.training.test_from_regressor)\n            _ret_vals.append(ret_vals)\n            _model_outputs.append(model_outputs)\n            if all_batches is False:\n                break\n        # average over all calls\n        if all_batches:\n            ret_vals = {}\n            for key in _ret_vals[0].keys():\n                ret_vals[key] = sum([_ret_val[key] for _ret_val in _ret_vals]) / len(_ret_vals)\n        else:\n            ret_vals = _ret_vals[0]\n        # make one tensor from all model outputs\n        if return_model_outputs:\n            model_outputs = {key: np.concatenate([_batch_output[key] for _batch_output in _model_outputs], axis=0) for key in _model_outputs[0].keys()}\n        return ret_vals if not return_model_outputs else (ret_vals, model_outputs)\n\n    def append_context_to_dict_keys(dictionary: dict, context: str):\n        \"\"\"Add context suffix to all dictionary keys for MLflow logging.\n\n        Args:\n            dictionary: Dictionary with metric names as keys.\n            context: Suffix to append (e.g., 'train', 'validation', 'test').\n\n        Returns:\n            New dictionary with keys formatted as 'original_key_context'.\n        \"\"\"\n        return dict({'{}_{}'.format(key, context): value for key, value in dictionary.items()})\n\n    # training loop\n    _flag_break_next_epoch = False\n    for epoch in range(cfg.nn_model.training.max_epochs):\n        # train one epoch\n        if not _flag_break_next_epoch:\n            ret_vals_train = train_one_epoch(model, train_loader, optimizer, scaler, epoch)\n        else:\n            ret_vals_train = test_or_validate_one_epoch(model, train_loader, n_passes=cfg.nn_model.training.n_passes_test)\n        # validate one epoch\n        ret_vals_validation = test_or_validate_one_epoch(model, validation_loader, n_passes=cfg.nn_model.training.n_passes_test)\n        # test one epoch\n        ret_vals_test = test_or_validate_one_epoch(model, test_loader, n_passes=cfg.nn_model.training.n_passes_test)\n        # lr scheduler step\n        if not _flag_break_next_epoch:\n            lr_scheduler.step(ret_vals_validation['loss'] if cfg.nn_model.network.feed_forward_nn is False else ret_vals_validation['mse_loss'])\n        # early stopping\n            early_stopping(ret_vals_validation['loss'] if cfg.nn_model.network.feed_forward_nn is False else ret_vals_validation['mse_loss'],\n                           model, epoch, corresponding_loss = ret_vals_train['loss'])\n        # capacity scheduler\n        capacity_scheduler.update(ret_vals_validation['mse_loss'])\n        # log stats with logging\n        string = 'Epoch: {}/{} | train/validate/test: {:.4f}/{:.4f}/{:.4f} | mse: {:.4f}/{:.4f}/{:.4f} | kl_loss: {:.4f}/{:.4f}/{:.4f} | regressor_loss: {:.4f}/{:.4f}/{:.4f} | pop. dim: {}/{}/{} | \\\n            \\t\\t\\t| batches: {} | lr: {:.6f} |'.format(\n            epoch, cfg.nn_model.training.max_epochs,\n            ret_vals_train['loss'], ret_vals_validation['loss'], ret_vals_test['loss'],\n            ret_vals_train['mse_loss'], ret_vals_validation['mse_loss'], ret_vals_test['mse_loss'],\n            ret_vals_train['kl_loss'], ret_vals_validation['kl_loss'], ret_vals_test['kl_loss'],\n            ret_vals_train['regressor_loss'], ret_vals_validation['regressor_loss'], ret_vals_test['regressor_loss'],\n            ret_vals_train['populated_dims'], ret_vals_validation['populated_dims'], ret_vals_test['populated_dims'],\n            len(train_loader),\n            optimizer.param_groups[0]['lr'])\n        string = string + ' capacity: {:.4f} |'.format(capacity_scheduler.get_capacity()) if cfg.nn_model.training.use_capacity else string\n        string = string + ' EarlyStopping: {}/{} |'.format(early_stopping.counter, early_stopping.patience)\n        logging.info(string)\n        # log stats with mlflow\n        mlflow.log_metrics(append_context_to_dict_keys(ret_vals_train, 'train'), step=epoch, )\n        mlflow.log_metrics(append_context_to_dict_keys(ret_vals_validation, 'validation'), step=epoch)\n        mlflow.log_metrics(append_context_to_dict_keys(ret_vals_test, 'test'), step=epoch)\n        mlflow.log_metric('lr', optimizer.param_groups[0]['lr'], step=epoch)\n        mlflow.log_metric('EarlyStopping_counter', early_stopping.counter, step=epoch)\n        mlflow.log_metric('capacity', capacity_scheduler.get_capacity(), step=epoch) if cfg.nn_model.training.use_capacity else None\n\n        # check early stopping\n        if early_stopping.early_stop:\n            logging.info(\"Early stopping\")\n            mlflow.log_param('ended_by', 'early_stopping')\n            # let the evaluation run one more time to record the outputs of the best model\n            _flag_break_next_epoch = True\n            # load the best model\n            model.load(filepaths.filepath_model_current_hydra_output(), device=device)\n        if _flag_break_next_epoch:\n            break\n\n    # Check performance of model on all datasets\n\n    # load best model\n    model.load(filepaths.filepath_model_current_hydra_output(), device=device)\n\n    # close initial dataset\n    dataset.close()\n    # copy dataset to hydra output directory\n    _path = filepaths.filepath_dataset_current_hydra_output()\n    shutil.copy(filepaths.filepath_dataset_from_config(cfg.dataset_name, cfg.dataset_path), _path)\n    dataset = h5py.File(_path, 'r+')\n    # add model outputs to dataset\n    for context, dataloader in zip(['train', 'test', 'validation', 'common_test'], [train_loader, test_loader, validation_loader, common_test_loader]):\n        ret_vals, model_outputs = test_or_validate_one_epoch(model, dataloader, n_passes=cfg.nn_model.training.n_passes_test, all_batches=True, return_model_outputs=True)\n\n        # log stats with logging\n        string = context\n        string = string + ': loss: {:.4f} | mse: {:.4f} | kl_loss: {:.4f} | regressor_loss: {:.4f} | pop. dim: {} |'.format(\n            ret_vals['loss'],\n            ret_vals['mse_loss'],\n            ret_vals['kl_loss'],\n            ret_vals['regressor_loss'],\n            ret_vals['populated_dims'],\n        )\n        logging.info(string)\n\n        # save loss function values\n        for key, value in ret_vals.items():\n            dataset.create_dataset(context+'/'+key, data=value) \n        # save reconstructed timeseries and raw loss function values\n        for key, value in model_outputs.items():\n            dataset.create_dataset(context+'/'+key, data=value)\n        # log to mlflow\n        mlflow.log_metrics(append_context_to_dict_keys(ret_vals, context), step=epoch)\n    dataset.close()\n\n    # save this file and the vae_architecture.py file to hydra output directory\n    shutil.copy(Path(__file__), filepaths.dir_current_hydra_output())\n    return ret_vals['mse_loss']\n</code></pre>"},{"location":"bnode_core/nn/vae/#bnode_core.nn.vae.vae_train_test.main","title":"<code>main()</code>","text":"<p>Entry point for VAE training via Hydra CLI.</p> <p>Initializes Hydra configuration system and launches train with validated config. Auto-detects config directory and uses 'train_test_vae' as the default config name.</p> <p>This function can be registered in pyproject.toml, enabling command-line execution via a custom script name.</p> <p>Examples:</p> <p>Run from command line::</p> <pre><code>uv run python -m bnode_core.nn.vae.vae_train_test\n</code></pre> <p>With config overrides::</p> <pre><code>uv run python -m bnode_core.nn.vae.vae_train_test \\\n    nn_model.training.lr_start=0.0001 \\\n    dataset_name=my_dataset\n</code></pre> Side Effects <ul> <li>Registers config store with Hydra</li> <li>Auto-detects config directory from filepaths</li> <li>Launches Hydra-decorated train function</li> </ul> Source code in <code>src/bnode_core/nn/vae/vae_train_test.py</code> <pre><code>def main():\n    \"\"\"Entry point for VAE training via Hydra CLI.\n\n    Initializes Hydra configuration system and launches train with validated\n    config. Auto-detects config directory and uses 'train_test_vae' as the\n    default config name.\n\n    This function can be registered in pyproject.toml, enabling command-line\n    execution via a custom script name.\n\n    Examples:\n        Run from command line::\n\n            uv run python -m bnode_core.nn.vae.vae_train_test\n\n        With config overrides::\n\n            uv run python -m bnode_core.nn.vae.vae_train_test \\\\\n                nn_model.training.lr_start=0.0001 \\\\\n                dataset_name=my_dataset\n\n\n    Side Effects:\n        - Registers config store with Hydra\n        - Auto-detects config directory from filepaths\n        - Launches Hydra-decorated train function\n    \"\"\"\n    from bnode_core.config import get_config_store\n    cs = get_config_store()\n    config_dir = filepaths.config_dir_auto_recognize()\n    config_name = 'train_test_vae'\n    hydra.main(config_path=str(config_dir.absolute()), config_name=config_name, version_base=None)(train)()\n</code></pre>"},{"location":"bnode_core/ode/node/","title":"NODE","text":""},{"location":"bnode_core/ode/node/#bnode_core.ode.node.node_architecture","title":"<code>bnode_core.ode.node.node_architecture</code>","text":"<p>Neural ODE (NODE) Architecture Module.</p> <p>This module implements a Neural Ordinary Differential Equation (NODE) architecture for modeling dynamical systems. NODE directly learns the differential equations governing a system's evolution by parameterizing the derivative function with neural networks. Specifically, this module learns a State-Space representation of the system dynamics:</p>"},{"location":"bnode_core/ode/node/#bnode_core.ode.node.node_architecture--attention","title":"Attention","text":"<p>This documentation is AI written and may contain inaccuracies. Please verify the details before use.</p>"},{"location":"bnode_core/ode/node/#bnode_core.ode.node.node_architecture--overview","title":"Overview","text":"<p>Neural ODEs represent a continuous-depth neural network where the hidden state evolves according to a learned ODE:</p> <pre><code>dx/dt = f_\u03b8(x, u, p, t)\n</code></pre> Where <ul> <li>x: System states</li> <li>u: Control inputs (optional)</li> <li>p: System parameters (optional)</li> <li>t: Time</li> <li>f_\u03b8: Neural network parameterized by \u03b8</li> </ul> <p>Outputs are generated an optional output network:</p> <pre><code>y = g_\u03c6(x, u, p, t)\n</code></pre> Where <ul> <li>y: System outputs/measurements</li> <li>g_\u03c6: Neural network parameterized by \u03c6</li> </ul>"},{"location":"bnode_core/ode/node/#bnode_core.ode.node.node_architecture--architecture-components","title":"Architecture Components","text":"<ol> <li> <p>NeuralODEFunc: Learns the state derivative function f_\u03b8</p> <ul> <li>Input: Current states, controls (optional), parameters (optional)</li> <li>Output: State derivatives dx/dt</li> <li>Uses normalization for numerical stability</li> </ul> </li> <li> <p>OutputNetwork: Maps states to observable outputs (optional)</p> <ul> <li>Input: States, controls (optional), parameters (optional)</li> <li>Output: System outputs/measurements</li> <li>Decouples internal dynamics from observations</li> </ul> </li> <li> <p>NeuralODE: Main model combining ODE solver and output network</p> <ul> <li>Integrates state derivatives using torchdiffeq solvers</li> <li>Supports both training and inference modes</li> <li>Includes pre-training on derivatives and main training with ODE solver</li> </ul> </li> </ol>"},{"location":"bnode_core/ode/node/#bnode_core.ode.node.node_architecture--key-features","title":"Key Features","text":"<ul> <li>Continuous-time modeling: No discretization artifacts</li> <li>Variable time step: Can predict at arbitrary time points</li> <li>Flexible: Supports controls, parameters, and outputs</li> <li>Normalized: Built-in normalization for numerical stability</li> </ul>"},{"location":"bnode_core/ode/node/#bnode_core.ode.node.node_architecture--training-modes","title":"Training Modes","text":"<ol> <li> <p>Pre-training Mode:</p> <ul> <li>Trains on state derivatives directly (if available in dataset, could construct these numerically, e.g. by finite differences)</li> <li>Fast initial parameter estimation</li> <li>No ODE solver required</li> <li>Loss: ||dx/dt - f_\u03b8(x, u, p)||\u00b2</li> </ul> </li> <li> <p>Main Training Mode:</p> <ul> <li>Integrates ODE from initial conditions</li> <li>Uses torchdiffeq solvers (Euler, RK4, dopri5, etc.)</li> <li>More accurate but slower</li> <li>Loss: ||x(t) - \u222bf_\u03b8(x, u, p)dt||\u00b2</li> </ul> </li> </ol>"},{"location":"bnode_core/ode/node/#bnode_core.ode.node.node_architecture--supported-solvers","title":"Supported Solvers","text":"<p>See torchdiffeq documentation.</p>"},{"location":"bnode_core/ode/node/#bnode_core.ode.node.node_architecture.NeuralODEFunc","title":"<code>NeuralODEFunc</code>","text":"<p>               Bases: <code>Module</code></p> <p>Neural network function representing ODE right-hand side f_\u03b8(x, u, p).</p> <p>This module learns the state derivative function for a dynamical system:</p> <pre><code>dx/dt = f_\u03b8(x, u, p)\n</code></pre> <p>The network includes normalization layers for numerical stability and supports optional control inputs and system parameters.</p> <p>Parameters:</p> Name Type Description Default <code>states_dim</code> <code>int</code> <p>Dimension of state vector x.</p> required <code>controls_dim</code> <code>int</code> <p>Dimension of control input vector u. Default: 0.</p> <code>0</code> <code>parameters_dim</code> <code>int</code> <p>Dimension of parameter vector p. Default: 0.</p> <code>0</code> <code>hidden_dim</code> <code>int</code> <p>Hidden layer dimension. Default: 20.</p> <code>20</code> <code>n_layers</code> <code>int</code> <p>Number of layers (minimum 2). Default: 3.</p> <code>3</code> <code>activation</code> <code>Module</code> <p>Activation function class. Default: nn.ELU.</p> <code>ELU</code> <code>intialization</code> <code>str</code> <p>Weight initialization method ('identity', 'xavier', 'kaiming', 'orthogonal'). Default: 'identity'.</p> <code>'identity'</code> <p>Attributes:</p> Name Type Description <code>states_dim</code> <code>int</code> <p>Dimension of states.</p> <code>controls_dim</code> <code>int</code> <p>Dimension of controls.</p> <code>parameters_dim</code> <code>int</code> <p>Dimension of parameters.</p> <code>include_controls</code> <code>bool</code> <p>Whether controls are used.</p> <code>include_parameters</code> <code>bool</code> <p>Whether parameters are used.</p> <code>normalization_states</code> <code>NormalizationLayer1D</code> <p>Normalizes input states.</p> <code>normalization_states_der</code> <code>NormalizationLayer1D</code> <p>Normalizes output derivatives.</p> <code>normalization_controls</code> <code>NormalizationLayer1D</code> <p>Normalizes control inputs.</p> <code>normalization_parameters</code> <code>NormalizationLayer1D</code> <p>Normalizes parameters.</p> <code>system_nn</code> <code>Sequential</code> <p>Neural network for derivative function.</p> <p>Forward Args:</p> <pre><code>states (torch.Tensor): State tensor of shape [batch_size, states_dim].\nparameters (torch.Tensor, optional): Parameters [batch_size, parameters_dim].\ncontrols (torch.Tensor, optional): Controls [batch_size, controls_dim].\n</code></pre> <p>Returns:</p> Type Description <p>Tuple of (states_der, states_der_norm): - states_der (torch.Tensor): Denormalized state derivatives [batch_size, states_dim] - states_der_norm (torch.Tensor): Normalized state derivatives [batch_size, states_dim]</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If normalization layers of states_der are not initialized before  forward pass. If no pre-training is done, this normalization layer must not  be initialized.</p> Source code in <code>src/bnode_core/ode/node/node_architecture.py</code> <pre><code>class NeuralODEFunc(nn.Module):\n    \"\"\"Neural network function representing ODE right-hand side f_\u03b8(x, u, p).\n\n    This module learns the state derivative function for a dynamical system:\n\n        dx/dt = f_\u03b8(x, u, p)\n\n    The network includes normalization layers for numerical stability and supports\n    optional control inputs and system parameters.\n\n    Args:\n        states_dim (int): Dimension of state vector x.\n        controls_dim (int, optional): Dimension of control input vector u. Default: 0.\n        parameters_dim (int, optional): Dimension of parameter vector p. Default: 0.\n        hidden_dim (int, optional): Hidden layer dimension. Default: 20.\n        n_layers (int, optional): Number of layers (minimum 2). Default: 3.\n        activation (nn.Module, optional): Activation function class. Default: nn.ELU.\n        intialization (str, optional): Weight initialization method\n            ('identity', 'xavier', 'kaiming', 'orthogonal'). Default: 'identity'.\n\n    Attributes:\n        states_dim (int): Dimension of states.\n        controls_dim (int): Dimension of controls.\n        parameters_dim (int): Dimension of parameters.\n        include_controls (bool): Whether controls are used.\n        include_parameters (bool): Whether parameters are used.\n        normalization_states (NormalizationLayer1D): Normalizes input states.\n        normalization_states_der (NormalizationLayer1D): Normalizes output derivatives.\n        normalization_controls (NormalizationLayer1D): Normalizes control inputs.\n        normalization_parameters (NormalizationLayer1D): Normalizes parameters.\n        system_nn (nn.Sequential): Neural network for derivative function.\n\n    Forward Args:\n\n        states (torch.Tensor): State tensor of shape [batch_size, states_dim].\n        parameters (torch.Tensor, optional): Parameters [batch_size, parameters_dim].\n        controls (torch.Tensor, optional): Controls [batch_size, controls_dim].\n\n    Returns:\n        Tuple of (states_der, states_der_norm):\n            - states_der (torch.Tensor): Denormalized state derivatives [batch_size, states_dim]\n            - states_der_norm (torch.Tensor): Normalized state derivatives [batch_size, states_dim]\n\n    Raises:\n        AssertionError: If normalization layers of states_der are not initialized before \n            forward pass. If no pre-training is done, this normalization layer must not \n            be initialized.\n\n    \"\"\"\n\n    def __init__(self,\n                states_dim,\n                controls_dim: int = 0,\n                parameters_dim: int = 0,\n                hidden_dim: int = 20,\n                n_layers: int = 3,\n                activation: nn.Module = nn.ELU,\n                intialization: str = 'identity',\n                ):\n        super().__init__()\n        self.states_dim = states_dim\n\n        self.controls_dim = controls_dim\n        self.parameters_dim = parameters_dim\n\n        self.include_controls = True if controls_dim &gt; 0 else False\n        self.include_parameters = True if parameters_dim &gt; 0 else False\n\n        self.normalization_states = NormalizationLayer1D(num_features=states_dim)\n        self.normalization_states_der = NormalizationLayer1D(num_features=states_dim)\n        self.normalization_controls = NormalizationLayer1D(num_features=controls_dim) if self.include_controls else None\n        self.normalization_parameters = NormalizationLayer1D(num_features=parameters_dim) if self.include_parameters else None\n\n        # initialize system dynamics\n        modules =[\n            nn.Linear(states_dim + controls_dim + parameters_dim, hidden_dim),\n            activation()\n        ]\n        if n_layers &lt; 2:\n            logging.warning('n_layers must be at least 2, setting n_layers to 2')#\n        for i in range(n_layers-2):\n            modules.append(nn.Linear(hidden_dim, hidden_dim))\n            modules.append(activation())\n        modules.append(nn.Linear(hidden_dim, states_dim))\n        self.system_nn = nn.Sequential(*modules)\n\n        initialize_weights_biases(self.system_nn, method=intialization)\n\n    def forward(self, states: torch.Tensor, parameters: Optional[torch.Tensor] = None, controls: Optional[torch.Tensor] = None) -&gt; tuple[torch.Tensor, torch.Tensor]:\n\n        assert self.normalization_states_der._initialized, \"the states derivative normalization layer must be initialized before calling the forward pass\"\n\n        # normalize inputs\n        states = self.normalization_states(states)\n        if self.include_controls:\n            controls = self.normalization_controls(controls)\n        if self.include_parameters:\n            parameters = self.normalization_parameters(parameters)\n\n        # concatenate inputs\n        if self.include_controls and self.include_parameters:\n            x = torch.cat((states, controls, parameters), dim=1)\n        elif self.include_controls:\n            x = torch.cat((states, controls), dim=1)\n        elif self.include_parameters:\n            x = torch.cat((states, parameters), dim=1)\n        else:\n            x = states\n\n        # forward pass system dynamics\n        states_der_norm = self.system_nn(x)\n\n        # denormalize output\n        states_der = self.normalization_states_der(states_der_norm, denormalize=True)\n\n        return states_der, states_der_norm\n</code></pre>"},{"location":"bnode_core/ode/node/#bnode_core.ode.node.node_architecture.OutputNetwork","title":"<code>OutputNetwork</code>","text":"<p>               Bases: <code>Module</code></p> <p>Neural network mapping states to observable outputs.</p> <p>This module learns the observation function that maps internal states to measurable outputs:     y = g_\u03b8(x, u, p)</p> <p>Useful when system states are not directly observable or when outputs represent derived quantities.</p> <p>Parameters:</p> Name Type Description Default <code>states_dim</code> <code>int</code> <p>Dimension of state vector x.</p> required <code>outputs_dim</code> <code>int</code> <p>Dimension of output vector y.</p> required <code>controls_dim</code> <code>int</code> <p>Dimension of control inputs. Default: 0.</p> <code>0</code> <code>parameters_dim</code> <code>int</code> <p>Dimension of parameters. Default: 0.</p> <code>0</code> <code>controls_to_output</code> <code>bool</code> <p>Whether to include controls in output mapping. Default: False.</p> <code>False</code> <code>hidden_dim</code> <code>int</code> <p>Hidden layer dimension. Default: 20.</p> <code>20</code> <code>n_layers</code> <code>int</code> <p>Number of layers (minimum 2). Default: 3.</p> <code>3</code> <code>activation</code> <code>Module</code> <p>Activation function. Default: nn.ELU.</p> <code>ELU</code> <code>intialization</code> <code>str</code> <p>Weight initialization method. Default: 'identity'.</p> <code>'identity'</code> <p>Attributes:</p> Name Type Description <code>states_dim</code> <code>int</code> <p>Dimension of states.</p> <code>outputs_dim</code> <code>int</code> <p>Dimension of outputs.</p> <code>controls_dim</code> <code>int</code> <p>Dimension of controls (0 if not used).</p> <code>parameters_dim</code> <code>int</code> <p>Dimension of parameters.</p> <code>include_parameters</code> <code>bool</code> <p>Whether parameters are used.</p> <code>controls_to_output</code> <code>bool</code> <p>Whether controls feed into output network.</p> <code>normalization_states</code> <code>NormalizationLayer1D</code> <p>Normalizes states.</p> <code>normalization_controls</code> <code>NormalizationLayer1D</code> <p>Normalizes controls.</p> <code>normalization_parameters</code> <code>NormalizationLayer1D</code> <p>Normalizes parameters.</p> <code>normalization_outputs</code> <code>NormalizationLayer1D</code> <p>Normalizes outputs.</p> <code>output_nn</code> <code>Sequential</code> <p>Neural network for output mapping.</p> Forward Args <p>states (torch.Tensor): States [batch_size, states_dim]. parameters (torch.Tensor, optional): Parameters [batch_size, parameters_dim]. controls (torch.Tensor, optional): Controls [batch_size, controls_dim].</p> <p>Returns:</p> Type Description <p>Tuple of (outputs, outputs_norm): - outputs (torch.Tensor): Denormalized outputs [batch_size, outputs_dim] - outputs_norm (torch.Tensor): Normalized outputs [batch_size, outputs_dim]</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If normalization of outputs are not initialized before forward pass.</p> Source code in <code>src/bnode_core/ode/node/node_architecture.py</code> <pre><code>class OutputNetwork(nn.Module):\n    \"\"\"Neural network mapping states to observable outputs.\n\n    This module learns the observation function that maps internal states to\n    measurable outputs:\n        y = g_\u03b8(x, u, p)\n\n    Useful when system states are not directly observable or when outputs\n    represent derived quantities.\n\n    Args:\n        states_dim (int): Dimension of state vector x.\n        outputs_dim (int): Dimension of output vector y.\n        controls_dim (int, optional): Dimension of control inputs. Default: 0.\n        parameters_dim (int, optional): Dimension of parameters. Default: 0.\n        controls_to_output (bool, optional): Whether to include controls in\n            output mapping. Default: False.\n        hidden_dim (int, optional): Hidden layer dimension. Default: 20.\n        n_layers (int, optional): Number of layers (minimum 2). Default: 3.\n        activation (nn.Module, optional): Activation function. Default: nn.ELU.\n        intialization (str, optional): Weight initialization method. Default: 'identity'.\n\n    Attributes:\n        states_dim (int): Dimension of states.\n        outputs_dim (int): Dimension of outputs.\n        controls_dim (int): Dimension of controls (0 if not used).\n        parameters_dim (int): Dimension of parameters.\n        include_parameters (bool): Whether parameters are used.\n        controls_to_output (bool): Whether controls feed into output network.\n        normalization_states (NormalizationLayer1D): Normalizes states.\n        normalization_controls (NormalizationLayer1D): Normalizes controls.\n        normalization_parameters (NormalizationLayer1D): Normalizes parameters.\n        normalization_outputs (NormalizationLayer1D): Normalizes outputs.\n        output_nn (nn.Sequential): Neural network for output mapping.\n\n    Forward Args:\n        states (torch.Tensor): States [batch_size, states_dim].\n        parameters (torch.Tensor, optional): Parameters [batch_size, parameters_dim].\n        controls (torch.Tensor, optional): Controls [batch_size, controls_dim].\n\n    Returns:\n        Tuple of (outputs, outputs_norm):\n            - outputs (torch.Tensor): Denormalized outputs [batch_size, outputs_dim]\n            - outputs_norm (torch.Tensor): Normalized outputs [batch_size, outputs_dim]\n\n    Raises:\n        AssertionError: If normalization of outputs are not initialized before forward pass.\n\n\n    \"\"\"\n\n    def __init__(self,\n                states_dim,\n                outputs_dim,\n                controls_dim: int = 0,\n                parameters_dim: int = 0,\n                controls_to_output: bool = False,\n                hidden_dim: int = 20,\n                n_layers: int = 3,\n                activation: nn.Module = nn.ELU,\n                intialization: str = 'identity',\n                ):\n        super().__init__()\n\n        self.states_dim = states_dim\n        self.outputs_dim = outputs_dim\n        self.controls_dim = controls_dim if controls_to_output else 0\n        self.parameters_dim = parameters_dim\n\n        self.include_parameters = True if parameters_dim &gt; 0 else False\n        self.controls_to_output = controls_to_output if controls_dim &gt; 0 else False\n\n        self.normalization_states = NormalizationLayer1D(num_features=states_dim)\n        self.normalization_controls = NormalizationLayer1D(num_features=controls_dim) if self.controls_to_output else None\n        self.normalization_parameters = NormalizationLayer1D(num_features=parameters_dim) if self.include_parameters else None\n        self.normalization_outputs = NormalizationLayer1D(num_features=outputs_dim)\n\n        # initialize output nn\n        modules = [\n            nn.Linear(self.states_dim + self.controls_dim + self.parameters_dim, hidden_dim),\n            activation()\n        ]\n        if n_layers &lt; 2:\n            logging.warning('n_layers must be at least 2, setting n_layers to 2')\n        for i in range(n_layers-2):\n            modules.append(nn.Linear(hidden_dim, hidden_dim))\n            modules.append(activation())\n        modules.append(nn.Linear(hidden_dim, outputs_dim))\n        self.output_nn = nn.Sequential(*modules)\n\n        initialize_weights_biases(self.output_nn, method=intialization)\n\n    def forward(self, states: torch.Tensor, parameters: Optional[torch.Tensor] = None, controls: Optional[torch.Tensor] = None) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        assert self.normalization_outputs._initialized, \"the outputs normalization layer must be initialized before calling the forward pass\"\n\n        # normalize inputs\n        states = self.normalization_states(states)\n        if self.controls_to_output:\n            controls = self.normalization_controls(controls)\n        if self.include_parameters:\n            parameters = self.normalization_parameters(parameters)\n\n        # concatenate inputs\n        if self.controls_to_output and self.include_parameters:\n            x = torch.cat((states, controls, parameters), dim=1)\n        elif self.controls_to_output:\n            x = torch.cat((states, controls), dim=1)\n        elif self.include_parameters:\n            x = torch.cat((states, parameters), dim=1)\n        else:\n            x = states\n\n        # forward pass system dynamics\n        outputs_norm = self.output_nn(x)\n\n        # denormalize output\n        outputs = self.normalization_outputs(outputs_norm, denormalize=True)\n\n        return outputs, outputs_norm\n</code></pre>"},{"location":"bnode_core/ode/node/#bnode_core.ode.node.node_architecture.NeuralODE","title":"<code>NeuralODE</code>","text":"<p>               Bases: <code>Module</code></p> <p>Complete Neural ODE model for dynamical system learning.</p> <p>Main class that combines the ODE function, output network, and ODE solver for training and inference on continuous-time dynamical systems.</p> The model learns <p>dx/dt = f_\u03b8(x, u, p)    (ODE function) y = g_\u03c6(x, u, p)        (Output function, optional)</p> <p>And integrates: x(t) = x(t\u2080) + \u222b[t\u2080,t] f_\u03b8(x(\u03c4), u(\u03c4), p) d\u03c4</p> <p>Parameters:</p> Name Type Description Default <code>states_dim</code> <code>int</code> <p>Dimension of state vector x.</p> required <code>controls_dim</code> <code>int</code> <p>Dimension of control inputs u. Default: 0.</p> <code>0</code> <code>parameters_dim</code> <code>int</code> <p>Dimension of parameters p. Default: 0.</p> <code>0</code> <code>outputs_dim</code> <code>int</code> <p>Dimension of outputs y. Default: 0.</p> <code>0</code> <code>controls_to_output_nn</code> <code>bool</code> <p>Include controls in output mapping. Default: False.</p> <code>False</code> <code>hidden_dim</code> <code>int</code> <p>Hidden dimension for ODE network. Default: 20.</p> <code>20</code> <code>n_layers</code> <code>int</code> <p>Layers in ODE network. Default: 3.</p> <code>3</code> <code>hidden_dim_output_nn</code> <code>int</code> <p>Hidden dimension for output network. Default: 20.</p> <code>20</code> <code>n_layers_output_nn</code> <code>int</code> <p>Layers in output network. Default: 2.</p> <code>2</code> <code>activation</code> <code>Module</code> <p>Activation function. Default: nn.ELU.</p> <code>ELU</code> <code>intialization</code> <code>str</code> <p>Initialization for output network. Default: 'identity'.</p> <code>'identity'</code> <code>initialization_ode</code> <code>str</code> <p>Initialization for ODE network. Default: 'identity'.</p> <code>'identity'</code> <p>Attributes:</p> Name Type Description <code>include_controls</code> <code>bool</code> <p>Whether model uses controls.</p> <code>include_parameters</code> <code>bool</code> <p>Whether model uses parameters.</p> <code>include_outputs</code> <code>bool</code> <p>Whether model has output network.</p> <code>ode_fun_count</code> <code>int</code> <p>Counter for ODE function evaluations.</p> <code>NeuralODEFunc</code> <code>NeuralODEFunc</code> <p>ODE right-hand side function.</p> <code>OutputNetwork</code> <code>OutputNetwork</code> <p>Output mapping network (if outputs_dim &gt; 0).</p> <code>current_controls</code> <code>Tensor</code> <p>Controls for current integration.</p> <code>current_times</code> <code>Tensor</code> <p>Time points for current integration.</p> <code>current_parameters</code> <code>Tensor</code> <p>Parameters for current integration.</p> <p>Methods:</p> Name Description <code>normalization_init</code> <p>Initialize normalization from HDF5 dataset.</p> <code>forward</code> <p>Forward pass.</p> <code>set_input</code> <p>Set inputs for ODE integration.</p> <code>forward_ODE</code> <p>ODE function compatible with torchdiffeq.</p> <code>model_and_loss_evaluation</code> <p>Compute loss for training/testing.</p> <code>get_progress_string</code> <p>Format training progress string.</p> <code>save</code> <p>Save model checkpoint.</p> <code>load</code> <p>Load model checkpoint.</p> Training Modes <p>Pre-training (pre_training=True):     - Uses state derivatives directly from data     - Bypasses ODE solver for fast training     - Loss: ||dx/dt - f_\u03b8(x)||\u00b2 + ||y - g_\u03c6(x)||\u00b2     - Requires 'states_der' in dataset</p> <p>Main training (pre_training=False):     - Integrates ODE from initial conditions     - Uses torchdiffeq solver (Euler, RK4, dopri5, etc.)     - Loss: ||x(t) - x\u0302(t)||\u00b2 + ||y(t) - \u0177(t)||\u00b2     - More accurate but computationally expensive</p> Loss Components <ul> <li>loss_states: MSE between true and predicted states (normalized)</li> <li>loss_outputs: MSE between true and predicted outputs (normalized)</li> <li>loss_states_der: MSE for state derivatives (pre-training only)</li> <li>rmse_states: RMSE for states (main training only)</li> <li>rmse_outputs: RMSE for outputs (main training only)</li> </ul> Solver Options <p>Configured via train_cfg:     - solver: Solver name ('euler', 'rk4', 'dopri5', etc.)     - solver_rtol: Relative tolerance for adaptive solvers     - solver_atol: Absolute tolerance for adaptive solvers     - solver_norm: Norm for step size control ('max', 'mixed')     - use_adjoint: Use adjoint method for backpropagation     - evaluate_at_control_times: Force evaluation at control time points</p> Notes <ul> <li>Normalization must be initialized before any forward pass</li> <li>Adjoint method saves memory but may be slower for small models</li> <li>Control interpolation uses nearest-neighbor for robustness</li> <li>ODE function counter tracks solver efficiency</li> <li>All tensors use normalized values internally for stability</li> </ul> See Also <ul> <li><code>NeuralODEFunc</code>: ODE right-hand side network</li> <li><code>OutputNetwork</code>: Output mapping network</li> <li><code>bnode_core.ode.trainer</code>: Training pipeline</li> <li><code>torchdiffeq.odeint</code>: ODE solver</li> </ul> Source code in <code>src/bnode_core/ode/node/node_architecture.py</code> <pre><code>class NeuralODE(nn.Module):\n    \"\"\"Complete Neural ODE model for dynamical system learning.\n\n    Main class that combines the ODE function, output network, and ODE solver\n    for training and inference on continuous-time dynamical systems.\n\n    The model learns:\n        dx/dt = f_\u03b8(x, u, p)    (ODE function)\n        y = g_\u03c6(x, u, p)        (Output function, optional)\n\n    And integrates: x(t) = x(t\u2080) + \u222b[t\u2080,t] f_\u03b8(x(\u03c4), u(\u03c4), p) d\u03c4\n\n    Args:\n        states_dim (int): Dimension of state vector x.\n        controls_dim (int, optional): Dimension of control inputs u. Default: 0.\n        parameters_dim (int, optional): Dimension of parameters p. Default: 0.\n        outputs_dim (int, optional): Dimension of outputs y. Default: 0.\n        controls_to_output_nn (bool, optional): Include controls in output mapping. Default: False.\n        hidden_dim (int, optional): Hidden dimension for ODE network. Default: 20.\n        n_layers (int, optional): Layers in ODE network. Default: 3.\n        hidden_dim_output_nn (int, optional): Hidden dimension for output network. Default: 20.\n        n_layers_output_nn (int, optional): Layers in output network. Default: 2.\n        activation (nn.Module, optional): Activation function. Default: nn.ELU.\n        intialization (str, optional): Initialization for output network. Default: 'identity'.\n        initialization_ode (str, optional): Initialization for ODE network. Default: 'identity'.\n\n    Attributes:\n        include_controls (bool): Whether model uses controls.\n        include_parameters (bool): Whether model uses parameters.\n        include_outputs (bool): Whether model has output network.\n        ode_fun_count (int): Counter for ODE function evaluations.\n        NeuralODEFunc (NeuralODEFunc): ODE right-hand side function.\n        OutputNetwork (OutputNetwork): Output mapping network (if outputs_dim &gt; 0).\n        current_controls (torch.Tensor): Controls for current integration.\n        current_times (torch.Tensor): Time points for current integration.\n        current_parameters (torch.Tensor): Parameters for current integration.\n\n    Methods:\n        normalization_init(dataset): Initialize normalization from HDF5 dataset.\n        forward(states, parameters, controls, pre_training, times): Forward pass.\n        set_input(controls, times, parameters): Set inputs for ODE integration.\n        forward_ODE(t, x): ODE function compatible with torchdiffeq.\n        model_and_loss_evaluation(...): Compute loss for training/testing.\n        get_progress_string(...): Format training progress string.\n        save(path): Save model checkpoint.\n        load(path, device): Load model checkpoint.\n\n    Training Modes:\n        **Pre-training (pre_training=True)**:\n            - Uses state derivatives directly from data\n            - Bypasses ODE solver for fast training\n            - Loss: ||dx/dt - f_\u03b8(x)||\u00b2 + ||y - g_\u03c6(x)||\u00b2\n            - Requires 'states_der' in dataset\n\n        **Main training (pre_training=False)**:\n            - Integrates ODE from initial conditions\n            - Uses torchdiffeq solver (Euler, RK4, dopri5, etc.)\n            - Loss: ||x(t) - x\u0302(t)||\u00b2 + ||y(t) - \u0177(t)||\u00b2\n            - More accurate but computationally expensive\n\n    Loss Components:\n        - **loss_states**: MSE between true and predicted states (normalized)\n        - **loss_outputs**: MSE between true and predicted outputs (normalized)\n        - **loss_states_der**: MSE for state derivatives (pre-training only)\n        - **rmse_states**: RMSE for states (main training only)\n        - **rmse_outputs**: RMSE for outputs (main training only)\n\n    Solver Options:\n        Configured via train_cfg:\n            - **solver**: Solver name ('euler', 'rk4', 'dopri5', etc.)\n            - **solver_rtol**: Relative tolerance for adaptive solvers\n            - **solver_atol**: Absolute tolerance for adaptive solvers\n            - **solver_norm**: Norm for step size control ('max', 'mixed')\n            - **use_adjoint**: Use adjoint method for backpropagation\n            - **evaluate_at_control_times**: Force evaluation at control time points\n\n\n    Notes:\n        - Normalization must be initialized before any forward pass\n        - Adjoint method saves memory but may be slower for small models\n        - Control interpolation uses nearest-neighbor for robustness\n        - ODE function counter tracks solver efficiency\n        - All tensors use normalized values internally for stability\n\n    See Also:\n        - ``NeuralODEFunc``: ODE right-hand side network\n        - ``OutputNetwork``: Output mapping network\n        - ``bnode_core.ode.trainer``: Training pipeline\n        - ``torchdiffeq.odeint``: ODE solver\n    \"\"\"\n\n    def __init__(self,\n                states_dim,\n                controls_dim: int = 0,\n                parameters_dim: int = 0,\n                outputs_dim: int = 0, \n                controls_to_output_nn: bool = False,\n                hidden_dim: int = 20,\n                n_layers: int = 3,\n                hidden_dim_output_nn: int = 20,\n                n_layers_output_nn: int = 2,\n                activation: nn.Module = nn.ELU,\n                intialization: str = 'identity',\n                initialization_ode: str = 'identity',\n                ): \n        super().__init__()\n\n        self.include_controls = True if controls_dim &gt; 0 else False\n        self.include_parameters = True if parameters_dim &gt; 0 else False\n        self.include_outputs = True if outputs_dim &gt; 0 else False\n\n        self.ode_fun_count = 0\n\n        self.NeuralODEFunc = NeuralODEFunc(states_dim=states_dim,\n                                            controls_dim=controls_dim,\n                                            parameters_dim=parameters_dim,\n                                            hidden_dim=hidden_dim,\n                                            n_layers=n_layers,\n                                            activation=activation,\n                                            intialization=initialization_ode)\n        self.OutputNetwork = OutputNetwork(states_dim=states_dim,\n                                            outputs_dim=outputs_dim,\n                                            controls_dim=controls_dim,\n                                            parameters_dim=parameters_dim,\n                                            controls_to_output=controls_to_output_nn,\n                                            hidden_dim=hidden_dim_output_nn,\n                                            n_layers=n_layers_output_nn,\n                                            activation=activation,\n                                            intialization=intialization) if outputs_dim &gt; 0 else None\n\n    def normalization_init(self, dataset: h5py.File):\n        # initialize normalization layers\n        def reshape_array(array):\n            arr = array.transpose(1,0,2).reshape(array.shape[1],-1).transpose(1,0)\n            return arr\n\n        # states\n        self.NeuralODEFunc.normalization_states.initialize_normalization(reshape_array(dataset['train']['states'][:]))\n        if self.include_outputs:\n            self.OutputNetwork.normalization_states.initialize_normalization(reshape_array(dataset['train']['states'][:]))\n\n        # states derivative\n        if False: # 'states_der' in dataset['train'].keys(): # removed on 2024-11-25 as states_der could be approximated badly by FMU\n            _data = dataset['train']['states_der'][:] # TODO maybe remove this as states_der can be approximated badly by FMU?\n        else:\n            _data = (dataset['train']['states'][:,:,1:] - dataset['train']['states'][:,:,:-1]) / (dataset['time'][1:] - dataset['time'][:-1]).reshape(1,1,-1)\n        self.NeuralODEFunc.normalization_states_der.initialize_normalization(reshape_array(_data))\n\n        # controls\n        if self.include_controls:\n            self.NeuralODEFunc.normalization_controls.initialize_normalization(reshape_array(dataset['train']['controls'][:]))\n            if self.include_outputs:\n                if self.OutputNetwork.controls_to_output:\n                    self.OutputNetwork.normalization_controls.initialize_normalization(reshape_array(dataset['train']['controls'][:]))\n        # parameters\n        if self.include_parameters:\n            self.NeuralODEFunc.normalization_parameters.initialize_normalization(dataset['train']['parameters'][:])\n            self.OutputNetwork.normalization_parameters.initialize_normalization(dataset['train']['parameters'][:])\n\n        # outputs\n        if self.include_outputs:\n            self.OutputNetwork.normalization_outputs.initialize_normalization(reshape_array(dataset['train']['outputs'][:]))\n\n        logging.info('Initialized normalization layers')\n\n    def forward(self, \n                states, \n                parameters = None, \n                controls = None, \n                pre_training = False,\n                times = None):\n        if pre_training is True:\n            states_der, states_der_norm = self.NeuralODEFunc(states, parameters, controls)\n            outputs, outputs_norm = self.OutputNetwork(states, parameters, controls) if self.include_outputs else (None, None)\n            return states_der, outputs, states_der_norm, outputs_norm\n        else:\n            states_der, _ = self.NeuralODEFunc(states, parameters, controls)\n            # the output network needs to be called from external with NeuralOde.OutputNetwork(states, parameters, controls)\n            return states_der\n\n    def set_input(self, controls=None, times=None, parameters=None):\n        self.current_controls = controls\n        self.current_times = times\n        self.current_parameters = parameters\n\n    def forward_ODE(self, t, x):\n        if self.include_controls:\n            try:\n                idx = torch.nonzero(self.current_times[0][0] &gt; t)\n                if len(idx) == 0:\n                    idx = -1\n                else:\n                    idx = idx[0][0] - 1\n                u = self.current_controls[:,:,idx]\n            except:\n                u = self.current_controls[:,:,-1]\n                warnings.warn('something went wrong when trying to get the control input at time t, using the last control input instead')\n        else:\n            u = None\n        # call\n        x_dot = self.__call__(x.swapaxes(0,1), self.current_parameters, u)\n        x_dot = x_dot.swapaxes(0,1)\n        self.ode_fun_count += 1\n        return x_dot\n\n    def model_and_loss_evaluation(self, data_batch, train_cfg, pre_train, device, return_model_outputs, test = False, last_batch=True, activate_deterministic_mode = False): # last two arguments for compatibility with trainer\n        # get data\n        time = data_batch['time'].to(device)\n        states = data_batch['states'].to(device)\n        if 'states_der' in data_batch.keys():\n            states_der = data_batch['states_der'].to(device)\n        parameters = data_batch['parameters'].to(device) if 'parameters' in data_batch.keys() else None\n        controls = data_batch['controls'].to(device) if 'controls' in data_batch.keys() else None\n        outputs = data_batch['outputs'].to(device) if 'outputs' in data_batch.keys() else None\n        # squeeze data if in pre_train\n        if pre_train is True:\n            time = time.squeeze(2)\n            states = states.squeeze(2)\n            states_der = states_der.squeeze(2)\n            controls = controls.squeeze(2) if controls is not None else None\n            outputs = outputs.squeeze(2) if outputs is not None else None\n\n        \"\"\"pre-training\"\"\"\n        if pre_train is True:\n            # forward pass\n            states_der_hat, outputs_hat, states_der_norm_hat, outputs_norm_hat = self.__call__(states, parameters, controls, pre_training = True)\n            # get norms\n            states_der_norm = self.NeuralODEFunc.normalization_states_der(states_der).detach()\n            outputs_norm = self.OutputNetwork.normalization_outputs(outputs).detach() if self.include_outputs else None\n            # calculate losses\n            loss_states_der = torch.mean(torch.square((states_der_norm - states_der_norm_hat)))\n            loss_outputs = torch.mean(torch.square(outputs_norm - outputs_norm_hat)) if self.include_outputs else torch.zeros(1).to(device)\n            loss = loss_states_der + loss_outputs\n            # make return values\n            ret_val = {\n                'loss': loss,\n                'loss_states_der': loss_states_der,\n                'loss_outputs': loss_outputs,\n            }\n            if return_model_outputs:\n                model_outputs = {\n                    'states_der_hat': states_der_hat,\n                    'outputs_hat': outputs_hat,\n                }\n            if test is True:\n                # call value.item() for each value in return_value\n                ret_val = dict({key: value.item() for key, value in ret_val.items()})\n            # detach model outputs from computational graph\n            if return_model_outputs:\n                model_outputs = dict({key: value.cpu().detach().numpy() for key, value in model_outputs.items()})\n            return ret_val if return_model_outputs is False else (ret_val, model_outputs)\n\n        \"\"\"training\"\"\"\n        if pre_train is False:\n            _time_logging0 = pyTime.time()\n            # provide inputs to NeuralODE\n            self.set_input(controls, time, parameters)\n            self.ode_fun_count = 0\n            # forward pass\n            x0 = states[:, :, 0].swapaxes(0,1) # x is shape [batch_size, states_dim, timeseries_length], but for odeint it must be [states_dim, batch_size]\n            time = time[0,0,:] # as we used equidistant time steps in data generation, we can use the first time vector\n            # specify options for odeint\n            _fixed_step_solvers = ['euler', 'midpoint', 'rk4', 'implicit_adams', 'explicit_adams']\n            _base_options = {}\n            if train_cfg.solver_norm == 'max':\n                _base_options['norm'] = torchdiffeq._impl.misc._linf_norm\n            elif train_cfg.solver_norm == 'mixed':\n                _base_options['norm'] = _mixed_norm_tensor\n            if self.include_controls or train_cfg.evaluate_at_control_times is True:\n                if train_cfg.evaluate_at_control_times is True:\n                    if train_cfg.solver in _fixed_step_solvers:\n                        _base_options['perturb'] = True\n                    else:\n                        _base_options['jump_t'] = time\n            options = _base_options.copy()\n\n            if train_cfg.use_adjoint is True and train_cfg.solver not in ['euler', 'midpoint', 'rk4', 'implicit_adams', 'explicit_adams']:\n                adjoint_options = _base_options.copy()\n                adjoint_options['norm'] = 'seminorm'\n                states_hat = odeint_adjoint(self.forward_ODE, x0, time,\n                                method=train_cfg.solver, \n                                rtol = train_cfg.solver_rtol, \n                                atol = train_cfg.solver_atol,\n                                adjoint_params=self.parameters(),\n                                adjoint_options=adjoint_options,\n                                options=options)\n            else:\n                states_hat = odeint(self.forward_ODE, x0, time, \n                                method=train_cfg.solver,\n                                rtol = train_cfg.solver_rtol, \n                                atol = train_cfg.solver_atol,\n                                options=options)\n            time_odeint = pyTime.time() - _time_logging0\n            _time_logging0 = pyTime.time()\n            ode_calls_forward = self.ode_fun_count\n            self.ode_fun_count = 0 # reset ode_fun_count for adjoint pass\n            # x is of shape [timeseries_length, states_dim, batch_size], but we need [batch_size, states_dim, timeseries_length]\n            states_hat = states_hat.swapaxes(0,2)\n            # calculate outputs\n            if self.include_outputs:\n                outputs_hat = torch.empty((states_hat.shape[0], self.OutputNetwork.outputs_dim , states_hat.shape[2])).to(device)\n                for i in range(states_hat.shape[2]):\n                    if self.OutputNetwork.controls_to_output is True:\n                        outputs_hat[:,:,i], _ = self.OutputNetwork(states = states_hat[:,:,i], controls = controls[:,:,i], parameters = parameters)\n                    else:\n                        outputs_hat[:,:,i], _ = self.OutputNetwork(states_hat[:,:,i], controls = None, parameters = parameters)\n            time_outputs = pyTime.time() - _time_logging0\n            # maybe something like this is faster:\n            # test = x_norm.reshape(2,-1).swapaxes(0,1)\n            # print(test.shape)\n            # test = test.swapaxes(0,1).reshape(512,2,1000)\n            # print(test.shape)\n\n            # calculate loss\n            # normalize states and outputs\n            states_norm = torch.empty_like(states).to(device)\n            outputs_norm = torch.empty_like(outputs).to(device) if self.include_outputs else None\n            states_hat_norm = torch.empty_like(states_hat).to(device)\n            outputs_hat_norm = torch.empty_like(outputs_hat).to(device) if self.include_outputs else None\n\n            for i in range(states.shape[2]):\n                states_norm[:,:,i] = self.NeuralODEFunc.normalization_states(states[:,:,i])\n                states_hat_norm[:,:,i] = self.NeuralODEFunc.normalization_states(states_hat[:,:,i])\n                if self.include_outputs:\n                    outputs_norm[:,:,i] = self.OutputNetwork.normalization_outputs(outputs[:,:,i])\n                    outputs_hat_norm[:,:,i] = self.OutputNetwork.normalization_outputs(outputs_hat[:,:,i])\n\n            loss_states = torch.mean(torch.square((states_norm - states_hat_norm)))\n            loss_outputs = torch.mean(torch.square(outputs_norm - outputs_hat_norm)) if self.include_outputs else torch.zeros(1).to(device)\n            loss = loss_states + loss_outputs\n            rmse_states = torch.sqrt(torch.mean(torch.square((states_norm - states_hat_norm))))\n            rmse_outputs = torch.sqrt(torch.mean(torch.square(outputs_norm - outputs_hat_norm))) if self.include_outputs else torch.zeros(1).to(device)\n            # make ret_vals\n            ret_val = {\n                'loss': loss,\n                'loss_states': loss_states,\n                'loss_outputs': loss_outputs,\n                'rmse_states': rmse_states,\n                'rmse_outputs': rmse_outputs,\n            }\n            if test is True:\n                # call value.item() for each value in return_value\n                ret_val = dict({key: value.item() for key, value in ret_val.items()})\n            ret_val['time_odeint'] = time_odeint\n            ret_val['time_outputs'] = time_outputs\n            ret_val['ode_calls_forward'] = ode_calls_forward\n            # append model output if necessary\n            if return_model_outputs:\n                model_outputs = {\n                    'states_hat': states_hat,\n                }\n                if self.include_outputs:\n                    model_outputs['outputs_hat'] = outputs_hat\n            # detach model outputs from computational graph\n            if return_model_outputs:\n                model_outputs = dict({key: value.cpu().detach().numpy() for key, value in model_outputs.items()})\n            return ret_val if return_model_outputs is False else (ret_val, model_outputs)\n\n    def get_progress_string(self, ret_vals_train, ret_vals_validation, ret_vals_test, pre_train):\n        if pre_train is True:\n            _str = '[train/val/test] loss: {:.5f}/{:.5f}/{:.5f} | loss_states_der: {:.5f}/{:.5f}/{:.5f} | loss_outputs: {:.5f}/{:.5f}/{:.5f}'.format(\n            ret_vals_train['loss'], ret_vals_validation['loss'], ret_vals_test['loss'],\n            ret_vals_train['loss_states_der'], ret_vals_validation['loss_states_der'], ret_vals_test['loss_states_der'],\n            ret_vals_train['loss_outputs'], ret_vals_validation['loss_outputs'], ret_vals_test['loss_outputs']\n        )\n        else:\n            try:\n                _str =  '[train/val/test] loss: {:.5f}/{:.5f}/{:.5f} | loss_states: {:.5f}/{:.5f}/{:.5f} | loss_outputs: {:.5f}/{:.5f}/{:.5f} | \\n \\t \\t|rmse_states: {:.3f}/{:.3f}/{:.3f} | rmse_outputs: {:.3f}/{:.3f}/{:.3f} | time_forward: {:.5f} | time_backward: {:.5f}'.format(\n                    ret_vals_train['loss'], ret_vals_validation['loss'], ret_vals_test['loss'],\n                    ret_vals_train['loss_states'], ret_vals_validation['loss_states'], ret_vals_test['loss_states'],\n                    ret_vals_train['loss_outputs'], ret_vals_validation['loss_outputs'], ret_vals_test['loss_outputs'],\n                    ret_vals_train['rmse_states'], ret_vals_validation['rmse_states'], ret_vals_test['rmse_states'],\n                    ret_vals_train['rmse_outputs'], ret_vals_validation['rmse_outputs'], ret_vals_test['rmse_outputs'],\n                    ret_vals_train['time_forward'], ret_vals_train['time_backward']\n                )\n            except:\n                _str = 'error in get_progress_string'\n        return _str\n\n    def save(self, path: Path):\n        if not path.parent.exists():\n            path.parent.mkdir(parents=True)\n        torch.save(self.state_dict(), path)\n        logging.info('\\t \\t \\tSaved model to {}'.format(path))\n\n    def load(self, path: Path, device: torch.device):\n        self.load_state_dict(torch.load(path, map_location=device))\n        logging.info('\\tLoaded model from {}'.format(path))\n</code></pre>"},{"location":"bnode_core/ode/trainer/","title":"Trainer Module","text":""},{"location":"bnode_core/ode/trainer/#bnode_core.ode.trainer","title":"<code>bnode_core.ode.trainer</code>","text":"<p>Neural ODE and Balanced Neural ODE Training Module.</p> <p>This module provides the main training pipeline for Neural ODE (NODE) and Balanced Neural ODE (BNODE) models. It handles model initialization, multi-phase training, validation, testing, and MLflow experiment tracking.</p>"},{"location":"bnode_core/ode/trainer/#bnode_core.ode.trainer--architecture-support","title":"Architecture Support","text":"<p>The trainer automatically detects and supports two model architectures:</p> <ul> <li>Neural ODE (NODE): Direct neural differential equation models.</li> <li>Balanced Neural ODE (BNODE): Latent-space ODE models with encoder-decoder   architecture for improved training stability and representation learning.</li> </ul>"},{"location":"bnode_core/ode/trainer/#bnode_core.ode.trainer--training-pipeline-overview","title":"Training Pipeline Overview","text":"<p>The training process follows these stages:</p> <ol> <li> <p>Model Instantiation</p> <ul> <li>Automatically detects NODE vs BNODE from config</li> <li>Initializes normalization layers using dataset statistics</li> <li>Sets up device (CPU/CUDA) based on availability and config</li> </ul> </li> <li> <p>Pre-training (Optional, NODE only)</p> <ul> <li>Can be enabled in config: <code>nn_model.training.pre_train=true</code></li> <li>Trains on state derivatives (<code>state_der</code>) if present in dataset</li> <li>Uses collocation method for initial parameter estimation</li> <li>Not supported for BNODE models (No latent states gradients available,    but you can mock this behavior by using a short main training phase with    states_grad_loss)</li> </ul> </li> <li> <p>Multi-Phase Main Training</p> <ul> <li>Configured as a list in <code>nn_model.training.main_training</code></li> <li>Each phase can have different hyperparameters:<ul> <li>Solver type (euler, rk4, dopri5, etc.)</li> <li>Learning rate, batch size, sequence length</li> <li>Early stopping patience and threshold</li> </ul> </li> <li>See <code>resources/config/nn_model/bnode_pytest.yaml</code> for an example</li> </ul> </li> <li> <p>Final Testing</p> <ul> <li>Evaluates model on all dataset splits (train/val/test)</li> <li>Optionally saves predictions and internal variables to dataset</li> <li>Logs final metrics to MLflow</li> </ul> </li> </ol>"},{"location":"bnode_core/ode/trainer/#bnode_core.ode.trainer--key-training-features","title":"Key Training Features","text":"<p>Compatibility with NODE and BNODE</p> <ul> <li>Trainer auto-detects model type from config</li> <li>Both models provide a consistent training interface with    e.g. the <code>model_and_loss_evaluation</code> method.</li> </ul> <p>Adaptive Batch Processing</p> <p>Each epoch processes a specified number of batches (not entire dataset). Configured via <code>nn_model.training.main_training[i].batches_per_epoch</code>.</p> <p>NaN Recovery</p> <ul> <li>If NaN loss detected, automatically reloads last checkpoint</li> <li>Reduces gradient clipping norm to stabilize training</li> <li>Note: LR scheduling might be a better long-term solution</li> </ul> <p>Reparameterization Control (BNODE)</p> <ul> <li>Training uses active reparameterization (variational inference)</li> <li>When evaluating (validation/test, or at final test for all datasets),    reparameterization is disabled. Also for deterministic mode.</li> <li>Ensures consistent evaluation metrics</li> </ul> <p>Progressive Sequence Length Increase</p> <ul> <li>When switching phases, sequence length gradually increases</li> <li>Initial test with final sequence length to assess extrapolation</li> <li>Training sequence length increases gradually (controlled by     <code>seq_len_increase_in_batches</code>)</li> <li>Validation/test always use full sequence length to monitor extrapolation performance</li> <li>Early abort if stable extrapolation achieved:     <code>loss_train &lt; 2 * loss_validation</code> for N consecutive epochs     (<code>seq_len_increase_abort_after_n_stable_epochs</code>)</li> </ul> <p>MLflow Integration</p> <ul> <li>Logs metrics at end of each phase: <code>{metric}_{context}_job{phase}_final</code></li> <li>Final test metrics logged as: <code>{metric}_final</code></li> <li>All Hydra outputs and trained models saved as artifacts</li> <li>Experiment tracking with run name, parameters, and tags</li> </ul>"},{"location":"bnode_core/ode/trainer/#bnode_core.ode.trainer--typical-usage-examples","title":"Typical Usage Examples","text":"<p>As other modules of the <code>bnode_core</code> package, we use Hydra for configuration management.</p> <p>Basic training with default config:</p> <pre><code>uv run trainer nn_model=latent_ode_base dataset_name=myDataset\n</code></pre> <p>Training with custom model configuration:</p> <pre><code>uv run trainer nn_model=myCustomModel dataset_name=myDataset \\\n    mlflow_experiment_name=my_experiment \\\n    nn_model.network.lat_states_dim=1024 \\\n</code></pre> <p>Hyperparameter sweep (multi-run mode):</p> <pre><code>uv run trainer \\\n    nn_model=latent_ode_base \\\n    dataset_name=myDataset \\\n    nn_model.training.beta_start_override=0.1,0.01,0.001 \\\n    -m\n</code></pre> <p>Override specific training parameters:</p> <pre><code>uv run trainer \\\n    nn_model=latent_ode_base \\\n    dataset_name=myDataset \\\n    nn_model.training.lr_start_override=1e-4 \\\n    nn_model.training.batch_size_override=512 \\\n    use_cuda=false\n</code></pre> <p>View available configuration options (from Hydra):</p> <pre><code>uv run trainer --help\n</code></pre>"},{"location":"bnode_core/ode/trainer/#bnode_core.ode.trainer--configuration","title":"Configuration","text":"<p>For detailed configuration options, see:</p> <ul> <li>Config Documentation: Consult the Config section of the documentation</li> <li>Config Files: examples in <code>resources/config/nn_model/</code> directory</li> <li>Config Schema: <code>bnode_core.config</code> module for all available parameters</li> <li>Search Tip: Use Ctrl+F in config files to find specific parameter behavior</li> </ul>"},{"location":"bnode_core/ode/trainer/#bnode_core.ode.trainer--command-line-interface","title":"Command Line Interface","text":"<p>The trainer is registered as a UV script in <code>pyproject.toml</code>, enabling direct execution via <code>uv run trainer</code>. All Hydra config parameters can be overridden via command line using dot notation.</p>"},{"location":"bnode_core/ode/trainer/#bnode_core.ode.trainer--notes","title":"Notes","text":"<ul> <li>CUDA is automatically used if available (override with <code>use_cuda=false</code>)</li> <li>Model checkpoints saved after each phase: <code>model_phase_{i}.pt</code></li> <li>Failed artifact logging tracked in <code>could_not_log_artifacts.txt</code></li> <li>Supports mixed precision training (AMP) when enabled</li> <li>Early stopping based on validation loss with configurable patience</li> </ul>"},{"location":"bnode_core/ode/trainer/#bnode_core.ode.trainer--see-also","title":"See Also","text":"<p>bnode_core.config : Configuration schemas and validation bnode_core.ode.node.node_architecture : Neural ODE model implementation bnode_core.ode.bnode.bnode_architecture : Balanced Neural ODE model implementation bnode_core.nn.nn_utils.load_data : Dataset loading utilities</p>"},{"location":"bnode_core/ode/trainer/#bnode_core.ode.trainer.initialize_model","title":"<code>initialize_model(cfg: train_test_config_class, train_dataset: TimeSeriesDataset, hdf5_dataset: hdf5_dataset_class, initialize_normalization=True, model_type: str = None)</code>","text":"<p>Initialize and configure NODE or BNODE model with dataset statistics.</p> <p>Automatically detects model type from config and initializes normalization layers using training dataset statistics. Handles device placement (CPU/CUDA) and copies model architecture file to Hydra output directory.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>train_test_config_class</code> <p>Validated Hydra configuration.</p> required <code>train_dataset</code> <code>TimeSeriesDataset</code> <p>Training dataset for normalization.</p> required <code>hdf5_dataset</code> <code>Dataset</code> <p>HDF5 dataset handle for statistics.</p> required <code>initialize_normalization</code> <code>bool</code> <p>Whether to initialize normalization layers from dataset statistics. Defaults to True.</p> <code>True</code> <code>model_type</code> <code>str</code> <p>Force specific model type ('node' or 'bnode'). If None, auto-detects from config. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>model</code> <code>Module</code> <p>Initialized model (NeuralODE or BalancedNeuralODE) moved to appropriate device.</p> Side Effects <ul> <li>Modifies cfg.use_cuda based on availability</li> <li>Copies model architecture source file to Hydra output directory</li> <li>Logs device and parameter count information</li> </ul> Notes <ul> <li>CUDA is used if available and cfg.use_cuda=True</li> <li>Normalization uses training set statistics only</li> <li>Model type detection based on network class in config</li> </ul> Source code in <code>src/bnode_core/ode/trainer.py</code> <pre><code>def initialize_model(cfg: train_test_config_class, train_dataset: TimeSeriesDataset, hdf5_dataset: hdf5_dataset_class, \n                     initialize_normalization=True, model_type: str = None):\n    \"\"\"Initialize and configure NODE or BNODE model with dataset statistics.\n\n    Automatically detects model type from config and initializes normalization\n    layers using training dataset statistics. Handles device placement (CPU/CUDA)\n    and copies model architecture file to Hydra output directory.\n\n    Args:\n        cfg (train_test_config_class): Validated Hydra configuration.\n        train_dataset (TimeSeriesDataset): Training dataset for normalization.\n        hdf5_dataset (hdf5_dataset_class): HDF5 dataset handle for statistics.\n        initialize_normalization (bool, optional): Whether to initialize normalization\n            layers from dataset statistics. Defaults to True.\n        model_type (str, optional): Force specific model type ('node' or 'bnode').\n            If None, auto-detects from config. Defaults to None.\n\n    Returns:\n        model (torch.nn.Module): Initialized model (NeuralODE or BalancedNeuralODE) moved\n            to appropriate device.\n\n    Side Effects:\n        - Modifies cfg.use_cuda based on availability\n        - Copies model architecture source file to Hydra output directory\n        - Logs device and parameter count information\n\n    Notes:\n        - CUDA is used if available and cfg.use_cuda=True\n        - Normalization uses training set statistics only\n        - Model type detection based on network class in config\n    \"\"\"\n    _cuda_available = torch.cuda.is_available()\n    logging.info('CUDA available: {} | cfg.use_cuda: {}'.format(_cuda_available, cfg.use_cuda))\n    if _cuda_available and cfg.use_cuda:\n        cfg.use_cuda = True\n    else:\n        cfg.use_cuda = False\n    logging.info(\"---&gt; Training with cuda: {}\".format(cfg.use_cuda))\n    device = torch.device('cuda' if torch.cuda.is_available() and cfg.use_cuda else 'cpu')\n    # create model (insert specific creations here)\n    from bnode_core.config import neural_ode_network_class, latent_ode_network_class\n    if model_type == None:\n        if type(cfg.nn_model.network) is neural_ode_network_class:\n            model_type='node'\n        elif type(cfg.nn_model.network) is latent_ode_network_class:\n            model_type='bnode'\n        else: \n            raise ValueError('The neural network class could not be resolved')\n        assert model_type in ['node', 'bnode']\n    if model_type == 'node':\n        model = NeuralODE(states_dim=train_dataset[0]['states'].shape[0],\n                        controls_dim=train_dataset[0]['controls'].shape[0] if 'controls' in train_dataset[0].keys() else 0,\n                        parameters_dim=train_dataset[0]['parameters'].shape[0] if 'parameters' in train_dataset[0].keys() else 0,\n                        outputs_dim=train_dataset[0]['outputs'].shape[0] if 'outputs' in train_dataset[0].keys() else 0,\n                        controls_to_output_nn=cfg.nn_model.network.controls_to_output_nn,\n                        hidden_dim=cfg.nn_model.network.linear_hidden_dim, \n                        n_layers=cfg.nn_model.network.n_linear_layers,\n                        hidden_dim_output_nn=cfg.nn_model.network.hidden_dim_output_nn,\n                        n_layers_output_nn=cfg.nn_model.network.n_layers_output_nn,\n                        activation=eval(cfg.nn_model.network.activation),\n                        intialization=cfg.nn_model.training.pre_training.initialization_type,\n                        initialization_ode=cfg.nn_model.training.initialization_type_ode,)\n        # initialize normalizations\n        if initialize_normalization:\n            model.normalization_init(hdf5_dataset)\n    elif model_type == 'bnode':\n        model = BalancedNeuralODE(\n                        states_dim=train_dataset[0]['states'].shape[0],\n                        lat_states_mu_dim=cfg.nn_model.network.lat_states_dim,\n                        parameters_dim=train_dataset[0]['parameters'].shape[0] if 'parameters' in train_dataset[0].keys() else 0,\n                        lat_parameters_dim=cfg.nn_model.network.lat_parameters_dim,\n                        controls_dim=train_dataset[0]['controls'].shape[0] if 'controls' in train_dataset[0].keys() else 0,\n                        lat_controls_dim=cfg.nn_model.network.lat_controls_dim,\n                        outputs_dim=train_dataset[0]['outputs'].shape[0] if 'outputs' in train_dataset[0].keys() else 0,\n                        hidden_dim=cfg.nn_model.network.linear_hidden_dim,\n                        n_layers=cfg.nn_model.network.n_linear_layers,\n                        controls_to_decoder=cfg.nn_model.network.controls_to_decoder,\n                        predict_states=cfg.nn_model.network.predict_states,\n                        activation=eval(cfg.nn_model.network.activation),\n                        initialization_type=cfg.nn_model.training.initialization_type,\n                        initialization_type_ode=cfg.nn_model.training.initialization_type_ode,\n                        initialization_type_ode_matrix=cfg.nn_model.training.initialization_type_ode_matrix,\n                        lat_ode_type=cfg.nn_model.network.lat_ode_type,\n                        include_params_encoder= cfg.nn_model.network.include_params_encoder,\n                        params_to_state_encoder=cfg.nn_model.network.params_to_state_encoder,\n                        params_to_control_encoder=cfg.nn_model.network.params_to_control_encoder,\n                        params_to_decoder=cfg.nn_model.network.params_to_decoder,\n                        controls_to_state_encoder=cfg.nn_model.network.controls_to_state_encoder,\n                        state_encoder_linear = cfg.nn_model.network.state_encoder_linear,\n                        control_encoder_linear = cfg.nn_model.network.control_encoder_linear,\n                        parameter_encoder_linear = cfg.nn_model.network.parameter_encoder_linear,\n                        ode_linear = cfg.nn_model.network.ode_linear,\n                        decoder_linear = cfg.nn_model.network.decoder_linear,\n                        lat_state_mu_independent = cfg.nn_model.network.lat_state_mu_independent,\n                        )\n        # initialize normalizations\n        if initialize_normalization:\n            model.normalization_init(hdf5_dataset)\n    logging.info('Initialized model: {}'.format(model))\n    logging.info('Number of trainable parameters: {}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n    model.to(device)\n    logging.info('moved model to {}'.format(device))\n    return model\n</code></pre>"},{"location":"bnode_core/ode/trainer/#bnode_core.ode.trainer.train_all_phases","title":"<code>train_all_phases(cfg: train_test_config_class)</code>","text":"<p>Execute complete multi-phase training pipeline with MLflow tracking.</p> <p>Main orchestration function that coordinates:</p> <ul> <li>Dataset loading</li> <li>Model initialization  </li> <li>Optional pre-training (NODE only)</li> <li>Multi-phase main training</li> <li>Final testing and evaluation</li> <li>MLflow artifact logging</li> </ul> <p>The function processes a job list consisting of optional pre-training, multiple main training phases, and final testing. Each phase can have different hyperparameters and training strategies.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>train_test_config_class</code> <p>Validated Hydra configuration containing: - dataset_path, dataset_name: Dataset location and identifier - nn_model.training.pre_train: Enable pre-training (NODE only) - nn_model.training.main_training: List of training phase configs - nn_model.training.test: Enable final testing - use_cuda: Device preference - mlflow_experiment_name: MLflow experiment name</p> required Side Effects <ul> <li>Creates/updates model checkpoints: model_phase_{i}.pt</li> <li>Logs metrics, parameters, and artifacts to MLflow</li> <li>Saves predictions to dataset if configured</li> <li>Copies Hydra outputs to MLflow artifacts</li> <li>Creates could_not_log_artifacts.txt on logging failures</li> </ul> Training Flow <ol> <li>Load HDF5 dataset and log to MLflow</li> <li>Build job list (pre-train, main phases, test)</li> <li>For each job:</li> <li>Initialize/reload dataloaders if needed</li> <li>Initialize/load model if needed</li> <li>Execute training or testing</li> <li>Save checkpoint and log metrics</li> <li>Copy all outputs to MLflow artifacts</li> </ol> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If CUDA memory errors occur repeatedly</p> <code>FileNotFoundError</code> <p>If dataset or checkpoint files missing</p> Notes <ul> <li>Decorated with @log_hydra_to_mlflow for automatic config logging</li> <li>Memory errors trigger dataloader recreation with adjusted settings</li> <li>NaN losses trigger checkpoint reload and gradient clipping adjustment</li> <li>Progressive sequence length increase during phase transitions</li> </ul> See Also <p>train_one_phase : Single training phase execution initialize_model : Model instantiation and initialization</p> Source code in <code>src/bnode_core/ode/trainer.py</code> <pre><code>@log_hydra_to_mlflow\ndef train_all_phases(cfg: train_test_config_class):\n    \"\"\"Execute complete multi-phase training pipeline with MLflow tracking.\n\n    Main orchestration function that coordinates:\n\n    - Dataset loading\n    - Model initialization  \n    - Optional pre-training (NODE only)\n    - Multi-phase main training\n    - Final testing and evaluation\n    - MLflow artifact logging\n\n    The function processes a job list consisting of optional pre-training,\n    multiple main training phases, and final testing. Each phase can have\n    different hyperparameters and training strategies.\n\n    Args:\n        cfg (train_test_config_class): Validated Hydra configuration containing:\n            - dataset_path, dataset_name: Dataset location and identifier\n            - nn_model.training.pre_train: Enable pre-training (NODE only)\n            - nn_model.training.main_training: List of training phase configs\n            - nn_model.training.test: Enable final testing\n            - use_cuda: Device preference\n            - mlflow_experiment_name: MLflow experiment name\n\n    Side Effects:\n        - Creates/updates model checkpoints: model_phase_{i}.pt\n        - Logs metrics, parameters, and artifacts to MLflow\n        - Saves predictions to dataset if configured\n        - Copies Hydra outputs to MLflow artifacts\n        - Creates could_not_log_artifacts.txt on logging failures\n\n    Training Flow:\n        1. Load HDF5 dataset and log to MLflow\n        2. Build job list (pre-train, main phases, test)\n        3. For each job:\n           - Initialize/reload dataloaders if needed\n           - Initialize/load model if needed\n           - Execute training or testing\n           - Save checkpoint and log metrics\n        4. Copy all outputs to MLflow artifacts\n\n    Raises:\n        RuntimeError: If CUDA memory errors occur repeatedly\n        FileNotFoundError: If dataset or checkpoint files missing\n\n    Notes:\n        - Decorated with @log_hydra_to_mlflow for automatic config logging\n        - Memory errors trigger dataloader recreation with adjusted settings\n        - NaN losses trigger checkpoint reload and gradient clipping adjustment\n        - Progressive sequence length increase during phase transitions\n\n    See Also:\n        train_one_phase : Single training phase execution\n        initialize_model : Model instantiation and initialization\n    \"\"\"\n    logging.info('Start training all phases....')\n    device = torch.device('cuda' if torch.cuda.is_available() and cfg.use_cuda else 'cpu')\n    logging.info('Using device: {}'.format(device))\n\n    # load hdf5 dataset\n    hdf5_dataset, _ = load_dataset_and_config(cfg.dataset_name, cfg.dataset_path)\n    mlflow.log_param('dataset_name', cfg.dataset_name)\n\n    # collect jobs\n    # job_list=[] filled with dict of style: {'skip': bool, 'test': bool, 'train_cfg': cfg, 'pre_train': bool}\n    job_list = []\n    # pre-training\n    job_list.append({'skip': not cfg.nn_model.training.pre_train or cfg.nn_model.training.load_pretrained_model or cfg.nn_model.training.load_trained_model_for_test,\n                     'test': False, 'train_cfg': cfg.nn_model.training.pre_training, 'pre_train': True})\n    # main training\n    for idx, main_train_cfg in enumerate(cfg.nn_model.training.main_training):\n        job_list.append({'skip': cfg.nn_model.training.load_trained_model_for_test, 'test': False, 'train_cfg': main_train_cfg, 'pre_train': False})\n    # test\n    if cfg.nn_model.training.test is True:\n        job_list.append({'skip': False, 'test': True, 'train_cfg': cfg.nn_model.training.main_training[-1], 'pre_train': False})\n    logging.info('Created job list: {}'.format(job_list))\n\n    # flags\n    _created_datasets_and_loaders=False\n    _loaded_seq_len=-1\n    _loaded_batch_size=-1\n    _created_model=False\n    _epoch_0 = 0\n    _reload_dataloaders_required = False\n    for idx, job in enumerate(job_list):\n        while True: # loop to catch memory errors\n            try:\n                if job['skip'] is False: # create dataloaders for this job\n                    if job['pre_train'] is True:\n                        logging.info('Starting Pre-Training with settings {}'.format(job['train_cfg']))\n                    elif job['test'] is True:\n                        logging.info('Starting Testing with settings {}'.format(job['train_cfg']))\n                    else:\n                        logging.info('Starting Train Job {} with settings {}'.format(idx, job['train_cfg']))\n                    # loading datasets and initializing dataloaders\n                    # set seq_len\n                    if job['pre_train'] is True:\n                        _load_seq_len = job['train_cfg'].load_seq_len\n                        _seq_len_batches = 1\n                    elif job['test'] is True:\n                        _load_seq_len = None\n                        _seq_len_batches = None\n                    else:\n                        _load_seq_len = job['train_cfg'].load_seq_len\n                        _seq_len_batches = job['train_cfg'].seq_len_train\n                    if _created_datasets_and_loaders is False or _load_seq_len != _loaded_seq_len: \n                        if _created_datasets_and_loaders is True:\n                            _keys = list(datasets.keys())\n                            for key in _keys:\n                                del datasets[key]\n                        # make torch tensor datasets\n                        datasets = {}\n                        for context in ['train', 'test', 'validation', 'common_test']:\n                            datasets[context] = make_stacked_dataset(hdf5_dataset, context, _load_seq_len, _seq_len_batches)\n                        _loaded_seq_len = _load_seq_len\n                        _reload_dataloaders_required = True\n                    else:\n                        for context in ['train', 'test', 'validation', 'common_test']:\n                            datasets[context].set_seq_len(_seq_len_batches)\n                        _reload_dataloaders_required = True # TODO; check if this is necessary\n                    _batch_size = job['train_cfg'].batch_size if job['test'] is False else cfg.nn_model.training.batch_size_test\n                    _drop_last = True if job['test'] is False else False\n                    _shuffle = True if job['test'] is False else False\n                    if _created_datasets_and_loaders is False or _loaded_batch_size != _batch_size or _reload_dataloaders_required is True or job['test'] is True:\n                        # initialiaze batch_loader, as batch size can't be set to a new value\n                        if _created_datasets_and_loaders is True:\n                            #del dataloaders\n                            _keys = list(dataloaders.keys())\n                            for key in _keys:\n                                del dataloaders[key]\n                        # create new\n                        dataloaders={}\n                        for context in ['train', 'test', 'validation', 'common_test']:\n                            if job['test'] is True and len(datasets[context]) == 0: # when only testing, datasets can be empty\n                                dataloaders[context] = None\n                                logging.info('Only Testing: No data for context {} in dataset. Skipping loading dataloader for this context'.format(context))\n                            else:\n                                _num_workers = cfg.n_workers_train_loader if context == 'train' else cfg.n_workers_other_loaders\n                                if context == 'train' and job['pre_train'] is True:\n                                    _num_workers = 1 * _num_workers\n                                if _batch_size &gt; len(datasets[context]):\n                                    _batch_size_here = int(len(datasets[context])/2)+3\n                                    logging.warning('Batch size {} is larger than dataset size {} for context {}. Setting batch size to {}'.format(_batch_size, len(datasets[context]), context, _batch_size_here))\n                                else:\n                                    _batch_size_here = _batch_size\n                                if len(datasets[context]) == 0:\n                                    raise ValueError('While creating dataloaders, dataset for context {} is empty. Aborting.'.format(context))\n                                dataloaders[context] = torch.utils.data.DataLoader(datasets[context], batch_size=_batch_size_here, shuffle=_shuffle,\n                                                                                    num_workers = _num_workers, persistent_workers=True, \n                                                                                    pin_memory=True, drop_last=_drop_last, prefetch_factor=cfg.prefetch_factor)\n                        _created_datasets_and_loaders = True\n                        _loaded_batch_size = _batch_size\n                        # update seq_len train for this job to the actual seq_len of the dataset\n                        if 'seq_len' in datasets['train'].__dict__.keys(): # for custom dataset (wiht map)\n                            job['train_cfg'].seq_len_train = datasets['train'].seq_len\n                        else:\n                            job['train_cfg'].seq_len_train = datasets['train'].datasets['time'].shape[2]\n\n\n                    _created_model_this_job = False\t\n                    # initialize model\n                    if _created_model is False:\n                        model = initialize_model(cfg, datasets['train'], hdf5_dataset)\n                        _created_model, _created_model_this_job = True, True\n                    if cfg.nn_model.training.load_pretrained_model is True and _created_model_this_job is True:\n                        _path = filepaths.filepath_from_local_or_ml_artifacts(cfg.nn_model.training.path_pretrained_model)\n                        model.load(path=_path, device=device)\n                        logging.info('Loaded pretrained model from {}'.format(_path))\n                        if cfg.nn_model.training.pre_trained_model_seq_len is not None: \n                            job_list[idx]['train_cfg'].seq_len_epoch_start = cfg.nn_model.training.pre_trained_model_seq_len\n                            logging.info('Set seq_len_epoch_start for next job to {}'.format(cfg.nn_model.training.pre_trained_model_seq_len))\n                        else:\n                            job_list[idx]['train_cfg'].seq_len_epoch_start = job['train_cfg'].seq_len_train\n                            logging.info('Set seq_len_epoch_start for this job to seq_len_train {} as no pre_trained_model_seq_len is given in config'.format(job['train_cfg'].seq_len_train))\n                    if cfg.nn_model.training.load_trained_model_for_test is True:\n                        _path = cfg.nn_model.training.path_trained_model\n                        _path = filepaths.filepath_from_local_or_ml_artifacts(_path)\n                        model.load(path=_path, device=device)\n                        logging.info('Loaded trained model from {}'.format(_path))\n\n                if job['skip'] is True:\n                    if job['pre_train'] is True:\n                        logging.info('Skipping Pre-Training')\n                    else:\n                        logging.info('Skipping Train Job {} as trained model is loaded in following phases'.format(idx))\n                else:\n                    if job['test'] is False:\n                        # train one phase\n                        _epoch_0 = train_one_phase(cfg, model, dataloaders, job['train_cfg'], job['test'], job['pre_train'], idx, _epoch_0)\n                        # set seq_len_epoch_start for next job\n                        if len(job_list) &gt; idx+1:\n                            # consequently, seq_len_epoch_start should be seq_len_train\n                            job_list[idx+1]['train_cfg'].seq_len_epoch_start = job['train_cfg'].seq_len_train if job['pre_train'] is False else 1\n                            logging.info('Set seq_len_epoch_start for next job to {}, the seq_len_train of this job'.format(job_list[idx+1]['train_cfg'].seq_len_epoch_start))\n                    else:\n                        logging.info('Testing model')\n                        hdf5_dataset.close()\n                        # copy dataset to hydra output directory\n                        _save_predictions = cfg.nn_model.training.save_predictions_in_dataset\n                        if _save_predictions is True:\n                            _path = filepaths.filepath_dataset_current_hydra_output()\n                            shutil.copy(filepaths.filepath_dataset_from_config(cfg.dataset_name, cfg.dataset_path), _path)\n                            logging.info('Adding predictions to dataset')\n                            logging.info('copied dataset to file: {}'.format(_path))\n                            hdf5_dataset = h5py.File(_path, 'r+')\n                        else:\n                            logging.info('Not saving predictions in dataset')\n                        for context in ['train', 'test', 'validation', 'common_test']:\n                            if dataloaders[context] is None:\n                                logging.info('No data for context {} in dataset. Skipping.'.format(context))\n                            else:\n                                logging.info('Testing of dataset for context {}'.format(context))\n                                if _save_predictions is True:\n                                    # Stream results batch-by-batch to HDF5 to reduce RAM usage\n                                    total_len = len(dataloaders[context].dataset)\n                                    data_iter = iter(dataloaders[context])\n                                    created_dsets = False\n                                    write_offset = 0\n                                    metrics_sum = {}\n                                    n_batches = 0\n                                    keys_to_save = []\n                                    while True:\n                                        try:\n                                            data_batch = next(data_iter)\n                                        except StopIteration:\n                                            break\n                                        with torch.no_grad():\n                                            logging.info(f\"\\t Batch {n_batches+1}/{int(total_len/cfg.nn_model.training.batch_size_test)+1}\")\n                                            ret_vals_batch, model_outputs_batch = model.model_and_loss_evaluation(\n                                                data_batch, job['train_cfg'], job['pre_train'], device,\n                                                return_model_outputs=True, test=True\n                                            )\n                                        # Initialize datasets on first batch according to save policy\n                                        if not created_dsets:\n                                            # Decide which keys to save\n                                            for key in model_outputs_batch.keys():\n                                                if cfg.nn_model.training.test_save_internal_variables is True:\n                                                    _save = True\n                                                else:\n                                                    if key in ['states_hat', 'states_der_hat', 'outputs_hat']:\n                                                        _save = True\n                                                    elif cfg.nn_model.training.test_save_internal_variables_for == context:\n                                                        _save = True\n                                                        logging.info('Saving internal variable {} as test_save_internal_variables_for context is {}'.format(key, context))\n                                                    else:\n                                                        _save = False\n                                                        logging.info('Not saving internal variable {} as test_save_no_internal_variables is True'.format(key))\n                                                if _save:\n                                                    keys_to_save.append(key)\n                                            # Create HDF5 datasets per key with full size on first dimension\n                                            for key in keys_to_save:\n                                                arr = model_outputs_batch[key]\n                                                shape_rest = arr.shape[1:]\n                                                dset_shape = (total_len,) + shape_rest\n                                                hdf5_dataset.create_dataset(context + '/' + key, shape=dset_shape, dtype=arr.dtype)\n                                            created_dsets = True\n                                        # Write this batch to HDF5\n                                        Batch = next(iter(model_outputs_batch.values())).shape[0] if len(model_outputs_batch) &gt; 0 else 0\n                                        for key in keys_to_save:\n                                            arr = model_outputs_batch[key]\n                                            hdf5_dataset[context + '/' + key][write_offset:write_offset + arr.shape[0], ...] = arr\n                                        write_offset += Batch\n                                        # Accumulate metrics for averaging later (match old np.mean over batches)\n                                        if n_batches == 0:\n                                            metrics_sum = {k: float(v) for k, v in ret_vals_batch.items()}\n                                        else:\n                                            for k, v in ret_vals_batch.items():\n                                                metrics_sum[k] += float(v)\n                                        n_batches += 1\n                                    # Compute mean metrics across batches\n                                    ret_vals = {k: (metrics_sum[k] / max(n_batches, 1)) for k in metrics_sum.keys()}\n                                else:\n                                    ret_vals = test_or_validate_one_epoch(model, dataloaders[context], job['train_cfg'], job['pre_train'], device, all_batches=True, return_model_outputs=False)\n                                # log stats with logging\n                                logging.info('Stats for context {}: {}'.format(context, ret_vals))\n                                # log stats with mlflow\n                                mlflow.log_metrics(append_context_to_dict_keys(ret_vals, context), step=_epoch_0+1) \n                                mlflow.log_metrics(append_context_to_dict_keys(ret_vals, '{}_final'.format(context)), step=_epoch_0+1)\n                                # save loss function values\n                                if _save_predictions is True:\n                                    for key, value in ret_vals.items():\n                                        hdf5_dataset.create_dataset(context+'/'+key, data=value)\n                        if _save_predictions is True:\n                            hdf5_dataset.close()\n                            # save this file\n                            shutil.copy(Path(__file__), filepaths.dir_current_hydra_output())\n                            logging.info('copied current trainer.py: {} \\nto: \\n{}'.format(Path(__file__), filepaths.dir_current_hydra_output()))\n                if cfg.use_cuda:\n                    torch.cuda.empty_cache() \n                break # break the exception loop\n            except RuntimeError as e:\n                if 'CUDA out of memory' in str(e) or 'CUDA memory is almost full' in str(e):\n                    logging.warning('CUDA out of memory error. Trying again in 10 seconds')\n                    pyTime.sleep(10)\n                    logging.info('Setting batch size to {}'.format(int(_batch_size * 0.7)))\n                    if not job['test']:\n                        job['train_cfg'].batch_size = int(_batch_size * 0.7)\n                    else:\n                        cfg.nn_model.training.batch_size_test = int(_batch_size * 0.7)\n                    if cfg.use_cuda:\n                        torch.cuda.empty_cache()\n                else:\n                    raise e\n</code></pre>"},{"location":"bnode_core/ode/trainer/#bnode_core.ode.trainer.main","title":"<code>main()</code>","text":"<p>Entry point for (B)NODE training via Hydra CLI.</p> <p>Initializes Hydra configuration system and launches train_all_phases with validated config. Auto-detects config directory and uses 'train_test_ode' as the default config name.</p> <p>This function is registered as 'trainer' in pyproject.toml, enabling command-line execution via::</p> <pre><code>uv run trainer [config_overrides]\n</code></pre> <p>Examples:</p> <p>See module docstring for usage examples.</p> Side Effects <ul> <li>Registers config store with Hydra</li> <li>Auto-detects config directory from filepaths</li> <li>Launches Hydra-decorated train_all_phases</li> </ul> Source code in <code>src/bnode_core/ode/trainer.py</code> <pre><code>def main():\n    \"\"\"Entry point for (B)NODE training via Hydra CLI.\n\n    Initializes Hydra configuration system and launches train_all_phases with\n    validated config. Auto-detects config directory and uses 'train_test_ode'\n    as the default config name.\n\n    This function is registered as 'trainer' in pyproject.toml, enabling\n    command-line execution via::\n\n        uv run trainer [config_overrides]\n\n    Examples:\n        See module docstring for usage examples.\n\n    Side Effects:\n        - Registers config store with Hydra\n        - Auto-detects config directory from filepaths\n        - Launches Hydra-decorated train_all_phases\n    \"\"\"\n    cs = get_config_store()\n    config_dir = filepaths.config_dir_auto_recognize()\n    config_name = 'train_test_ode'\n    hydra.main(config_path=str(config_dir.absolute()), config_name=config_name, version_base=None)(train_all_phases)()\n</code></pre>"},{"location":"bnode_core/ode/bnode/bnode/","title":"BNODE Module","text":""},{"location":"bnode_core/ode/bnode/bnode/#bnode_core.ode.bnode.bnode_architecture","title":"<code>bnode_core.ode.bnode.bnode_architecture</code>","text":""},{"location":"bnode_core/ode/bnode/bnode/#bnode_core.ode.bnode.bnode_modules","title":"<code>bnode_core.ode.bnode.bnode_modules</code>","text":""},{"location":"bnode_core/ode/bnode/bnode/#bnode_core.ode.bnode.bnode_modules.GeneralEncoder","title":"<code>GeneralEncoder</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/bnode_core/ode/bnode/bnode_modules.py</code> <pre><code>class GeneralEncoder(nn.Module):\n\n    def __init__(self, \n                 input_dim: int,\n                 lat_dim: int,\n                 hidden_dim: int,\n                 n_layers: int,\n                 activation: nn.Module = nn.ELU,\n                 initialization: str = 'identity',\n                 include_parameters: bool = False,\n                 include_controls: bool = False,\n                 param_dim: int = 0,\n                 controls_dim: int = 0,\n                 linear: bool = False,\n                 ):\n        # include_parameters and param_dim was added to incorporate a state and controls encoder that is dependent on parameters\n        super().__init__()\n        if param_dim &gt; 0 and include_parameters is False:\n            raise ValueError('param_dim &gt; 0 but include_parameters is False')\n        self.input_dim = input_dim\n        if include_parameters:\n            self.input_dim += param_dim\n        if include_controls:\n            self.input_dim += controls_dim\n        self.lat_dim = lat_dim\n\n        self.include_parameters = include_parameters\n        self.include_controls = include_controls\n        # define layers\n        self.normalization = NormalizationLayer1D(input_dim)\n        if include_parameters:\n            self.normalization_params = NormalizationLayer1D(param_dim)\n        if self.include_controls:\n            self.normalization_controls = NormalizationLayer1D(controls_dim)\n\n\n        if not linear:\n            modules = [\n                nn.Linear(self.input_dim, hidden_dim),\n                activation(),\n            ]\n            if n_layers &lt; 2:\n                logging.warning('n_layers must be at least 2, setting n_layers to 2')\n            for i in range(n_layers-2):\n                modules.append(nn.Linear(hidden_dim, hidden_dim))\n                modules.append(activation())\n            modules.append(nn.Linear(hidden_dim, 2 * self.lat_dim))\n            self.net = nn.Sequential(*modules)\n        else:\n            self.net= nn.Sequential(nn.Linear(self.input_dim, 2 * self.lat_dim, bias=True))\n\n        initialize_weights_biases(self.net, initialization)\n\n        # initialize mask\n        self.register_buffer(\"mask_set\", torch.tensor(False))\n        self.register_buffer(\"mask\", torch.ones(self.lat_dim))\n\n    def set_mask(self, mask):\n        self.mask_set.set_(torch.tensor(True, device=mask.device))\n        self.mask.set_(torch.tensor(mask.clone().detach(), device=mask.device, dtype=torch.float32))\n\n    def forward(self, x, params = None, controls = None):\n        \"\"\"\n        x: [batch_size, input_dim]\n        \"\"\"\n        x = self.normalization(x)\n        if self.include_parameters:\n            params = self.normalization_params(params)\n            x = torch.cat([x, params], dim=1)\n        if self.include_controls:\n            controls = self.normalization_controls(controls)\n            x = torch.cat([x, controls], dim=1)\n        mu, logvar = torch.split(self.net(x), self.lat_dim, dim=1)\n\n        if self.mask_set:\n            mu = mu * self.mask\n            logvar = torch.zeros(logvar.size(), device=logvar.device) # set logvar to zero if mask is set\n\n        return mu, logvar\n</code></pre>"},{"location":"bnode_core/ode/bnode/bnode/#bnode_core.ode.bnode.bnode_modules.GeneralEncoder.forward","title":"<code>forward(x, params=None, controls=None)</code>","text":"<p>x: [batch_size, input_dim]</p> Source code in <code>src/bnode_core/ode/bnode/bnode_modules.py</code> <pre><code>def forward(self, x, params = None, controls = None):\n    \"\"\"\n    x: [batch_size, input_dim]\n    \"\"\"\n    x = self.normalization(x)\n    if self.include_parameters:\n        params = self.normalization_params(params)\n        x = torch.cat([x, params], dim=1)\n    if self.include_controls:\n        controls = self.normalization_controls(controls)\n        x = torch.cat([x, controls], dim=1)\n    mu, logvar = torch.split(self.net(x), self.lat_dim, dim=1)\n\n    if self.mask_set:\n        mu = mu * self.mask\n        logvar = torch.zeros(logvar.size(), device=logvar.device) # set logvar to zero if mask is set\n\n    return mu, logvar\n</code></pre>"},{"location":"bnode_core/ode/bnode/bnode/#bnode_core.ode.bnode.bnode_modules.ssm_from_param","title":"<code>ssm_from_param</code>","text":"<p>               Bases: <code>Module</code></p> <p>This is a module to compute the system matrix A and control matrix B from parameters. It is used for BNODE with a linear latent ODE function. It is a simple linear layer that takes the parameters as input and outputs the system matrix A and control matrix B. It is a separate module to allow easy onnx export and allow one calculation of the matrices A and B for a whole trajectory.</p> Source code in <code>src/bnode_core/ode/bnode/bnode_modules.py</code> <pre><code>class ssm_from_param(nn.Module):\n    '''\n    This is a module to compute the system matrix A and control matrix B from parameters.\n    It is used for BNODE with a linear latent ODE function.\n    It is a simple linear layer that takes the parameters as input and outputs the system matrix A and control matrix B.\n    It is a separate module to allow easy onnx export and allow one calculation of the matrices A and B for a whole trajectory.\n    '''\n    def __init__(self, lat_parameter_dim: int, lat_state_dim: int, include_controls: bool = False, lat_control_dim: int = 0, initialization_ode: str = None):\n        super().__init__()\n        self.lat_state_dim = lat_state_dim\n        self.lat_parameter_dim = lat_parameter_dim\n        self.include_controls = include_controls\n        self.lat_control_dim = lat_control_dim\n\n        self.A_from_param = nn.Linear(self.lat_parameter_dim, self.lat_state_dim * self.lat_state_dim, bias=True)\n        if self.include_controls:\n            self.B_from_param = nn.Linear(self.lat_parameter_dim, self.lat_state_dim * self.lat_control_dim, bias=True)\n        # for the initialization of the weights and biases\n        _in_n_args = [self.lat_state_dim, self.lat_parameter_dim]\n        if self.include_controls:\n            _in_n_args.append(self.lat_control_dim)\n        _out_features = self.lat_state_dim\n        initialize_weights_biases(self, initialization_ode, in_n_args=_in_n_args, out_features=_out_features)\n\n    def forward(self, lat_parameters):\n        \"\"\"\n        lat_parameters: [batch_size, lat_parameter_dim]\n        \"\"\"\n        A = self.A_from_param(lat_parameters)\n        A = A.view(-1, self.lat_state_dim, self.lat_state_dim)\n\n        if self.include_controls:\n            B = self.B_from_param(lat_parameters)\n            B = B.view(-1, self.lat_state_dim, self.lat_control_dim)\n\n        return A, B if self.include_controls else A\n</code></pre>"},{"location":"bnode_core/ode/bnode/bnode/#bnode_core.ode.bnode.bnode_modules.ssm_from_param.forward","title":"<code>forward(lat_parameters)</code>","text":"<p>lat_parameters: [batch_size, lat_parameter_dim]</p> Source code in <code>src/bnode_core/ode/bnode/bnode_modules.py</code> <pre><code>def forward(self, lat_parameters):\n    \"\"\"\n    lat_parameters: [batch_size, lat_parameter_dim]\n    \"\"\"\n    A = self.A_from_param(lat_parameters)\n    A = A.view(-1, self.lat_state_dim, self.lat_state_dim)\n\n    if self.include_controls:\n        B = self.B_from_param(lat_parameters)\n        B = B.view(-1, self.lat_state_dim, self.lat_control_dim)\n\n    return A, B if self.include_controls else A\n</code></pre>"},{"location":"bnode_core/ode/bnode/bnode/#bnode_core.ode.bnode.bnode_modules.LatentODEFunc","title":"<code>LatentODEFunc</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/bnode_core/ode/bnode/bnode_modules.py</code> <pre><code>class LatentODEFunc(nn.Module):\n\n    def __init__(self,\n                 lat_state_mu_dim: int,\n                 lat_control_dim: int,\n                 lat_parameter_dim: int,\n                 hidden_dim: int,\n                 n_layers: int,\n                 activation: nn.Module = nn.ELU,\n                 initialization_ode: str = 'identity',\n                 initialization_system_matrix: str = None,\n                 lat_ode_type: str = 'variance_constant', # must be one of ['variance_constant', 'variance_dynamic', 'vanilla']\n                 linear: bool = False,\n                 lat_state_mu_independent: bool = False,\n                 ):\n        if initialization_system_matrix is not None and linear is False:\n            logging.warning('initialization_system_matrix is only used for linear models, setting it to None')\n            initialization_system_matrix = None\n        if initialization_system_matrix is not None and linear is True and lat_parameter_dim &gt; 0:\n            logging.warning('initialization_system_matrix is only used for linear models without parameters, setting it to None')\n            logging.warning('if you want to do eigenvalue shifting with parameters, you can use the move_eigvals_net method')\n            initialization_system_matrix = None\n\n        super().__init__()\n        self.lat_state_dim = lat_state_mu_dim if not lat_ode_type == 'variance_dynamic' else 2 * lat_state_mu_dim\n        self.lat_state_mu_dim = lat_state_mu_dim\n        self.lat_control_dim = lat_control_dim\n        self.lat_parameter_dim = lat_parameter_dim\n\n        self.include_controls = True if lat_control_dim &gt; 0 else False\n        self.include_parameters = True if lat_parameter_dim &gt; 0 else False\n\n        self.lat_ode_type = lat_ode_type\n        self.lat_state_mu_independent = lat_state_mu_independent\n        if self.lat_ode_type != 'variance_dynamic' and self.lat_state_mu_independent:\n            logging.warning('lat_state_mu_independent is only used for lat_ode_type variance_dynamic, setting it to False')\n            self.lat_state_mu_independent, lat_state_mu_independent = False, False\n        self.linear = linear\n\n        # initialize mask\n        self.register_buffer(\"mask_set\", torch.tensor(False))\n        self.register_buffer(\"mask\", torch.ones(self.lat_state_dim))\n\n        # define layers\n        if not linear:\n            if lat_ode_type == 'variance_dynamic':\n                if self.lat_state_mu_independent is False:\n                    modules = self.construct_net(self.lat_state_dim, self.lat_state_dim, hidden_dim, n_layers, activation)\n                    self.net = nn.Sequential(*modules)\n                else:\n                    modules = self.construct_net(lat_state_mu_dim, lat_state_mu_dim, hidden_dim, n_layers, activation) # (f(\\mu(x), u, p))\n                    self.net = nn.Sequential(*modules)\n                    modules = self.construct_net(self.lat_state_dim, lat_state_mu_dim, hidden_dim, n_layers, activation) # (f(\\mu(x), \\sigma(x), u, p))\n                    self.net_sigma = nn.Sequential(*modules)\n            else:\n                modules = self.construct_net(self.lat_state_dim, self.lat_state_dim, hidden_dim, n_layers, activation)\n                self.net = nn.Sequential(*modules)\n            initialize_weights_biases(self.net, initialization_ode)\n            if self.lat_state_mu_independent:\n                initialize_weights_biases(self.net_sigma, initialization_ode)\n        else: # linear case\n            if self.lat_state_mu_independent is True: # TODO: test this. This must be set here, before calling initialize_weights_biases if we use a train_to_negative_eigenvalues method\n                # create a mask of the form for the system matrix A:\n                # [[1,0]   * [\\mu(x),\n                #  [1,1]]    \\sigma(x)]\n                self.register_buffer(\"mask_A_mu_independent\", torch.zeros(self.lat_state_dim, self.lat_state_dim))\n                mask_A_mu_independent = torch.zeros(self.lat_state_dim, self.lat_state_dim)\n                mask_A_mu_independent[:self.lat_state_mu_dim, :self.lat_state_mu_dim] = 1\n                mask_A_mu_independent[self.lat_state_mu_dim:, :] = 1\n                self.mask_A_mu_independent.set_(mask_A_mu_independent)\n                assert self.mask_A_mu_independent.requires_grad == False\n            # construct a linear state space model: xdot = Ax + Bu\n            if not self.include_parameters:\n                # if no parameters are included, we can use a simple linear model\n                self.A = nn.Linear(self.lat_state_dim, self.lat_state_dim, bias=False)\n                initialize_weights_biases(self.A, initialization_system_matrix)\n                if self.include_controls:\n                    self.B = nn.Linear(self.lat_control_dim, self.lat_state_dim, bias=False)\n                    if initialization_ode == 'move_eigvals_net':\n                        logging.warning('move_eigvals_net is an unsuitable initialization method for the control matrix')\n                    initialize_weights_biases(self.B, initialization_ode)\n            else:           \n                # if parameters are included, we need to calculate the matrices A and B depending on the parameters\n                self.ssm_from_param = ssm_from_param(lat_parameter_dim, self.lat_state_dim, self.include_controls, self.lat_control_dim, initialization_ode)\n\n    def set_mask(self, mask):\n        '''\n        Set the mask for the latent state, used for deterministic training.\n        mask: [lat_state_dim] or [lat_state_mu_dim] if lat_ode_type == 'variance_dynamic': mask for the latent state (boolean tensor or tensor with values 0 or 1)\n        '''\n        self.mask_set.set_(torch.tensor(True, device=mask.device))\n        if self.lat_ode_type == 'variance_dynamic':\n            _mask = torch.zeros(self.lat_state_dim, device=mask.device)\n            _mask[:self.lat_state_mu_dim] = mask # set mask for mu\n            # sigma mask is set to 1, as we do not want to include dynamics for sigma\n            self.mask.set_(torch.tensor(_mask.clone().detach(), device=mask.device, dtype=torch.float32))\n        else:\n            self.mask.set_(torch.tensor(mask.clone().detach(), device=mask.device, dtype=torch.float32))\n\n    def construct_net(self, state_in_dim, state_out_dim, hidden_dim, n_layers, activation):\n        modules = [\n                nn.Linear(state_in_dim + self.lat_control_dim + self.lat_parameter_dim, hidden_dim),\n                activation(),\n            ]\n        if n_layers &lt; 2:\n            logging.warning('n_layers must be at least 2, setting n_layers to 2')#\n        for i in range(n_layers-2):\n            modules.append(nn.Linear(hidden_dim, hidden_dim))\n            modules.append(activation())\n        modules.append(nn.Linear(hidden_dim, state_out_dim))\n        return modules\n\n    def forward(self, lat_states, lat_parameters = None, lat_controls=None, \n                A_from_param: torch.Tensor = None, B_from_param: torch.Tensor = None\n                ):\n        \"\"\"\n        lat_state: [batch_size, lat_state_dim]\n        lat_parameters: [batch_size, lat_parameter_dim]\n        lat_controls: [batch_size, lat_control_dim]\n        \"\"\"\n        if not self.linear:\n            if not self.lat_ode_type == 'variance_dynamic' or (self.lat_ode_type == 'variance_dynamic' and self.lat_state_mu_independent is False):\n                if self.include_controls and self.include_parameters:\n                    x = torch.cat([lat_states, lat_parameters, lat_controls], dim=1)\n                elif self.include_controls:\n                    x = torch.cat([lat_states, lat_controls], dim=1)\n                elif self.include_parameters:\n                    x = torch.cat([lat_states, lat_parameters], dim=1)\n                else:\n                    x = lat_states\n                lat_states_dot = self.net(x)\n            else: # independent mu from sigma\n                _states_mu = lat_states[:, :self.lat_state_mu_dim]\n                if self.include_controls and self.include_parameters:\n                    x_mu = torch.cat([_states_mu, lat_parameters, lat_controls], dim=1)\n                    x_sigma = torch.cat([lat_states, lat_parameters, lat_controls], dim=1)\n                elif self.include_controls:\n                    x_mu = torch.cat([_states_mu, lat_controls], dim=1)\n                    x_sigma = torch.cat([lat_states, lat_controls], dim=1)\n                elif self.include_parameters:\n                    x_mu = torch.cat([_states_mu, lat_parameters], dim=1)\n                    x_sigma = torch.cat([lat_states, lat_parameters], dim=1)\n                else:\n                    x_mu = _states_mu\n                    x_sigma = lat_states\n                lat_state_mu_dot = self.net(x_mu)\n                lat_state_sigma_dot = self.net_sigma(x_sigma)\n                lat_states_dot = torch.cat([lat_state_mu_dot, lat_state_sigma_dot], dim=1)\n        else: # linear case\n            if self.include_parameters:\n                if self.lat_state_mu_independent: # TODO: test this\n                    A_from_param = A_from_param * self.mask_A_mu_independent # element-wise multiplication with mask\n                lat_states_dot = torch.matmul(A_from_param, lat_states.unsqueeze(-1)).squeeze(-1)\n                if self.include_controls:\n                    B_from_param = B_from_param.view(-1, self.lat_state_dim, self.lat_control_dim)\n                    lat_states_dot += torch.matmul(B_from_param, lat_controls.unsqueeze(-1)).squeeze(-1)\n            else:\n                if self.lat_state_mu_independent: # TODO: test this \n                    # TODO: could make this more efficient by using a multiplication with the mask as above?\n                    self.A.weight = torch.nn.Parameter(self.A.weight * self.mask_A_mu_independent)\n                lat_states_dot = self.A(lat_states)\n                if self.include_controls:\n                    lat_states_dot += self.B(lat_controls)\n\n        if self.mask_set:\n            lat_states_dot = lat_states_dot * self.mask\n\n        return lat_states_dot\n</code></pre>"},{"location":"bnode_core/ode/bnode/bnode/#bnode_core.ode.bnode.bnode_modules.LatentODEFunc.set_mask","title":"<code>set_mask(mask)</code>","text":"<p>Set the mask for the latent state, used for deterministic training. mask: [lat_state_dim] or [lat_state_mu_dim] if lat_ode_type == 'variance_dynamic': mask for the latent state (boolean tensor or tensor with values 0 or 1)</p> Source code in <code>src/bnode_core/ode/bnode/bnode_modules.py</code> <pre><code>def set_mask(self, mask):\n    '''\n    Set the mask for the latent state, used for deterministic training.\n    mask: [lat_state_dim] or [lat_state_mu_dim] if lat_ode_type == 'variance_dynamic': mask for the latent state (boolean tensor or tensor with values 0 or 1)\n    '''\n    self.mask_set.set_(torch.tensor(True, device=mask.device))\n    if self.lat_ode_type == 'variance_dynamic':\n        _mask = torch.zeros(self.lat_state_dim, device=mask.device)\n        _mask[:self.lat_state_mu_dim] = mask # set mask for mu\n        # sigma mask is set to 1, as we do not want to include dynamics for sigma\n        self.mask.set_(torch.tensor(_mask.clone().detach(), device=mask.device, dtype=torch.float32))\n    else:\n        self.mask.set_(torch.tensor(mask.clone().detach(), device=mask.device, dtype=torch.float32))\n</code></pre>"},{"location":"bnode_core/ode/bnode/bnode/#bnode_core.ode.bnode.bnode_modules.LatentODEFunc.forward","title":"<code>forward(lat_states, lat_parameters=None, lat_controls=None, A_from_param: torch.Tensor = None, B_from_param: torch.Tensor = None)</code>","text":"<p>lat_state: [batch_size, lat_state_dim] lat_parameters: [batch_size, lat_parameter_dim] lat_controls: [batch_size, lat_control_dim]</p> Source code in <code>src/bnode_core/ode/bnode/bnode_modules.py</code> <pre><code>def forward(self, lat_states, lat_parameters = None, lat_controls=None, \n            A_from_param: torch.Tensor = None, B_from_param: torch.Tensor = None\n            ):\n    \"\"\"\n    lat_state: [batch_size, lat_state_dim]\n    lat_parameters: [batch_size, lat_parameter_dim]\n    lat_controls: [batch_size, lat_control_dim]\n    \"\"\"\n    if not self.linear:\n        if not self.lat_ode_type == 'variance_dynamic' or (self.lat_ode_type == 'variance_dynamic' and self.lat_state_mu_independent is False):\n            if self.include_controls and self.include_parameters:\n                x = torch.cat([lat_states, lat_parameters, lat_controls], dim=1)\n            elif self.include_controls:\n                x = torch.cat([lat_states, lat_controls], dim=1)\n            elif self.include_parameters:\n                x = torch.cat([lat_states, lat_parameters], dim=1)\n            else:\n                x = lat_states\n            lat_states_dot = self.net(x)\n        else: # independent mu from sigma\n            _states_mu = lat_states[:, :self.lat_state_mu_dim]\n            if self.include_controls and self.include_parameters:\n                x_mu = torch.cat([_states_mu, lat_parameters, lat_controls], dim=1)\n                x_sigma = torch.cat([lat_states, lat_parameters, lat_controls], dim=1)\n            elif self.include_controls:\n                x_mu = torch.cat([_states_mu, lat_controls], dim=1)\n                x_sigma = torch.cat([lat_states, lat_controls], dim=1)\n            elif self.include_parameters:\n                x_mu = torch.cat([_states_mu, lat_parameters], dim=1)\n                x_sigma = torch.cat([lat_states, lat_parameters], dim=1)\n            else:\n                x_mu = _states_mu\n                x_sigma = lat_states\n            lat_state_mu_dot = self.net(x_mu)\n            lat_state_sigma_dot = self.net_sigma(x_sigma)\n            lat_states_dot = torch.cat([lat_state_mu_dot, lat_state_sigma_dot], dim=1)\n    else: # linear case\n        if self.include_parameters:\n            if self.lat_state_mu_independent: # TODO: test this\n                A_from_param = A_from_param * self.mask_A_mu_independent # element-wise multiplication with mask\n            lat_states_dot = torch.matmul(A_from_param, lat_states.unsqueeze(-1)).squeeze(-1)\n            if self.include_controls:\n                B_from_param = B_from_param.view(-1, self.lat_state_dim, self.lat_control_dim)\n                lat_states_dot += torch.matmul(B_from_param, lat_controls.unsqueeze(-1)).squeeze(-1)\n        else:\n            if self.lat_state_mu_independent: # TODO: test this \n                # TODO: could make this more efficient by using a multiplication with the mask as above?\n                self.A.weight = torch.nn.Parameter(self.A.weight * self.mask_A_mu_independent)\n            lat_states_dot = self.A(lat_states)\n            if self.include_controls:\n                lat_states_dot += self.B(lat_controls)\n\n    if self.mask_set:\n        lat_states_dot = lat_states_dot * self.mask\n\n    return lat_states_dot\n</code></pre>"},{"location":"bnode_core/ode/bnode/bnode/#bnode_core.ode.bnode.bnode_modules.Decoder","title":"<code>Decoder</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/bnode_core/ode/bnode/bnode_modules.py</code> <pre><code>class Decoder(nn.Module):\n\n    def __init__(self, \n                 lat_state_mu_dim: int,\n                 lat_control_dim: int,\n                 lat_parameter_dim: int,\n                 state_dim: int,\n                 outputs_dim: int,\n                 hidden_dim: int,\n                 n_layers: int,\n                 activation: nn.Module = nn.ELU,\n                 initialization: str = 'identity',\n                 linear: bool = False,\n                 include_states_grad: bool = False,\n                 include_outputs_grad: bool = False,\n                 ):\n        super().__init__()\n        self.lat_state_mu_dim = lat_state_mu_dim\n        self.lat_control_dim = lat_control_dim\n        self.lat_parameter_dim = lat_parameter_dim\n\n        self.out_dim = state_dim + outputs_dim\n        self.state_dim = state_dim\n        self.outputs_dim = outputs_dim\n\n        self.include_controls = True if lat_control_dim &gt; 0 else False\n        self.include_parameters = True if lat_parameter_dim &gt; 0 else False\n        self.include_outputs = True if outputs_dim &gt; 0 else False\n        self.include_states = True if state_dim &gt; 0 else False\n        self.include_states_grad = include_states_grad and self.include_states\n        self.include_outputs_grad = include_outputs_grad and self.include_outputs\n\n        self.linear = linear\n\n        self.onnx_export = False # used for disabling the concatenation of the outputs in the forward method\n\n        # define layers\n        self.state_normalization = NormalizationLayer1D(state_dim) if state_dim &gt; 0 else None\n        self.outputs_normalization = NormalizationLayer1D(outputs_dim) if outputs_dim &gt; 0 else None\n        self.states_grad_normalization = NormalizationLayer1D(state_dim) if self.include_states_grad else None\n        self.outputs_grad_normalization = NormalizationLayer1D(outputs_dim) if self.include_outputs_grad else None\n\n        if self.state_dim == 0 and self.outputs_dim == 0:\n            raise ValueError('state_dim and outputs_dim cannot be both zero, you need to include at least one of them')\n\n        if not linear:\n            modules = [\n                nn.Linear(self.lat_state_mu_dim + self.lat_control_dim + self.lat_parameter_dim, hidden_dim),\n                activation(),\n            ]\n            if n_layers &lt; 2:\n                logging.warning('n_layers must be at least 2, setting n_layers to 2')\n            for i in range(n_layers-2):\n                modules.append(nn.Linear(hidden_dim, hidden_dim))\n                modules.append(activation())\n            modules.append(nn.Linear(hidden_dim, self.out_dim))\n            self.net = nn.Sequential(*modules)\n\n            initialize_weights_biases(self.net, initialization)\n        else:\n            # construct a linear state space model: y = Cx + Du\n            if not self.include_parameters:\n                self.C = nn.Linear(self.lat_state_mu_dim, self.out_dim, bias=True) # a bias shouldn't be harmful\n                initialize_weights_biases(self.C, initialization)\n                if self.include_controls:\n                    self.D = nn.Linear(self.lat_control_dim, self.out_dim, bias=False)\n                    initialize_weights_biases(self.D, initialization)\n            else:\n                self.C_and_constant_from_param = nn.Linear(self.lat_parameter_dim, self.out_dim * self.lat_state_mu_dim + self.out_dim, bias=True) # to get C matrix + constant\n                initialize_weights_biases(self.C_and_constant_from_param, initialization)\n                if self.include_controls:\n                    self.D_from_param = nn.Linear(self.lat_parameter_dim, self.out_dim * self.lat_control_dim, bias=True)\n                    initialize_weights_biases(self.D_from_param, initialization)\n\n    def forward(self, \n                lat_state: torch.Tensor,\n                lat_parameters: torch.Tensor = None,\n                lat_controls: torch.Tensor = None,\n                ):\n        \"\"\"\n        lat_state: [batch_size, lat_state_mu_dim]\n        lat_parameters: [batch_size, lat_parameter_dim]\n        lat_controls: [batch_size, lat_control_dim]\n        \"\"\"\n        if not self.linear:\n            if self.include_controls and self.include_parameters:\n                lat_state = torch.cat([lat_state, lat_parameters, lat_controls], dim=1)\n            elif self.include_controls:\n                lat_state = torch.cat([lat_state, lat_controls], dim=1)\n            elif self.include_parameters:\n                lat_state = torch.cat([lat_state, lat_parameters], dim=1)\n            x = self.net(lat_state)\n        else:\n            if not self.include_parameters:\n                x = self.C(lat_state)\n                if self.include_controls:\n                    x += self.D(lat_controls)\n            else:\n                _C = self.C_and_constant_from_param(lat_parameters)\n                C, constant = torch.split(_C, [self.out_dim * self.lat_state_mu_dim, self.out_dim], dim=1) # TODO: check if this is correct\n                C = C.view(-1, self.out_dim, self.lat_state_mu_dim)\n                constant = constant.reshape(-1, self.out_dim)\n                x = torch.matmul(C, lat_state.unsqueeze(-1)).squeeze(-1) + constant\n                if self.include_controls:\n                    D = self.D_from_param(lat_parameters)\n                    D = D.view(-1, self.out_dim, self.lat_control_dim)\n                    x += torch.matmul(D, lat_controls.unsqueeze(-1)).squeeze(-1)\n        if self.include_outputs and self.include_states:\n            state_norm, outputs_norm = torch.split(x, [self.state_dim, self.outputs_dim], dim=1)\n        elif self.include_states:\n            state_norm, outputs_norm = x, None\n        elif self.include_outputs:\n            state_norm, outputs_norm = None, x  \n        else:\n            raise ValueError('state_dim and outputs_dim cannot be both zero, you need to include at least one of them')\n\n        state = self.state_normalization(state_norm, denormalize=True) if self.include_states else None\n        outputs = self.outputs_normalization(outputs_norm, denormalize=True) if self.include_outputs else None\n        if self.onnx_export:\n            # return without concatenation for ONNX export\n            if self.include_outputs and self.include_states:\n                return state, outputs\n            elif self.include_outputs:\n                return outputs\n            elif self.include_states:\n                return state\n        if self.include_outputs and self.include_states:\n            return torch.cat([state_norm, outputs_norm, state, outputs], dim=1)\n        elif self.include_outputs:\n            return torch.cat([outputs_norm, outputs], dim=1)\n        elif self.include_states:\n            return torch.cat([state_norm, state], dim=1)\n\n    def split_return(self, x):\n        if self.include_outputs and self.include_states:\n            state_norm, outputs_norm, state, outputs = torch.split(x, [self.state_dim, self.outputs_dim, self.state_dim, self.outputs_dim], dim=1)\n        elif self.include_states:\n            state_norm, state = torch.split(x, [self.state_dim, self.state_dim], dim=1)\n            outputs_norm, outputs = None, None\n        elif self.include_outputs:\n            outputs_norm, outputs = torch.split(x, [self.outputs_dim, self.outputs_dim], dim=1)\n            state_norm, state = None, None\n        return state, outputs, state_norm, outputs_norm\n</code></pre>"},{"location":"bnode_core/ode/bnode/bnode/#bnode_core.ode.bnode.bnode_modules.Decoder.forward","title":"<code>forward(lat_state: torch.Tensor, lat_parameters: torch.Tensor = None, lat_controls: torch.Tensor = None)</code>","text":"<p>lat_state: [batch_size, lat_state_mu_dim] lat_parameters: [batch_size, lat_parameter_dim] lat_controls: [batch_size, lat_control_dim]</p> Source code in <code>src/bnode_core/ode/bnode/bnode_modules.py</code> <pre><code>def forward(self, \n            lat_state: torch.Tensor,\n            lat_parameters: torch.Tensor = None,\n            lat_controls: torch.Tensor = None,\n            ):\n    \"\"\"\n    lat_state: [batch_size, lat_state_mu_dim]\n    lat_parameters: [batch_size, lat_parameter_dim]\n    lat_controls: [batch_size, lat_control_dim]\n    \"\"\"\n    if not self.linear:\n        if self.include_controls and self.include_parameters:\n            lat_state = torch.cat([lat_state, lat_parameters, lat_controls], dim=1)\n        elif self.include_controls:\n            lat_state = torch.cat([lat_state, lat_controls], dim=1)\n        elif self.include_parameters:\n            lat_state = torch.cat([lat_state, lat_parameters], dim=1)\n        x = self.net(lat_state)\n    else:\n        if not self.include_parameters:\n            x = self.C(lat_state)\n            if self.include_controls:\n                x += self.D(lat_controls)\n        else:\n            _C = self.C_and_constant_from_param(lat_parameters)\n            C, constant = torch.split(_C, [self.out_dim * self.lat_state_mu_dim, self.out_dim], dim=1) # TODO: check if this is correct\n            C = C.view(-1, self.out_dim, self.lat_state_mu_dim)\n            constant = constant.reshape(-1, self.out_dim)\n            x = torch.matmul(C, lat_state.unsqueeze(-1)).squeeze(-1) + constant\n            if self.include_controls:\n                D = self.D_from_param(lat_parameters)\n                D = D.view(-1, self.out_dim, self.lat_control_dim)\n                x += torch.matmul(D, lat_controls.unsqueeze(-1)).squeeze(-1)\n    if self.include_outputs and self.include_states:\n        state_norm, outputs_norm = torch.split(x, [self.state_dim, self.outputs_dim], dim=1)\n    elif self.include_states:\n        state_norm, outputs_norm = x, None\n    elif self.include_outputs:\n        state_norm, outputs_norm = None, x  \n    else:\n        raise ValueError('state_dim and outputs_dim cannot be both zero, you need to include at least one of them')\n\n    state = self.state_normalization(state_norm, denormalize=True) if self.include_states else None\n    outputs = self.outputs_normalization(outputs_norm, denormalize=True) if self.include_outputs else None\n    if self.onnx_export:\n        # return without concatenation for ONNX export\n        if self.include_outputs and self.include_states:\n            return state, outputs\n        elif self.include_outputs:\n            return outputs\n        elif self.include_states:\n            return state\n    if self.include_outputs and self.include_states:\n        return torch.cat([state_norm, outputs_norm, state, outputs], dim=1)\n    elif self.include_outputs:\n        return torch.cat([outputs_norm, outputs], dim=1)\n    elif self.include_states:\n        return torch.cat([state_norm, state], dim=1)\n</code></pre>"},{"location":"bnode_core/ode/bnode/bnode_export/","title":"BNODE Export","text":""},{"location":"bnode_core/ode/bnode/bnode_export/#bnode_core.ode.bnode.bnode_export","title":"<code>bnode_core.ode.bnode.bnode_export</code>","text":"<p>BNODE Model Export to ONNX Format.</p> <p>This module provides functionality to export trained Balanced Neural ODE (BNODE)  models to ONNX format for deployment in production environments. The export process  decomposes the BNODE architecture into individual ONNX components that can be  integrated into external inference pipelines.</p>"},{"location":"bnode_core/ode/bnode/bnode_export/#bnode_core.ode.bnode.bnode_export--warning","title":"Warning","text":"<p>This documentation is AI generated and may contain inaccuracies. Please verify.</p>"},{"location":"bnode_core/ode/bnode/bnode_export/#bnode_core.ode.bnode.bnode_export--overview","title":"Overview","text":"<p>BNODE models consist of multiple neural network components organized in a  variational autoencoder (VAE) structure with a latent ODE:</p> <ol> <li> <p>Encoders: Transform high-dimensional inputs to latent representations</p> <ul> <li>State encoder: Maps physical states to latent state space</li> <li>Control encoder: Maps control inputs to latent control space (optional)</li> <li>Parameter encoder: Maps system parameters to latent parameter space (optional)</li> </ul> </li> <li> <p>Latent ODE: Defines dynamics in the learned latent space</p> <ul> <li>Can be linear (SSM-based) or nonlinear (neural network)</li> <li>Supports variance propagation (constant or dynamic)</li> <li>Optionally conditioned on latent controls and parameters</li> </ul> </li> <li> <p>Decoder: Reconstructs physical quantities from latent states</p> <ul> <li>Maps latent states back to physical state space</li> <li>Optionally reconstructs system outputs</li> <li>Can incorporate latent parameters and controls</li> </ul> </li> </ol>"},{"location":"bnode_core/ode/bnode/bnode_export/#bnode_core.ode.bnode.bnode_export--export-process","title":"Export Process","text":"<p>The export workflow involves:</p> <ol> <li>Model Loading: Retrieve trained model from MLflow or local directory</li> <li>Configuration: Load training configuration and dataset for normalization</li> <li>Component Separation: Extract individual neural network modules</li> <li>ONNX Conversion: Export each component with dynamic batch dimensions</li> <li>Example I/O: Save sample inputs/outputs in HDF5 for validation</li> </ol>"},{"location":"bnode_core/ode/bnode/bnode_export/#bnode_core.ode.bnode.bnode_export--exported-artifacts","title":"Exported Artifacts","text":"<p>For each BNODE model, the following files are generated:</p> <ul> <li><code>encoder_states.onnx</code>: State encoder neural network</li> <li><code>encoder_controls.onnx</code>: Control encoder (if applicable)</li> <li><code>encoder_parameters.onnx</code>: Parameter encoder (if applicable)</li> <li><code>latent_ode.onnx</code>: Latent ODE function</li> <li><code>latent_ode_ssm_from_param.onnx</code>: SSM parameter mapping (linear models only)</li> <li><code>decoder.onnx</code>: Decoder neural network</li> <li><code>*_example_io.hdf5</code>: Example input/output data for each component</li> <li><code>bnode_config.yaml</code>: Complete model configuration</li> </ul>"},{"location":"bnode_core/ode/bnode/bnode_export/#bnode_core.ode.bnode.bnode_export--configuration","title":"Configuration","text":"<p>The export process is configured using Hydra with the <code>onnx_export_config_class</code>  dataclass. Configuration can be provided via:</p> <ol> <li>YAML config file (<code>conf/onnx_export.yaml</code>)</li> <li>Command-line overrides</li> <li>Programmatic instantiation</li> </ol> Required Configuration Fields <p>Either <code>mlflow_run_id</code> OR <code>model_directory</code> must be specified:</p> <ul> <li>mlflow_run_id (str): MLflow run ID to retrieve model from tracking server</li> <li>model_directory (str): Local path to model artifacts directory</li> </ul> Optional Configuration Fields <ul> <li>mlflow_tracking_uri (str): MLflow server URI (default: local <code>./mlruns</code>)</li> <li>model_checkpoint_path (str): Specific checkpoint file (default: latest)</li> <li>config_path (str): Custom config file (default: <code>.hydra/config_validated.yaml</code>)</li> <li>dataset_path (str): Dataset for normalization (default: <code>dataset.hdf5</code>)</li> <li>output_dir (str): Export destination (default: Hydra output directory)</li> </ul>"},{"location":"bnode_core/ode/bnode/bnode_export/#bnode_core.ode.bnode.bnode_export--dataset-requirements","title":"Dataset Requirements","text":"<p>A dataset in HDF5 format is required for model initialization and normalization.  The dataset must contain:</p> <ul> <li>Structure: Training/validation/test splits with trajectories</li> <li>Variables: States, controls, parameters, outputs (as applicable)</li> <li>Format: Shape <code>(n_trajectories, n_variables, n_timesteps)</code></li> <li>Source: Generated by <code>data_generation</code> module or provided externally</li> </ul> <p>The dataset is used to: 1. Initialize model dimensions and normalization layers 2. Provide example inputs for ONNX graph tracing 3. Validate exported models against known data</p>"},{"location":"bnode_core/ode/bnode/bnode_export/#bnode_core.ode.bnode.bnode_export--typical-usage-examples","title":"Typical Usage Examples","text":"<p>Example 1: Export from MLflow Run::</p> <pre><code># Export model from MLflow tracking server\nuv run bnode_export mlflow_run_id=abc123def456 \\\n                    mlflow_tracking_uri=http://localhost:5000 \\\n                    output_dir=./exports/my_model\n</code></pre> <p>Example 2: Export from Local Directory::</p> <pre><code># Export model from local artifacts\nuv run bnode_export model_directory=./outputs/2024-11-07/10-30-45 \\\n                    output_dir=./exports/my_model\n</code></pre> <p>Example 3: Custom Checkpoint and Dataset::</p> <pre><code># Specify custom checkpoint and dataset paths\nuv run bnode_export mlflow_run_id=abc123def456 \\\n                    model_checkpoint_path=./checkpoints/model_phase_3.pt \\\n                    dataset_path=./data/custom_dataset.hdf5 \\\n                    output_dir=./exports/custom_export\n</code></pre> <p>Example 4: Export with Hydra Multirun::</p> <pre><code># Export multiple models in parallel\nuv run bnode_export --multirun \\\n                    mlflow_run_id=run1,run2,run3 \\\n                    output_dir=./exports\n</code></pre>"},{"location":"bnode_core/ode/bnode/bnode_export/#bnode_core.ode.bnode.bnode_export--onnx-export-backend-selection","title":"ONNX Export Backend Selection","text":"<p>This module uses PyTorch's ONNX export with <code>dynamo=False</code> to leverage the  legacy TorchScript-based exporter. This choice is made for the following reasons:</p> <p>Why Legacy Exporter (dynamo=False)?</p> <ol> <li> <p>Stability: The TorchScript-based exporter is mature and battle-tested with     complex neural network architectures including custom layers and control flow.</p> </li> <li> <p>Complex Model Support: BNODE models contain:</p> </li> <li> <p>Conditional logic (variance modes, parameter inclusion)</p> </li> <li>Custom normalization layers with stateful initialization</li> <li>Multiple encoder/decoder components with optional inputs</li> <li>Dynamic control flow based on model configuration</li> </ol> <p>The legacy exporter handles these patterns more reliably.</p> <ol> <li>Keyword Arguments: Models with optional keyword arguments (like latent     parameters and controls) require the <code>kwargs</code> parameter, which works     seamlessly with the legacy exporter.</li> </ol> <p>About the New Exporter (dynamo=True)</p> <p>Starting in PyTorch 2.9, <code>dynamo=True</code> is the default, using the new  <code>torch.export</code>-based exporter with ONNXScript. This provides:</p> <ul> <li>Better support for dynamic shapes and LLMs</li> <li>More modern export pipeline</li> <li>Enhanced control flow handling</li> </ul> <p>However, it may encounter issues with:</p> <ul> <li>Complex conditional logic</li> <li>Custom layer implementations</li> <li>Stateful modules (like normalization with deferred initialization)</li> </ul> <p>Migration Path</p> <p>If you wish to try the new exporter:</p> <ol> <li>Change <code>dynamo=False</code> to <code>dynamo=True</code> in all <code>torch.onnx.export</code> calls</li> <li>Test thoroughly with your specific model configurations</li> <li>Address any tracing warnings or errors</li> <li>Validate exported ONNX models match PyTorch reference outputs</li> </ol> <p>The legacy exporter will remain supported for the foreseeable future, so there's  no immediate need to migrate unless you require new exporter-specific features.</p> <p>Deprecation Warnings</p> <p>You may see deprecation warnings when using <code>dynamo=False</code>. These are  informational and do not affect functionality. The warnings encourage trying  the new exporter but the legacy path remains fully supported.</p>"},{"location":"bnode_core/ode/bnode/bnode_export/#bnode_core.ode.bnode.bnode_export--notes","title":"Notes","text":"<ul> <li>All exported ONNX models use dynamic batch dimensions for flexible inference</li> <li>Models are exported in evaluation mode (dropout/batch norm frozen)</li> <li>Normalization parameters are embedded in the exported models</li> <li>Linear latent ODEs export separate SSM parameter mapping for efficiency</li> <li>Export uses <code>kwargs</code> parameter to handle optional model inputs correctly</li> </ul>"},{"location":"bnode_core/ode/bnode/bnode_export/#bnode_core.ode.bnode.bnode_export--see-also","title":"See Also","text":"<ul> <li><code>bnode_core.ode.trainer</code> : Training pipeline for BNODE models</li> <li><code>bnode_core.config.onnx_export_config_class</code> : Configuration dataclass</li> <li><code>bnode_core.ode.bnode</code> : BNODE model architecture definitions</li> </ul>"},{"location":"bnode_core/ode/bnode/bnode_export/#bnode_core.ode.bnode.bnode_export.load_trained_latent_ode","title":"<code>load_trained_latent_ode(cfg_export)</code>","text":"<p>Load a trained BNODE model from MLflow or local directory.</p> <p>This function retrieves a trained BNODE model and its associated artifacts (configuration, dataset, checkpoint) from either an MLflow tracking server or a local directory. It reconstructs the model architecture and loads the trained weights.</p> <p>Parameters:</p> Name Type Description Default <code>cfg_export</code> <code>onnx_export_config_class</code> <p>Export configuration containing:</p> <ul> <li>mlflow_run_id: MLflow run identifier (if loading from MLflow)</li> <li>model_directory: Local directory path (if loading locally)</li> <li>mlflow_tracking_uri: MLflow tracking server URI</li> <li>config_path: Optional custom configuration path</li> <li>dataset_path: Optional custom dataset path</li> <li>model_checkpoint_path: Optional specific checkpoint file</li> </ul> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing:</p> <ul> <li>'model': Initialized BNODE model with loaded weights</li> <li>'cfg': OmegaConf configuration object</li> <li>'dataset_file': Opened HDF5 dataset file handle</li> <li>'dataset': Processed training dataset (stacked format)</li> <li>'temp_dir': Temporary directory path (if artifacts were downloaded, None otherwise)</li> </ul> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If dataset file cannot be found at specified path.</p> Notes <ul> <li>CUDA is disabled (use_cuda=False) for CPU-based ONNX export</li> <li>Normalization is not re-initialized (uses saved parameters)</li> <li>Latest checkpoint is used if model_checkpoint_path is None</li> <li>Model is loaded in evaluation mode for deterministic inference</li> <li>When loading from MLflow remote server, artifacts are downloaded to Hydra output folder</li> <li>Temporary artifact directory is named 'mlflow_artifacts_{run_id}' in Hydra output</li> </ul> Source code in <code>src/bnode_core/ode/bnode/bnode_export.py</code> <pre><code>def load_trained_latent_ode(cfg_export):\n    \"\"\"Load a trained BNODE model from MLflow or local directory.\n\n    This function retrieves a trained BNODE model and its associated artifacts\n    (configuration, dataset, checkpoint) from either an MLflow tracking server\n    or a local directory. It reconstructs the model architecture and loads the\n    trained weights.\n\n    Args:\n        cfg_export (onnx_export_config_class): Export configuration containing:\n\n            - mlflow_run_id: MLflow run identifier (if loading from MLflow)\n            - model_directory: Local directory path (if loading locally)\n            - mlflow_tracking_uri: MLflow tracking server URI\n            - config_path: Optional custom configuration path\n            - dataset_path: Optional custom dataset path\n            - model_checkpoint_path: Optional specific checkpoint file\n\n    Returns:\n        dict (dict): Dictionary containing:\n\n            - 'model': Initialized BNODE model with loaded weights\n            - 'cfg': OmegaConf configuration object\n            - 'dataset_file': Opened HDF5 dataset file handle\n            - 'dataset': Processed training dataset (stacked format)\n            - 'temp_dir': Temporary directory path (if artifacts were downloaded, None otherwise)\n\n    Raises:\n        FileNotFoundError: If dataset file cannot be found at specified path.\n\n    Notes:\n        - CUDA is disabled (use_cuda=False) for CPU-based ONNX export\n        - Normalization is not re-initialized (uses saved parameters)\n        - Latest checkpoint is used if model_checkpoint_path is None\n        - Model is loaded in evaluation mode for deterministic inference\n        - When loading from MLflow remote server, artifacts are downloaded to Hydra output folder\n        - Temporary artifact directory is named 'mlflow_artifacts_{run_id}' in Hydra output\n    \"\"\"\n    temp_dir = None\n\n    # get artifacts directory\n    if cfg_export.mlflow_run_id is not None:\n        mlflow.set_tracking_uri(cfg_export.mlflow_tracking_uri)\n        mlflow_run = mlflow.get_run(cfg_export.mlflow_run_id)\n        artifact_uri = mlflow_run.info.artifact_uri\n\n        # Check if artifacts are on remote server (not local file://)\n        if not artifact_uri.startswith('file://'):\n            # Download artifacts to temporary directory in Hydra output folder\n            hydra_output_dir = filepaths.dir_current_hydra_output()\n            temp_dir = hydra_output_dir / f'mlflow_artifacts_{cfg_export.mlflow_run_id}'\n            temp_dir.mkdir(parents=True, exist_ok=True)\n            logging.info(f'Downloading MLflow artifacts from remote server to: {temp_dir}')\n\n            # Download all artifacts\n            mlflow.artifacts.download_artifacts(\n                run_id=cfg_export.mlflow_run_id,\n                dst_path=str(temp_dir)\n            )\n            dir_artifacts = temp_dir\n            logging.info(f'Successfully downloaded artifacts to {temp_dir}')\n        else:\n            # Local MLflow artifacts\n            dir_artifacts = Path(artifact_uri.replace('file://', ''))\n    else:\n        dir_artifacts = Path(cfg_export.model_directory)\n    logging.info('Resolved artifacts uri as {}'.format(str(dir_artifacts)))\n\n    # get config and dataset paths\n    if cfg_export.config_path is None:\n        path_config = dir_artifacts / '.hydra' / 'config_validated.yaml'\n    else: \n        raise ValueError('Custom config_path is not supported in this version.')\n\n    # dataset path\n    if cfg_export.dataset_path is None:\n        path_dataset = dir_artifacts / 'dataset.hdf5'\n    else:\n        if cfg_export.dataset_path.startswith('file://'):\n            path_dataset = Path(cfg_export.dataset_path.replace('file://', ''))\n        else:\n            path_dataset = Path(cfg_export.dataset_path)\n\n\n    # load config (and validate it using the dataclass?)\n    with open(path_config) as file:\n        cfg_dict = yaml.load(file, Loader=yaml.FullLoader)\n        cfg = OmegaConf.create(cfg_dict)\n        cfg.use_cuda = False\n    logging.info('Loaded config of BNODE: {}'.format(str(cfg)))\n\n\n    # load training dataset\n    if path_dataset.is_file():\n        dataset_file = h5py.File(path_dataset, 'r')\n    else:\n        raise FileNotFoundError(f'Dataset file {path_dataset} not found. Please provide a valid dataset path.')\n    dataset = make_stacked_dataset(dataset_file, 'train')\n    model = initialize_model(cfg, train_dataset=dataset, hdf5_dataset=None, \n                             initialize_normalization=False, model_type='bnode')\n\n    # load latest checkpoint\n    if cfg_export.model_checkpoint_path is None:\n        path_checkpoint = sorted(dir_artifacts.rglob('model_phase_*.pt'))[-1]\n    else:\n        if cfg_export.model_checkpoint_path.startswith('file://'):\n            path_checkpoint = Path(cfg_export.model_checkpoint_path.replace('file://', ''))\n        else:\n            path_checkpoint = Path(cfg_export.model_checkpoint_path)\n        if not path_checkpoint.is_file():\n            raise FileNotFoundError(f'Checkpoint file {path_checkpoint} not found. Please provide a valid checkpoint path.')\n\n    model.load(path_checkpoint, device='cpu')\n    return {'model': model, 'cfg': cfg, 'dataset_file': dataset_file, 'dataset': dataset, 'temp_dir': temp_dir}\n</code></pre>"},{"location":"bnode_core/ode/bnode/bnode_export/#bnode_core.ode.bnode.bnode_export.export_example_io_data","title":"<code>export_example_io_data(res, inputs, path_example_io)</code>","text":"<p>Export example input/output data for ONNX model validation.</p> <p>Saves the inputs and outputs of a model component to an HDF5 file for later validation. This allows users to verify that their ONNX runtime produces identical results to the PyTorch reference implementation.</p> <p>Parameters:</p> Name Type Description Default <code>res</code> <code>different types</code> <p>Model output(s). Can be: - torch.Tensor: Single output tensor - tuple/list: Multiple output tensors - dict: Named output tensors</p> required <code>inputs</code> <code>dict</code> <p>Dictionary of input tensors with their names as keys. Each value can be a torch.Tensor or None.</p> required <code>path_example_io</code> <code>Path</code> <p>Output path for the HDF5 file.</p> required HDF5 Structure <p>/inputs/     : dataset (array)     : dataset (array)     ... /outputs/     output: dataset (single output case)     OR     output_0, output_1, ...: datasets (multiple output case)     OR     , , ...: datasets (dict output case) Notes <ul> <li>Tensors are automatically converted to NumPy arrays</li> <li>None inputs are skipped</li> <li>Outputs are organized based on their type (tensor/tuple/dict)</li> </ul> Source code in <code>src/bnode_core/ode/bnode/bnode_export.py</code> <pre><code>def export_example_io_data(res, inputs, path_example_io):\n    \"\"\"Export example input/output data for ONNX model validation.\n\n    Saves the inputs and outputs of a model component to an HDF5 file for\n    later validation. This allows users to verify that their ONNX runtime\n    produces identical results to the PyTorch reference implementation.\n\n    Args:\n        res (different types): Model output(s). Can be:\n            - torch.Tensor: Single output tensor\n            - tuple/list: Multiple output tensors\n            - dict: Named output tensors\n        inputs (dict): Dictionary of input tensors with their names as keys.\n            Each value can be a torch.Tensor or None.\n        path_example_io (Path): Output path for the HDF5 file.\n\n    HDF5 Structure:\n        /inputs/\n            &lt;input_name_1&gt;: dataset (array)\n            &lt;input_name_2&gt;: dataset (array)\n            ...\n        /outputs/\n            output: dataset (single output case)\n            OR\n            output_0, output_1, ...: datasets (multiple output case)\n            OR\n            &lt;output_name_1&gt;, &lt;output_name_2&gt;, ...: datasets (dict output case)\n\n    Notes:\n        - Tensors are automatically converted to NumPy arrays\n        - None inputs are skipped\n        - Outputs are organized based on their type (tensor/tuple/dict)\n    \"\"\"\n    with h5py.File(path_example_io, 'w') as f:\n        # Save inputs\n        grp_in = f.create_group('inputs')\n        for in_key, in_val in inputs.items():\n            if isinstance(in_val, torch.Tensor):\n                grp_in.create_dataset(in_key, data=in_val.detach().numpy())\n            elif in_val is not None:\n                grp_in.create_dataset(in_key, data=in_val)\n        # Save outputs\n        grp_out = f.create_group('outputs')\n        if isinstance(res, torch.Tensor):\n            grp_out.create_dataset('output', data=res.detach().cpu().numpy())\n        elif isinstance(res, (tuple, list)):\n            for i, out_val in enumerate(res):\n                grp_out.create_dataset(f'output_{i}', data=out_val.detach().cpu().numpy())\n        elif isinstance(res, dict):\n            for out_key, out_val in res.items():\n                grp_out.create_dataset(out_key, data=out_val.detach().cpu().numpy())\n</code></pre>"},{"location":"bnode_core/ode/bnode/bnode_export/#bnode_core.ode.bnode.bnode_export.log_shapes_of_dict","title":"<code>log_shapes_of_dict(d, name='')</code>","text":"<p>Log the shapes of tensors in a data structure for debugging.</p> <p>Recursively traverses dictionaries, lists, or tuples and logs the shapes of any PyTorch tensors found. Useful for debugging model I/O during export.</p> Source code in <code>src/bnode_core/ode/bnode/bnode_export.py</code> <pre><code>def log_shapes_of_dict(d, name=''):\n    \"\"\"Log the shapes of tensors in a data structure for debugging.\n\n    Recursively traverses dictionaries, lists, or tuples and logs the shapes\n    of any PyTorch tensors found. Useful for debugging model I/O during export.\n    \"\"\"\n    if name:\n        logging.info(f\"Shapes in {name}:\")\n    else:\n        logging.info(\"Shapes in .... :\")\n    if isinstance(d, dict):\n        for key, value in d.items():\n            if isinstance(value, torch.Tensor):\n                logging.info(f\"\\t{key}: {value.shape}\")\n            elif isinstance(value, (tuple, list)):\n                logging.info(f\"\\t{key}: {[v.shape for v in value if isinstance(v, torch.Tensor)]}\")\n            else:\n                logging.info(f\"\\t{key}: {value}\")\n    elif isinstance(d, (tuple, list)):\n        for i, value in enumerate(d):\n            if isinstance(value, torch.Tensor):\n                logging.info(f\"\\t[{i}]: {value.shape}\")\n            elif isinstance(value, (tuple, list)):\n                logging.info(f\"\\t[{i}]: {[v.shape for v in value if isinstance(v, torch.Tensor)]}\")\n            else:\n                logging.info(f\"\\t[{i}]: {value}\")\n    else:\n        logging.info(f\"\\t{type(d)}: {d}\")\n</code></pre>"},{"location":"bnode_core/ode/bnode/bnode_export/#bnode_core.ode.bnode.bnode_export.export_bnode","title":"<code>export_bnode(cfg_export: onnx_export_config_class)</code>","text":"<p>Main function for BNODE model export to ONNX format.</p> <p>This function orchestrates the complete export process:</p> <ol> <li>Loads trained BNODE model and configuration</li> <li>Extracts individual encoder, decoder, and ODE components</li> <li>Exports each component to ONNX format with dynamic batch dimensions</li> <li>Saves example input/output data for validation</li> <li>Exports configuration file for reference</li> </ol> <p>The function is designed to be invoked via the <code>uv run bnode_export</code> command, which is registered in <code>pyproject.toml</code>. Hydra manages configuration loading and command-line argument parsing.</p> <p>Parameters:</p> Name Type Description Default <code>cfg_export</code> <code>onnx_export_config_class</code> <p>Export configuration managed by Hydra. Configuration can be specified via YAML files or command-line overrides.</p> required Export Workflow <ol> <li> <p>Model Loading: </p> <ul> <li>Retrieves model from MLflow or local directory</li> <li>Loads configuration and dataset for normalization</li> <li>Restores trained weights from checkpoint</li> </ul> </li> <li> <p>Component Extraction:</p> <ul> <li>State encoder (always present)</li> <li>Control encoder (if model uses controls)</li> <li>Parameter encoder (if model uses parameters)</li> <li>Latent ODE function</li> <li>SSM parameter mapping (linear models only)</li> <li>Decoder (states and/or outputs)</li> </ul> </li> <li> <p>ONNX Export:</p> <ul> <li>Each component is exported separately</li> <li>Dynamic batch dimensions allow flexible inference</li> <li>Input/output names are explicitly defined</li> <li>Example I/O saved for each component</li> </ul> </li> <li> <p>Artifact Organization:</p> <ul> <li>All exports saved to output_dir or Hydra output</li> <li>Configuration exported as <code>bnode_config.yaml</code></li> <li>Example data in <code>*_example_io.hdf5</code> files</li> </ul> </li> </ol> Command-Line Usage <p>Export from MLflow::</p> <pre><code>uv run bnode_export mlflow_run_id=&lt;run_id&gt; \\\n                    mlflow_tracking_uri=&lt;uri&gt; \\\n                    output_dir=&lt;output_path&gt;\n</code></pre> <p>Export from local directory::</p> <pre><code>uv run bnode_export model_directory=&lt;path&gt; \\\n                    output_dir=&lt;output_path&gt;\n</code></pre> <p>With custom checkpoint::</p> <pre><code>uv run bnode_export mlflow_run_id=&lt;run_id&gt; \\\n                    model_checkpoint_path=&lt;checkpoint.pt&gt; \\\n                    dataset_path=&lt;dataset.hdf5&gt;\n</code></pre> Exported Files <ul> <li><code>encoder_states.onnx</code>: State encoder</li> <li><code>encoder_controls.onnx</code>: Control encoder (if applicable)</li> <li><code>encoder_parameters.onnx</code>: Parameter encoder (if applicable)</li> <li><code>latent_ode.onnx</code>: Latent dynamics function</li> <li><code>latent_ode_ssm_from_param.onnx</code>: SSM mapping (linear models)</li> <li><code>decoder.onnx</code>: Decoder network</li> <li><code>encoder_*_example_io.hdf5</code>: Example encoder I/O</li> <li><code>latent_ode_example_io.hdf5</code>: Example ODE I/O</li> <li><code>decoder_example_io.hdf5</code>: Example decoder I/O</li> <li><code>bnode_config.yaml</code>: Complete model configuration</li> </ul> ONNX Model Specifications <p>All exported models include:</p> <ul> <li>Dynamic axes: Batch dimension (axis 0) is dynamic</li> <li>Input names: Descriptive names for each input tensor</li> <li>Output names: Descriptive names for each output tensor</li> </ul> <p>Encoder outputs:</p> <ul> <li><code>latent_&lt;type&gt;_mu</code>: Mean of latent distribution</li> <li><code>latent_&lt;type&gt;_logvar</code>: Log-variance of latent distribution</li> </ul> <p>ODE outputs:</p> <ul> <li><code>lat_states_mu_dot</code>: State derivative (constant variance)</li> <li><code>concat(lat_states_mu_dot,lat_states_logvar_dot)</code>: Combined     derivatives (dynamic variance)</li> </ul> <p>Decoder outputs:</p> <ul> <li><code>states</code>: Reconstructed physical states</li> <li><code>outputs</code>: Reconstructed system outputs (if applicable)</li> </ul> Notes <ul> <li>Model is set to evaluation mode before export</li> <li>All computations performed on CPU (use_cuda=False)</li> <li>Normalization parameters are embedded in exported models</li> <li>Example I/O files can be used to validate ONNX Runtime inference</li> <li>Hydra output directory is used if output_dir not specified</li> </ul> See Also <ul> <li><code>load_trained_latent_ode</code>: Model loading function</li> <li><code>export_example_io_data</code>: Example I/O export function</li> <li><code>bnode_core.ode.trainer.initialize_model</code>: Model initialization</li> </ul> Source code in <code>src/bnode_core/ode/bnode/bnode_export.py</code> <pre><code>def export_bnode(cfg_export: onnx_export_config_class):\n    \"\"\"Main function for BNODE model export to ONNX format.\n\n    This function orchestrates the complete export process:\n\n    1. Loads trained BNODE model and configuration\n    2. Extracts individual encoder, decoder, and ODE components\n    3. Exports each component to ONNX format with dynamic batch dimensions\n    4. Saves example input/output data for validation\n    5. Exports configuration file for reference\n\n    The function is designed to be invoked via the ``uv run bnode_export`` command,\n    which is registered in ``pyproject.toml``. Hydra manages configuration loading\n    and command-line argument parsing.\n\n    Args:\n        cfg_export (onnx_export_config_class): Export configuration managed by Hydra.\n            Configuration can be specified via YAML files or command-line overrides.\n\n    Export Workflow:\n        1. **Model Loading**: \n            - Retrieves model from MLflow or local directory\n            - Loads configuration and dataset for normalization\n            - Restores trained weights from checkpoint\n\n        2. **Component Extraction**:\n            - State encoder (always present)\n            - Control encoder (if model uses controls)\n            - Parameter encoder (if model uses parameters)\n            - Latent ODE function\n            - SSM parameter mapping (linear models only)\n            - Decoder (states and/or outputs)\n\n        3. **ONNX Export**:\n            - Each component is exported separately\n            - Dynamic batch dimensions allow flexible inference\n            - Input/output names are explicitly defined\n            - Example I/O saved for each component\n\n        4. **Artifact Organization**:\n            - All exports saved to output_dir or Hydra output\n            - Configuration exported as ``bnode_config.yaml``\n            - Example data in ``*_example_io.hdf5`` files\n\n    Command-Line Usage:\n        Export from MLflow::\n\n            uv run bnode_export mlflow_run_id=&lt;run_id&gt; \\\\\n                                mlflow_tracking_uri=&lt;uri&gt; \\\\\n                                output_dir=&lt;output_path&gt;\n\n        Export from local directory::\n\n            uv run bnode_export model_directory=&lt;path&gt; \\\\\n                                output_dir=&lt;output_path&gt;\n\n        With custom checkpoint::\n\n            uv run bnode_export mlflow_run_id=&lt;run_id&gt; \\\\\n                                model_checkpoint_path=&lt;checkpoint.pt&gt; \\\\\n                                dataset_path=&lt;dataset.hdf5&gt;\n\n    Exported Files:\n        - ``encoder_states.onnx``: State encoder\n        - ``encoder_controls.onnx``: Control encoder (if applicable)\n        - ``encoder_parameters.onnx``: Parameter encoder (if applicable)\n        - ``latent_ode.onnx``: Latent dynamics function\n        - ``latent_ode_ssm_from_param.onnx``: SSM mapping (linear models)\n        - ``decoder.onnx``: Decoder network\n        - ``encoder_*_example_io.hdf5``: Example encoder I/O\n        - ``latent_ode_example_io.hdf5``: Example ODE I/O\n        - ``decoder_example_io.hdf5``: Example decoder I/O\n        - ``bnode_config.yaml``: Complete model configuration\n\n    ONNX Model Specifications:\n        All exported models include:\n\n        - **Dynamic axes**: Batch dimension (axis 0) is dynamic\n        - **Input names**: Descriptive names for each input tensor\n        - **Output names**: Descriptive names for each output tensor\n\n        Encoder outputs:\n\n        - ``latent_&lt;type&gt;_mu``: Mean of latent distribution\n        - ``latent_&lt;type&gt;_logvar``: Log-variance of latent distribution\n\n        ODE outputs:\n\n        - ``lat_states_mu_dot``: State derivative (constant variance)\n        - ``concat(lat_states_mu_dot,lat_states_logvar_dot)``: Combined\n            derivatives (dynamic variance)\n\n        Decoder outputs:\n\n        - ``states``: Reconstructed physical states\n        - ``outputs``: Reconstructed system outputs (if applicable)\n\n    Notes:\n        - Model is set to evaluation mode before export\n        - All computations performed on CPU (use_cuda=False)\n        - Normalization parameters are embedded in exported models\n        - Example I/O files can be used to validate ONNX Runtime inference\n        - Hydra output directory is used if output_dir not specified\n\n    See Also:\n        - ``load_trained_latent_ode``: Model loading function\n        - ``export_example_io_data``: Example I/O export function\n        - ``bnode_core.ode.trainer.initialize_model``: Model initialization\n    \"\"\"\n    logging.info('Exporting BNODE using the following config {}'.format(str(cfg_export)))\n\n    # load model\n    res = load_trained_latent_ode(cfg_export)\n    model, cfg, dataset_file, dataset = res['model'], res['cfg'], res['dataset_file'], res['dataset']\n    temp_dir = res['temp_dir']\n    model.eval()\n\n    # determine output dir\n    dir_output = Path(cfg_export.output_dir) if cfg_export.output_dir is not None else filepaths.dir_current_hydra_output()\n\n    # export bnode config\n    path_config = dir_output / 'bnode_config.yaml'\n    logging.info(f'Exporting BNODE config to {path_config}')\n    path_config.parent.mkdir(parents=True, exist_ok=True)\n    with open(path_config, 'w') as f:\n        yaml.dump(OmegaConf.to_container(cfg, resolve=True), f, default_flow_style=False)\n\n    # get test points for graph construction\n    test_state = dataset[0]['states'][:,0].unsqueeze(0)\n    test_control = dataset[0]['controls'][:,0].unsqueeze(0) if model.include_controls else None\n    test_parameters = dataset[0]['parameters'].unsqueeze(0) if model.include_parameters else None\n\n    # export the encoders\n    encoders = {'states': model.state_encoder, \n                'controls': model.controls_encoder if model.include_controls else None,\n                'parameters': model.parameter_encoder if model.include_params_encoder else None\n            }\n    # construct test inputs for graph construction\n    inputs_dict = {\n        'states': {'x': test_state},\n        'controls': {'x': test_control} if model.params_to_control_encoder is False else {'x': test_control, 'params': test_parameters},\n        'parameters': {'x': test_parameters},\n    }\n    # handling of additional inputs to state encoder\n    if model.params_to_state_encoder is True:\n        inputs_dict['states']['params'] = test_parameters\n    if model.controls_to_state_encoder is True:\n        inputs_dict['states']['controls'] = test_control\n\n    latents_dict = {}\n    for key, encoder in encoders.items():\n        if encoder is not None:\n            path_encoder = dir_output / f'encoder_{key}.onnx'\n            logging.info(f'Exporting {key} encoder to {path_encoder}')\n            # test model\n            log_shapes_of_dict(inputs_dict[key], f'Inputs for {key} encoder')\n            res = encoder(**inputs_dict[key])\n            log_shapes_of_dict(res, f'Outputs of {key} encoder')\n            logging.info(f'Test result {res}')\n            # export\n            input_names = list(inputs_dict[key].keys())\n            output_names=['latent_' + key + '_mu', 'latent_' + key + '_logvar']\n            dynamic_axes={}\n            for name in input_names:\n                dynamic_axes[name] = {0: 'batch_size'}\n            for name in output_names:\n                dynamic_axes[name] = {0: 'batch_size'}\n            # Use legacy TorchScript-based exporter for better stability\n            torch.onnx.export(encoder, \n                              args=(),\n                              kwargs=inputs_dict[key],\n                              f=path_encoder, \n                              input_names=input_names,\n                              output_names=output_names,\n                              dynamic_axes=dynamic_axes,\n                              dynamo=False\n            )\n            logging.info(f'Exported {key} encoder successfully')\n            # export also example io\n            path_example_io = dir_output / f'encoder_{key}_example_io.hdf5'\n            export_example_io_data(res, inputs_dict[key], path_example_io)\n            # save latent variable\n            latents_dict[key] = res[0] # the first is mu\n\n    # export ssm from parameters model and get A_from_param and B_from_param for the latent ODE function\n    ode = model.latent_ode_func\n    if ode.include_parameters is True and ode.linear is True:\n        # this is only possible if the model is linear and has parameters\n        logging.info('Exporting SSM from parameters')\n        ssm = model.latent_ode_func.ssm_from_param\n        path_ssm = dir_output / 'latent_ode_ssm_from_param.onnx'\n        logging.info(f'Export latent ODE SSM from parameters to {path_ssm}')\n        # construct test input\n        inputs = {\n            'lat_parameters': latents_dict['parameters'],\n        }\n        # test model\n        log_shapes_of_dict(inputs, 'Inputs for latent ODE SSM from parameters')\n        res = ssm(**inputs)\n        log_shapes_of_dict(res, 'Outputs of latent ODE SSM from parameters')\n        logging.info(f'Test result {res}')\n        # export\n        input_names=['lat_parameters']\n        output_names=['A', 'B'] if ode.include_controls else ['A']\n        dynamic_axes={}\n        for name in input_names:\n            dynamic_axes[name] = {0: 'batch_size'}\n        for name in output_names:\n            dynamic_axes[name] = {0: 'batch_size'}\n        torch.onnx.export(ssm, \n                          args=(),\n                          kwargs=inputs,\n                          f=path_ssm, \n                          input_names=input_names, \n                          output_names=output_names, \n                          dynamic_axes=dynamic_axes,\n                          dynamo=False)\n        logging.info(f'Exported latent ODE SSM from parameters successfully')\n        # get A_from_param and B_from_param\n        if ode.include_controls:\n            A_from_param, B_from_param = res\n        else:\n            A_from_param = res\n\n    # export the latent ode function\n    path_ode = dir_output / 'latent_ode.onnx'\n    logging.info(f'Export latent ODE to {path_ode}')\n    # construct test input\n    inputs = {\n        'lat_states': latents_dict['states'],\n        'lat_parameters': latents_dict['parameters'] if ode.include_parameters is True else None,\n        'lat_controls': latents_dict['controls'] if ode.include_controls is True else None,\n        'A_from_param': A_from_param if ode.include_parameters is True and ode.linear else None,\n        'B_from_param': B_from_param if ode.include_parameters is True and ode.linear and ode.include_controls else None,\n    }\n    # test model\n    log_shapes_of_dict(inputs, 'Inputs for latent ODE')\n    res = ode(**inputs)\n    log_shapes_of_dict(res, 'Outputs of latent ODE')\n    logging.info(f'Test result {res}')\n    # export\n    input_names=[]\n    for key in inputs.keys():\n        if inputs[key] != None:\n            input_names.append(key)\n    dynamic_axes={}\n    for name in input_names:\n        dynamic_axes[name] = {0: 'batch_size'}\n    if model.lat_ode_type == 'variance_constant' or model.lat_ode_type == 'vanilla':\n        output_names = ['lat_states_mu_dot']\n        dynamic_axes['lat_states_mu_dot'] = {0: 'batch_size'}\n    elif model.lat_ode_type == 'variance_dynamic':\n        output_names = ['concat(lat_states_mu_dot,lat_states_logvar_dot)']\n        dynamic_axes['concat(lat_states_mu_dot,lat_states_logvar_dot)'] = {0: 'batch_size'}\n    # Filter out None values and convert to tuple\n    filtered_inputs = {k: v for k, v in inputs.items() if v is not None}\n    torch.onnx.export(ode, \n                      args=(),\n                      kwargs=filtered_inputs,\n                      f=path_ode, \n                      input_names=input_names, \n                      output_names=output_names, \n                      dynamic_axes=dynamic_axes,\n                      dynamo=False)\n    logging.info(f'Exported latent ODE successfully')\n    # export also example io\n    path_example_io = dir_output / f'latent_ode_example_io.hdf5'\n    export_example_io_data(res, inputs, path_example_io)\n\n    # export the decoder\n    # TODO: What to with split return? because we have to tak the first n elements for states etc now... implement other function for this? / optional argument?\n    decoder = model.decoder\n    decoder.onnx_export = True  # disable concatenation of outputs for ONNX export\n    path_decoder = dir_output / 'decoder.onnx'\n    logging.info(f'Export decoder to {path_decoder}')\n    # construct test input\n    inputs = {\n        'lat_state': latents_dict['states'],\n        'lat_parameters': latents_dict['parameters'] if decoder.include_parameters is True else None,\n        'lat_controls': latents_dict['controls'] if decoder.include_controls is True else None,\n    }\n    # test model\n    log_shapes_of_dict(inputs, 'Inputs for decoder')\n    res = decoder(**inputs)\n    log_shapes_of_dict(res, 'Outputs of decoder')\n    logging.info(f'Test result {res}')\n    input_names = []\n    # export\n    for key in inputs.keys():\n        if inputs[key] != None:\n            input_names.append(key)\n    if decoder.include_outputs and decoder.include_states:\n        output_names = ['states', 'outputs']\n    elif decoder.include_outputs:\n        output_names = ['outputs']\n    elif decoder.include_states:\n        output_names = ['states']\n    dynamic_axes={}\n    for name in input_names:\n        dynamic_axes[name] = {0: 'batch_size'}\n    for name in output_names:\n        dynamic_axes[name] = {0: 'batch_size'}\n    # Filter out None values and convert to tuple\n    filtered_inputs = {k: v for k, v in inputs.items() if v is not None}\n    torch.onnx.export(decoder, \n                      args=(),\n                      kwargs=filtered_inputs,\n                      f=path_decoder, \n                      input_names=input_names, \n                      output_names=output_names, \n                      dynamic_axes=dynamic_axes,\n                      dynamo=False)\n    logging.info(f'Exported decoder successfully')\n    # export also example io\n    path_example_io = dir_output / f'decoder_example_io.hdf5'\n    export_example_io_data(res, inputs, path_example_io)\n\n    if temp_dir is not None:\n        logging.info(f'Cleaning up temporary directory: {temp_dir}')\n        shutil.rmtree(temp_dir, ignore_errors=True)\n        logging.info('Temporary directory cleaned up successfully')\n\n    # copy the current hydra folder to the output for reference\n    dir_hydra_current = filepaths.dir_current_hydra_output()\n    dir_hydra_output_copy = dir_output / 'hydra'\n    if dir_hydra_current.is_dir() and (dir_hydra_output_copy.absolute() != dir_hydra_current.absolute()):\n        logging.info(f'Copying current Hydra directory {dir_hydra_current} to output {dir_hydra_output_copy}')\n        shutil.copytree(dir_hydra_current, dir_hydra_output_copy, dirs_exist_ok=True)\n        logging.info('Hydra directory copied successfully')\n    else: \n       Warning(f'Current Hydra directory {dir_hydra_current} not found and could not be copied.')\n</code></pre>"},{"location":"bnode_core/ode/trainer_utils/test_from_mlflow/","title":"Testing from MLflow checkpointed runs","text":"<p><code>src/bnode_core/ode/trainer_utils/test_from_mlflow.py</code></p>"},{"location":"bnode_core/ode/trainer_utils/test_from_mlflow/#bnode_core.ode.trainer_utils.test_from_mlflow","title":"<code>bnode_core.ode.trainer_utils.test_from_mlflow</code>","text":"<p>Test trained models from MLflow runs on new datasets.</p> <p>This module provides functionality to load trained neural ODE models from MLflow runs and test them on different datasets. It downloads artifacts directly from the MLflow server (no local file access required) and executes validation runs.</p> <p>You can use all configuration options from trainer.py to override parameters for testing.</p>"},{"location":"bnode_core/ode/trainer_utils/test_from_mlflow/#bnode_core.ode.trainer_utils.test_from_mlflow--typical-usage-example","title":"Typical Usage Example","text":"<p>Test a single run on a new dataset using experiment_id (recommended):</p> <pre><code>    python test_from_mlflow.py \\\n        experiment_id=123456789 \\\n        run_name=bemused-hen-59 \\\n        # or run_id=8c2c32b9407a4e20946f72cd1c714776 \\\n        dataset_name=myTestData \\\n        mlflow_experiment_name=validation_results \\\n        mlflow_tracking_uri=http://localhost:5000 \\\n        n_processes=1 \\\n        nn_model_base=latent_ode_base \\\n        # specify overrides:\n        override nn_model.training.batch_size_test=128 \\\n        override nn_model.training.test_save_internal_variables=false \\\n        override n_workers_train_loader=1 \\\n        override n_workers_other_loaders=1 \\\n        override use_cuda=true\n</code></pre>"},{"location":"bnode_core/ode/trainer_utils/test_from_mlflow/#bnode_core.ode.trainer_utils.test_from_mlflow--command-line-arguments","title":"Command Line Arguments","text":"<p>Required:</p> <pre><code>dataset_name : str or list[str]\n    Name(s) of dataset(s) to test on. Comma-separated for multiple datasets.\nmlflow_experiment_name : str\n    Name for the new MLflow experiment where test results will be logged.\nnn_model_base : str\n    Base configuration for the neural network model (e.g., 'latent_ode_base').\n</code></pre> <p>Run Selection (one of):</p> <pre><code>run_id : str or list[str]\n    MLflow run ID(s) to test. Comma-separated for multiple runs.\nexperiment_id + run_name : str\n    Experiment ID and specific run name(s) within that experiment (recommended).\nexperiment_id : str\n    Experiment ID to test all runs from that experiment.\nexperiment : str (deprecated)\n    Experiment name - triggers warning as multiple experiments can share names.\n</code></pre> <p>Optional:</p> <pre><code>mlflow_tracking_uri : str\n    URI of MLflow tracking server. Defaults to 'http://localhost:5000'.\nn_processes : int\n    Number of parallel processes for testing. Defaults to 1 (sequential).\noverride &lt;key&gt;=&lt;value&gt; : \n    Override specific config parameters (e.g., 'override use_cuda=false').\n</code></pre>"},{"location":"bnode_core/ode/trainer_utils/test_from_mlflow/#bnode_core.ode.trainer_utils.test_from_mlflow--notes","title":"Notes","text":"<ul> <li>Artifacts are downloaded from MLflow server to Hydra output directory.</li> <li>Downloaded artifacts are stored in {hydra_output}/mlflow_test_artifacts/run_{run_id}/.</li> <li>Artifacts persist after testing for inspection and debugging.</li> <li>Model checkpoints are automatically retrieved from MLflow artifact storage.</li> <li>Results are logged to a new MLflow experiment specified by mlflow_experiment_name.</li> <li>When testing multiple runs/datasets, a Cartesian product is created (all combinations).</li> <li>Use experiment_id instead of experiment name to avoid ambiguity.</li> </ul>"},{"location":"bnode_core/ode/trainer_utils/test_from_mlflow/#bnode_core.ode.trainer_utils.test_from_mlflow--see-also","title":"See Also","text":"<p>bnode_core.ode.trainer : Main training module with train_all_phases function. mlflow : MLflow documentation for run and artifact management.</p>"},{"location":"bnode_core/ode/trainer_utils/test_from_mlflow/#bnode_core.ode.trainer_utils.test_from_mlflow.parse_overrides","title":"<code>parse_overrides(override_list)</code>","text":"<p>Parse list of override strings into a dict of key-value pairs. Each override should be in the form key=value.</p> Source code in <code>src/bnode_core/ode/trainer_utils/test_from_mlflow.py</code> <pre><code>def parse_overrides(override_list):\n    \"\"\"\n    Parse list of override strings into a dict of key-value pairs.\n    Each override should be in the form key=value.\n    \"\"\"\n    overrides = {}\n    for item in override_list:\n        if '=' not in item:\n            raise ValueError(f\"Override argument '{item}' is not in key=value format.\")\n        key, value = item.split('=', 1)\n        overrides[key.strip()] = value.strip()\n    return overrides\n</code></pre>"},{"location":"bnode_core/ode/trainer_utils/test_from_mlflow/#bnode_core.ode.trainer_utils.test_from_mlflow.get_run_ids","title":"<code>get_run_ids(args)</code>","text":"<p>Retrieve MLflow run IDs based on provided selection criteria.</p> <p>Resolves run IDs from either direct run_id specification, run_name lookup within an experiment, or all runs from an experiment.</p> <p>Parameters:</p> Name Type Description Default <code>command_line_args</code> <code>dict</code> <p>Parsed command line arguments containing one of:</p> <ul> <li>'run_id': Direct list of run IDs.</li> <li>'experiment_id' + 'run_name': Experiment ID and specific run names.</li> <li>'experiment_id': All runs from the experiment.</li> <li>'experiment': Experiment name (deprecated, triggers warning).</li> </ul> <p>Optional:</p> <ul> <li>'mlflow_tracking_uri': MLflow server URI.</li> </ul> required <p>Returns:</p> Name Type Description <code>list</code> <code>list[str]</code> <p>List of MLflow run IDs to test.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If incompatible argument combinations are provided (e.g., both run_name and run_id, or both run_id and experiment_id).</p> Side Effects <ul> <li>Sets MLflow tracking URI via mlflow.set_tracking_uri().</li> <li>Prints progress messages about run ID retrieval.</li> <li>Issues warning if 'experiment' (name) is used instead of 'experiment_id'.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_run_ids({'experiment_id': ['123456'], 'run_name': ['run1', 'run2']})\n['abc123', 'def456']\n</code></pre> Source code in <code>src/bnode_core/ode/trainer_utils/test_from_mlflow.py</code> <pre><code>def get_run_ids(args):\n    \"\"\"Retrieve MLflow run IDs based on provided selection criteria.\n\n    Resolves run IDs from either direct run_id specification, run_name lookup\n    within an experiment, or all runs from an experiment.\n\n    Args:\n        command_line_args (dict): Parsed command line arguments containing one of:\n\n            - 'run_id': Direct list of run IDs.\n            - 'experiment_id' + 'run_name': Experiment ID and specific run names.\n            - 'experiment_id': All runs from the experiment.\n            - 'experiment': Experiment name (deprecated, triggers warning).\n\n            Optional:\n\n            - 'mlflow_tracking_uri': MLflow server URI.\n\n    Returns:\n        list (list[str]): List of MLflow run IDs to test.\n\n    Raises:\n        ValueError: If incompatible argument combinations are provided\n            (e.g., both run_name and run_id, or both run_id and experiment_id).\n\n    Side Effects:\n        - Sets MLflow tracking URI via mlflow.set_tracking_uri().\n        - Prints progress messages about run ID retrieval.\n        - Issues warning if 'experiment' (name) is used instead of 'experiment_id'.\n\n    Examples:\n        &gt;&gt;&gt; get_run_ids({'experiment_id': ['123456'], 'run_name': ['run1', 'run2']})\n        ['abc123', 'def456']\n    \"\"\"\n    mlflow.set_tracking_uri(args.mlflow_tracking_uri)\n    if args.run_id:\n        return args.run_id\n    if args.experiment:\n        warnings.warn(\n            \"Using 'experiment' (name) is deprecated. Multiple experiments can have the same name. \"\n            \"Please use 'experiment_id' instead for unambiguous experiment identification.\",\n            DeprecationWarning,\n            stacklevel=2\n        )\n        experiment = mlflow.get_experiment_by_name(args.experiment)\n        if experiment is None:\n            raise ValueError(f\"Experiment with name '{args.experiment}' not found.\")\n        # TODO: What happens if there are multiple experiments with same name?\n        experiment_id = experiment.experiment_id\n    else:\n        experiment_id = args.experiment_id\n    runs = mlflow.search_runs(experiment_id)\n    if args.run_name:\n        run_ids = []\n        for run_name in args.run_name:\n            matching_runs = runs[runs[\"tags.mlflow.runName\"] == run_name]\n            if len(matching_runs) == 0:\n                raise ValueError(f\"No run found with name '{run_name}' in experiment {experiment_id}\")\n            run_ids.append(matching_runs[\"run_id\"].values[0])\n        return run_ids\n    return runs[\"run_id\"].to_list()\n</code></pre>"},{"location":"bnode_core/ode/trainer_utils/test_from_mlflow/#bnode_core.ode.trainer_utils.test_from_mlflow.main","title":"<code>main()</code>","text":"<p>Main execution function for testing models from MLflow runs.</p> <p>Orchestrates the complete workflow: 1. Parses and validates command line arguments. 2. Retrieves run IDs from MLflow using experiment_id (or deprecated experiment name). 3. For each run-dataset combination:    - Downloads artifacts from MLflow server to Hydra output directory.    - Loads the training configuration from downloaded artifacts.    - Updates config for testing (new dataset, model path, test mode).    - Applies any override parameters.    - Saves modified config to temporary files in Hydra output. 4. Executes testing jobs either sequentially or in parallel.</p> <p>The function creates a Cartesian product of runs \u00d7 datasets, generating one test job for each combination.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If command line arguments are invalid or incompatible.</p> <code>FileNotFoundError</code> <p>If MLflow artifacts (config, model) cannot be found.</p> Side Effects <ul> <li>Creates mlflow_test_artifacts directory in Hydra output for configs and artifacts.</li> <li>Downloads artifacts from MLflow server (if not local).</li> <li>Artifacts persist after testing for inspection.</li> <li>Launches subprocess calls to trainer.py for each test job.</li> <li>Logs results to MLflow under the specified experiment name.</li> <li>Prints progress messages throughout execution.</li> </ul> Notes <ul> <li>Model checkpoints are retrieved from the final training phase.</li> <li>Sequence length is set to match the last training phase.</li> <li>Original training dataset name is preserved for reference.</li> <li>Artifacts are organized as: {hydra_output}/mlflow_test_artifacts/run_{run_id}/</li> <li>Use experiment_id instead of experiment name to avoid ambiguity warnings.</li> </ul> <p>Examples:</p> <p>Command line usage::</p> <pre><code>python test_from_mlflow.py \\\n    experiment_id=123456789 \\\n    run_name=final-model-123 \\\n    dataset_name=validation_set \\\n    mlflow_experiment_name=validation_results \\\n    nn_model_base=latent_ode_base \\\n    n_processes=1\n</code></pre> Source code in <code>src/bnode_core/ode/trainer_utils/test_from_mlflow.py</code> <pre><code>def main():\n    \"\"\"Main execution function for testing models from MLflow runs.\n\n    Orchestrates the complete workflow:\n    1. Parses and validates command line arguments.\n    2. Retrieves run IDs from MLflow using experiment_id (or deprecated experiment name).\n    3. For each run-dataset combination:\n       - Downloads artifacts from MLflow server to Hydra output directory.\n       - Loads the training configuration from downloaded artifacts.\n       - Updates config for testing (new dataset, model path, test mode).\n       - Applies any override parameters.\n       - Saves modified config to temporary files in Hydra output.\n    4. Executes testing jobs either sequentially or in parallel.\n\n    The function creates a Cartesian product of runs \u00d7 datasets, generating\n    one test job for each combination.\n\n    Raises:\n        ValueError: If command line arguments are invalid or incompatible.\n        FileNotFoundError: If MLflow artifacts (config, model) cannot be found.\n\n    Side Effects:\n        - Creates mlflow_test_artifacts directory in Hydra output for configs and artifacts.\n        - Downloads artifacts from MLflow server (if not local).\n        - Artifacts persist after testing for inspection.\n        - Launches subprocess calls to trainer.py for each test job.\n        - Logs results to MLflow under the specified experiment name.\n        - Prints progress messages throughout execution.\n\n    Notes:\n        - Model checkpoints are retrieved from the final training phase.\n        - Sequence length is set to match the last training phase.\n        - Original training dataset name is preserved for reference.\n        - Artifacts are organized as: {hydra_output}/mlflow_test_artifacts/run_{run_id}/\n        - Use experiment_id instead of experiment name to avoid ambiguity warnings.\n\n    Examples:\n        Command line usage::\n\n            python test_from_mlflow.py \\\\\n                experiment_id=123456789 \\\\\n                run_name=final-model-123 \\\\\n                dataset_name=validation_set \\\\\n                mlflow_experiment_name=validation_results \\\\\n                nn_model_base=latent_ode_base \\\\\n                n_processes=1\n    \"\"\"\n    args = parse_args()\n    overrides = parse_overrides(args.override)\n    run_ids = get_run_ids(args)\n    dataset_names = args.dataset_name\n\n    # get temporary directory in Hydra output folder for saving config files and artifacts\n    now = datetime.now()\n    date_str = now.strftime(\"%Y-%m-%d\")\n    time_str = now.strftime(\"%H-%M-%S\")\n    hydra_output_dir = Path.cwd() / \"outputs\" / date_str / time_str\n    hydra_output_dir.mkdir(parents=True, exist_ok=True)\n    temp_dir_path = hydra_output_dir / \"mlflow_test_artifacts\"\n    temp_dir_path.mkdir(parents=True, exist_ok=True)\n    print(f\"using directory for artifacts and config files: {temp_dir_path}\")\n    # create nn_model directory\n    (temp_dir_path / \"nn_model\").mkdir(parents=True, exist_ok=True)\n\n    jobs = 0\n    training_dataset_list = []\n    artifact_dirs = {}  # Store artifact directories per run_id\n\n    for run_id in run_ids:\n        for dataset in dataset_names:\n            # Download artifacts from MLflow server\n            mlflow_run = mlflow.get_run(run_id)\n            artifact_uri = mlflow_run.info.artifact_uri\n\n            # Check if we need to download artifacts (not already local)\n            if run_id not in artifact_dirs:\n                if not artifact_uri.startswith('file://'):\n                    # Download artifacts from remote MLflow server\n                    run_artifact_dir = temp_dir_path / f\"run_{run_id}\"\n                    run_artifact_dir.mkdir(parents=True, exist_ok=True)\n                    print(f\"Downloading artifacts for run {run_id} from MLflow server to {run_artifact_dir}\")\n\n                    mlflow.artifacts.download_artifacts(\n                        run_id=run_id,\n                        dst_path=str(run_artifact_dir)\n                    )\n                    artifact_dirs[run_id] = run_artifact_dir\n                    print(f\"Successfully downloaded artifacts to {run_artifact_dir}\")\n                else:\n                    # Local artifacts - use direct path\n                    artifact_dirs[run_id] = Path(artifact_uri.replace('file://',''))\n                    print(f\"Using local artifacts from {artifact_dirs[run_id]}\")\n\n            # Get config path from downloaded/local artifacts\n            _config_path = artifact_dirs[run_id] / \".hydra\" / \"config_validated.yaml\"\n\n            if not _config_path.exists():\n                raise FileNotFoundError(f\"Config file not found at {_config_path}\")\n\n            # copy config to temporary directory\n            temp_config = temp_dir_path / f\"config_{jobs}.yaml\"\n            print(f\"copying config from {_config_path} to {temp_config}\")\n            shutil.copy(_config_path, temp_config)\n\n            # update config with dataset_name, sequence_length, model_path, test_mode\n            # open yaml file\n            with open(temp_config) as file:\n                config = yaml.load(file, Loader=yaml.FullLoader)\n\n            # update config\n            training_dataset_list.append(config[\"dataset_name\"])\n            config[\"dataset_name\"] = dataset\n            config[\"mlflow_experiment_name\"] = args.mlflow_experiment_name\n            # TODO: is this important\n            config[\"nn_model\"][\"training\"][\"pre_trained_model_seq_len\"] = config[\"nn_model\"][\"training\"][\"main_training\"][-1][\"seq_len_train\"]\n\n            # Use local path to downloaded model checkpoint\n            # TODO: could also look for latest checkpoint in dir\n            _model_filename = f\"model_phase_{len(config['nn_model']['training']['main_training'])}.pt\"\n            _model_path = artifact_dirs[run_id] / _model_filename\n\n            if not _model_path.exists():\n                raise FileNotFoundError(f\"Model checkpoint not found at {_model_path}\")\n\n            config[\"nn_model\"][\"training\"][\"path_trained_model\"] = str(_model_path)\n            print(f\"\\tmodel path: {_model_path}\")\n            config[\"nn_model\"][\"training\"][\"load_trained_model_for_test\"] = True\n\n            # set overrides (propagate to config using key-path logic)\n            for key, value in overrides.items():\n                print(f\"setting override: {key}={value}\")\n                path = key.split('.')\n                ref = config\n                for i, part in enumerate(path):\n                    if i == len(path) - 1:\n                        ref[part] = value\n                    else:\n                        if part not in ref:\n                            ref[part] = {}\n                        ref = ref[part]\n\n            config_nn_model = config[\"nn_model\"]\n            # delete nn_model from config\n            del config[\"nn_model\"]\n\n            # add defaults to the very top\n            config[\"defaults\"]= [\"base_train_test\", {\"nn_model\": f\"model{jobs}\"}, \"_self_\"]\n            # also to nn_model\n            config_nn_model[\"defaults\"] = [args.nn_model_base, \"_self_\"]\n\n            # save updated config to temporary directory\n            with open(temp_config, 'w') as file:\n                yaml.dump(config, file)\n\n            # save nn_model to temporary directory\n            with open(temp_dir_path / \"nn_model\" / f\"model{jobs}.yaml\", 'w') as file:\n                yaml.dump(config_nn_model, file)\n            jobs += 1\n    print(f\"Successfully created {jobs} jobs.\")\n    print(f\"Artifacts and configs saved in: {temp_dir_path}\")\n\n    print(\"starting jobs...\")\n    # remove all arguments from sys.argv\n\n    def wrap_train_all_phases(temp_dir, temp_config_name, training_dataset):\n        \"\"\"Execute trainer.py in a subprocess with specified configuration.\n\n        Args:\n            temp_dir (str): Path to temporary directory containing config files.\n            temp_config_name (str): Name of the config file to use.\n            training_dataset (str): Name of the original training dataset (for logging).\n\n        Returns:\n            subprocess.CompletedProcess: Result of the subprocess execution.\n        \"\"\"\n        # Split the command into executable and arguments instead of one string with spaces.\n        cmd = [\n            \"uv\",\n            \"run\",\n            \"trainer\",\n            f\"-cp={temp_dir}\",\n            f\"-cn={temp_config_name}\",\n            f\"+nn_model.training.training_dataset_name={training_dataset}\"\n        ]\n        print(f\"Running command: {' '.join(cmd)}\")\n        result = subprocess.run(cmd)\n        return result\n\n    if args.n_processes == 1:\n        print(\"running jobs sequentially\")\n        for i in range(jobs):\n            result = wrap_train_all_phases(str(temp_dir_path), f\"config_{i}.yaml\", training_dataset_list[i])\n            print(result)\n    else:\n        print(\"running jobs in parallel\")\n        warnings.warn(\"Parallel execution is not fully tested yet.\")\n        with Pool(processes=args.n_processes) as pool:\n            results = [pool.apply_async(wrap_train_all_phases, (str(temp_dir_path), f\"config_{i}.yaml\", training_dataset_list[i])) for i in range(jobs)]\n            for result in results:\n                result.get()\n\n    print(f\"All jobs completed.\")\n</code></pre>"}]}